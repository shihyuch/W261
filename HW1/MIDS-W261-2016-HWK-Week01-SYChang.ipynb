{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Fall 2016 Homework Week 1\n",
    "\n",
    "Shih Yu Chang<br />\n",
    "W261-2<br />\n",
    "sychang@ischool.berkeley.edu<br />\n",
    "Sep. 3, 2016\n",
    "\n",
    "\n",
    "I come from Mountain View, CA and am working at Oracle as sfotware enginner whorking for cloud networking. I began my MIDS from 2016 Spring and expect to graduate at 2017 Summer. I wish to learn applying ML knolwedge in analyzing Big Data. <br />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.0. \n",
    "#### Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Big Data\" is a domain that involves working with data at volumes, velocities and varieties that are exceed the boundaries of \"traditional\" vertically scaled solutions and systems. In dealing with Big Data, people require new tools and techniques, e.g., using parallel processing and coordinating large networking. \n",
    "\n",
    "\n",
    "We can apply Big Data at traffic control in big city. Some cities in China have seen their economy develop rapidly in recent years. As \n",
    "a result, local traffic has become much heavier, leading to an increase in vehicle accidents and traffic violations.The city government needed better ways to monitor and manage local traffic to provide better transportation services to the public. Taking a data-driven approach to the problem, the local traffic department installed more than thoudands digital monitoring devices in the city’s key checkpoints. These devices capture images and video data continuously. The traffic department now faces  a terabyte of data each month. The increasing amount of traffic data now poses challenges in the city’s ability to effectively manage traffic. Big Data technique can be applied here to store, retrival and anlyze data more efficiently. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "HW1.0.3. Bias Variance\n",
    "\n",
    "What is bias-variance decomposition in the context machine learning? How is it used in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias–variance decomposition is a method of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three components, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\n",
    "\n",
    "This tradeoff applies to all forms of supervised based machine learning: classification, function fitting (regression),and structured output learning. It has also been invoked to explain the effectiveness of heuristics in AI learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "3. HW1.1 WordCount using a single thread\n",
    "\n",
    "Back to Table of Contents\n",
    "\n",
    "Write a program called alice_words.py that creates a text file named alice_words.txt containing an alphabetical listing of all the words, and the number of times each occurs, in the text version of Alice’s Adventures in Wonderland. (You can obtain a free plain text version of the book, along with many others, from here The first 10 lines of your output file should look something like this (the counts are not totally precise):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shihyu/HW1\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check where is the current directory and change if necessary using something like: %cd W261MasterDir\n",
    "!pwd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘HW1’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#let's organize our homeworks into subfolders by week at least (and possibly by problem)\n",
    "!mkdir HW1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shihyu/HW1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# notice the use of % in the following magic command\n",
    "%cd HW1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  163k  100  163k    0     0   158k      0  0:00:01  0:00:01 --:--:--  199k\n"
     ]
    }
   ],
   "source": [
    "!curl 'http://www.gutenberg.org/cache/epub/11/pg11.txt' -o alicesTExtFilename.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\r",
      "\r\n",
      "\r",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r",
      "\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r",
      "\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r",
      "\r\n",
      "with this eBook or online at www.gutenberg.org\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "Title: Alice's Adventures in Wonderland\r",
      "\r\n",
      "\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#display the first few lines\n",
    "!head alicesTExtFilename.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beck',\n",
       " 'global',\n",
       " 'risk',\n",
       " 'management',\n",
       " 'operations',\n",
       " 'congratulations',\n",
       " 'sally',\n",
       " 'kk',\n",
       " 'forwarded',\n",
       " 'by']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#example of a regular expression to detect words in a string. \n",
    "import re\n",
    "line = \"\"\" 0017.2000-01-17.beck\t0\t global risk management operations\t\" congratulations, sally!!!  kk  ----------------------forwarded by kathy kokas/corp/enron on 01/17/2000  08:08 pm---------------------------  from: rick causey 01/17/2000 06:04 pm  sent by: enron announcements  to: all enron worldwide  cc:  subject: global risk management operations  recognizing enron \u0001, s increasing worldwide presence in the wholesale energy  business and the need to insure outstanding internal controls for all of our  risk management activities, regardless of location, a global risk management  operations function has been created under the direction of sally w. beck,  vice president. in this role, sally will report to rick causey, executive  vice president and chief accounting officer.  sally \u0001, s responsibilities with regard to global risk management operations  will mirror those of other recently created enron global functions. in this  role, sally will work closely with all enron geographic regions and wholesale  companies to insure that each entity receives individualized regional support  while also focusing on the following global responsibilities:  1. enhance communication among risk management operations professionals.  2. assure the proliferation of best operational practices around the globe.  3. facilitate the allocation of human resources.  4. provide training for risk management operations personnel.  5. coordinate user requirements for shared operational systems.  6. oversee the creation of a global internal control audit plan for risk  management activities.  7. establish procedures for opening new risk management operations offices  and create key benchmarks for measuring on-going risk controls.  each regional operations team will continue its direct reporting relationship  within its business unit, and will collaborate with sally in the delivery of  these critical items. the houston-based risk management operations team under  sue frusco \u0001, s leadership, which currently supports risk management activities  for south america and australia, will also report directly to sally.  sally retains her role as vice president of energy operations for enron  north america, reporting to the ena office of the chairman. she has been in  her current role over energy operations since 1997, where she manages risk  consolidation and reporting, risk management administration, physical product  delivery, confirmations and cash management for ena \u0001, s physical commodity  trading, energy derivatives trading and financial products trading.  sally has been with enron since 1992, when she joined the company as a  manager in global credit. prior to joining enron, sally had four years  experience as a commercial banker and spent seven years as a registered  securities principal with a regional investment banking firm. she also owned  and managed a retail business for several years.  please join me in supporting sally in this additional coordination role for  global risk management operations.\"\"\"\n",
    "re.findall(r'[a-z]+', line.lower()) [0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dictionaries are a good way to keep track of word counts\n",
    "\n",
    "wordCounts={}\n",
    "defaultdict are slightly more effectice way of doing word counting\n",
    "\n",
    "One way to do word counting but not best. A defaultdict is like a regular dictionary, except that when you try to look up a key it doesn’t contain, it first adds a value for it using a zero-argument function you provided when you created it. In order to use defaultdicts, you have to import them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 7\n",
      "accounting 1\n",
      "activities 3\n",
      "additional 1\n",
      "administration 1\n",
      "all 3\n",
      "allocation 1\n",
      "also 3\n",
      "america 2\n",
      "among 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here is an example of wordcounting with a defaultdict (dictionary structure with a nice \n",
    "# default behaviours when a key does not exist in the dictionary\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "line = \"\"\" 0017.2000-01-17.beck\t0\t global risk management operations\t\" congratulations, sally!!!  kk  ----------------------forwarded by kathy kokas/corp/enron on 01/17/2000  08:08 pm---------------------------  from: rick causey 01/17/2000 06:04 pm  sent by: enron announcements  to: all enron worldwide  cc:  subject: global risk management operations  recognizing enron \u0001, s increasing worldwide presence in the wholesale energy  business and the need to insure outstanding internal controls for all of our  risk management activities, regardless of location, a global risk management  operations function has been created under the direction of sally w. beck,  vice president. in this role, sally will report to rick causey, executive  vice president and chief accounting officer.  sally \u0001, s responsibilities with regard to global risk management operations  will mirror those of other recently created enron global functions. in this  role, sally will work closely with all enron geographic regions and wholesale  companies to insure that each entity receives individualized regional support  while also focusing on the following global responsibilities:  1. enhance communication among risk management operations professionals.  2. assure the proliferation of best operational practices around the globe.  3. facilitate the allocation of human resources.  4. provide training for risk management operations personnel.  5. coordinate user requirements for shared operational systems.  6. oversee the creation of a global internal control audit plan for risk  management activities.  7. establish procedures for opening new risk management operations offices  and create key benchmarks for measuring on-going risk controls.  each regional operations team will continue its direct reporting relationship  within its business unit, and will collaborate with sally in the delivery of  these critical items. the houston-based risk management operations team under  sue frusco \u0001, s leadership, which currently supports risk management activities  for south america and australia, will also report directly to sally.  sally retains her role as vice president of energy operations for enron  north america, reporting to the ena office of the chairman. she has been in  her current role over energy operations since 1997, where she manages risk  consolidation and reporting, risk management administration, physical product  delivery, confirmations and cash management for ena \u0001, s physical commodity  trading, energy derivatives trading and financial products trading.  sally has been with enron since 1992, when she joined the company as a  manager in global credit. prior to joining enron, sally had four years  experience as a commercial banker and spent seven years as a registered  securities principal with a regional investment banking firm. she also owned  and managed a retail business for several years.  please join me in supporting sally in this additional coordination role for  global risk management operations.\"\"\"\n",
    "wordCounts=defaultdict(int)\n",
    "for word in re.findall(r'[a-z]+', line.lower()):\n",
    "    #if word in [\"a\"]:\n",
    "        #print word,\"\\n\"\n",
    "    wordCounts[word] += 1\n",
    "for key in sorted(wordCounts)[0:10]:\n",
    "    print (key, wordCounts[key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 690, 'i': 543, 'alice': 403, 'e': 29, 'c': 6})\n"
     ]
    }
   ],
   "source": [
    "#HW1.1.1 How many times does the word alice occur in the book?\n",
    "import re\n",
    "'''\n",
    "import csv\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "with open(\"alicesTExtFilename.txt\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        counter.update(row)\n",
    "print(counter['it'])\n",
    "\n",
    "\n",
    "%%%%%\n",
    "\n",
    "file=open(\"alicesTExtFilename.txt\",\"r+\")\n",
    "wordcount={}\n",
    "for word in file.read().split():\n",
    "    if word not in wordcount:\n",
    "        wordcount[word] = 1\n",
    "    else:\n",
    "        wordcount[word] += 1\n",
    "print (word,wordcount)\n",
    "file.close();\n",
    "'''\n",
    "wanted = \"alice\"\n",
    "cnt = Counter()\n",
    "words = re.findall('\\w+', open('alicesTExtFilename.txt').read().lower())\n",
    "for word in words:\n",
    "    if word in wanted:\n",
    "        cnt[word] += 1\n",
    "print(cnt)\n",
    "\n",
    "\n",
    "# There are 403 Alice in this textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. HW1.2 Command Line Map Reduce Framework\n",
    "\n",
    "Back to Table of Contents\n",
    "\n",
    "Read through the provided mapreduce shell script (pWordCount.sh) provided below and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. Run the shell without any arguments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pWordCount.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pWordCount.sh\n",
    "#!/bin/bash\n",
    "## pWordCount.sh\n",
    "## Author: James G. Shanahan\n",
    "## Usage: pWordCount.sh m wordlist testFile.txt\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##       inputFile = a text input file\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "\n",
    "\n",
    "if [ $# -eq 0 ]\n",
    "  then\n",
    "    echo \"No arguments supplied\"\n",
    "    echo \"To run use\"  \n",
    "    echo \"     pWordCount.sh m wordlist inputFile\"\n",
    "    echo \"Input:\"\n",
    "    echo \"      number of processes (maps), e.g., 4\"\n",
    "    echo \"      wordlist = a space-separated list of words in quotes, e.g., 'the and of'\"\n",
    "    echo \"      inputFile = a text input file\"\n",
    "    exit\n",
    "fi\n",
    "    \n",
    "    \n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a text file  \n",
    "data=\"alicesTExtFilename.txt\"\n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "    \n",
    "## sorting keys (words) by alphbetaical order      \n",
    "    \n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles | sort -k1,1 > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "## pWordCount.sh\r\n",
      "## Author: James G. Shanahan\r\n",
      "## Usage: pWordCount.sh m wordlist testFile.txt\r\n",
      "## Input:\r\n",
      "##       m = number of processes (maps), e.g., 4\r\n",
      "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\r\n",
      "##       inputFile = a text input file\r\n",
      "##\r\n",
      "## Instructions: Read this script and its comments closely.\r\n",
      "##               Do your best to understand the purpose of each command,\r\n",
      "##               and focus on how arguments are supplied to mapper.py/reducer.py,\r\n",
      "##               as this will determine how the python scripts take input.\r\n",
      "##               When you are comfortable with the unix code below,\r\n",
      "##               answer the questions on the LMS for HW1 about the starter code.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "if [ $# -eq 0 ]\r\n",
      "  then\r\n",
      "    echo \"No arguments supplied\"\r\n",
      "    echo \"To run use\"  \r\n",
      "    echo \"     pWordCount.sh m wordlist inputFile\"\r\n",
      "    echo \"Input:\"\r\n",
      "    echo \"      number of processes (maps), e.g., 4\"\r\n",
      "    echo \"      wordlist = a space-separated list of words in quotes, e.g., 'the and of'\"\r\n",
      "    echo \"      inputFile = a text input file\"\r\n",
      "    exit\r\n",
      "fi\r\n",
      "    \r\n",
      "    \r\n",
      "## collect user input\r\n",
      "m=$1 ## the number of parallel processes (maps) to run\r\n",
      "\r\n",
      "wordlist=$2 ## if set to \"*\", then all words are used\r\n",
      "\r\n",
      "## a text file  \r\n",
      "data=\"alicesTExtFilename.txt\"\r\n",
      "\r\n",
      "## 'wc' determines the number of lines in the data\r\n",
      "## 'perl -pe' regex strips the piped wc output to a number\r\n",
      "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\r\n",
      "\r\n",
      "## determine the lines per chunk for the desired number of processes\r\n",
      "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\r\n",
      "\r\n",
      "## split the original file into chunks by line\r\n",
      "split -l $linesinchunk $data $data.chunk.\r\n",
      "\r\n",
      "## assign python mappers (mapper.py) to the chunks of data\r\n",
      "## and emit their output to temporary files\r\n",
      "for datachunk in $data.chunk.*; do\r\n",
      "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\r\n",
      "    ####\r\n",
      "    ####\r\n",
      "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\r\n",
      "    ####\r\n",
      "    ####\r\n",
      "done\r\n",
      "\r\n",
      "## wait for the mappers to finish their work\r\n",
      "wait\r\n",
      "    \r\n",
      "## sorting keys (words) by alphbetaical order      \r\n",
      "    \r\n",
      "\r\n",
      "## 'ls' makes a list of the temporary count files\r\n",
      "## 'perl -pe' regex replaces line breaks with spaces\r\n",
      "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\r\n",
      "\r\n",
      "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\r\n",
      "####\r\n",
      "####\r\n",
      "./reducer.py $countfiles | sort -k1,1 > $data.output\r\n",
      "####\r\n",
      "####\r\n",
      "\r\n",
      "## clean up the data chunks and temporary count files\r\n",
      "rm $data.chunk.*"
     ]
    }
   ],
   "source": [
    "!cat pWordCount.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x pWordCount.sh\n",
    "#run the shell without any arguments\n",
    "!./pWordCount.sh 4 \"Alice\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##3. HW1.3 WordCount via Command Line Map Reduce Framework ##\n",
    "\n",
    "## MAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Given a file and list of words, read lines and count occurrences of words\n",
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "filename = sys.argv[1]\n",
    "\n",
    "## Words in the word list are space delimited\n",
    "wordlist = sys.argv[2].lower().split(' ')\n",
    "counts = {}\n",
    "\n",
    "\n",
    "with open (filename, \"rU\") as myfile:\n",
    "    for text in myfile.readlines():  \n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word.lower() in wordlist:\n",
    "                try:\n",
    "                    counts[word.lower()] += 1\n",
    "                except:\n",
    "                    counts[word.lower()] = 1\n",
    "\n",
    "for word in counts:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Reduce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python2\n",
    "import sys\n",
    "counts = {}\n",
    "\n",
    "for intermediate_file in sys.argv:\n",
    "    with open(intermediate_file, 'rU') as infile:\n",
    "        # intermediate files are word <tab> count per line\n",
    "        for line in infile.readlines():\n",
    "            word_count = line.split('\\t')\n",
    "            if len(word_count) == 2:\n",
    "                try:\n",
    "                    counts[word_count[0]] += int(word_count[1])\n",
    "                except KeyError:\n",
    "                    counts[word_count[0]] = int(word_count[1])\n",
    "for word in counts:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t385\r\n",
      "the\t1797\r\n",
      "we\t28\r\n"
     ]
    }
   ],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x reducer.py\n",
    "\n",
    "# Set the execution permissions of the pWordCount.sh bash shell script\n",
    "!chmod a+x pWordCount.sh\n",
    "\n",
    "!./pWordCount.sh 4 \"Alice the we\" \n",
    "!cat *.output\n",
    "\n",
    "### For example, I input \"Alice\", \"the\",  \"we\", it sorted as alphabetical order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Below is  HW1.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pWordCount_n.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pWordCount_n.sh\n",
    "#!/bin/bash\n",
    "## pWordCount.sh\n",
    "## Author: James G. Shanahan\n",
    "## Usage: pWordCount.sh m wordlist testFile.txt\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##       inputFile = a text input file\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "\n",
    "\n",
    "if [ $# -eq 0 ]\n",
    "  then\n",
    "    echo \"No arguments supplied\"\n",
    "    echo \"To run use\"  \n",
    "    echo \"     pWordCount.sh m wordlist inputFile\"\n",
    "    echo \"Input:\"\n",
    "    echo \"      number of processes (maps), e.g., 4\"\n",
    "    echo \"      wordlist = a space-separated list of words in quotes, e.g., 'the and of'\"\n",
    "    echo \"      inputFile = a text input file\"\n",
    "    exit\n",
    "fi\n",
    "    \n",
    "    \n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a text file  \n",
    "data=\"alicesTExtFilename.txt\"\n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper_n.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "    \n",
    "## sorting keys (words) by alphbetaical order      \n",
    "    \n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer_n.py $countfiles | grep -e \"^[[:upper:]]\" | sort -k1,1 > $data.outputU\n",
    "./reducer_n.py $countfiles | grep -e \"^[[:lower:]]\" | sort -k1,1 > $data.outputL\n",
    "\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pWordCount_n.sh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_n.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_n.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Given a file and list of words, read lines and count occurrences of words\n",
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "filename = sys.argv[1]\n",
    "\n",
    "## Words in the word list are space delimited\n",
    "wordlist = sys.argv[2].split(' ')\n",
    "counts = {}\n",
    "\n",
    "\n",
    "with open (filename, \"rU\") as myfile:\n",
    "    for text in myfile.readlines():  \n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in wordlist:\n",
    "                try:\n",
    "                    counts[word] += 1\n",
    "                except:\n",
    "                    counts[word] = 1\n",
    "\n",
    "for word in counts:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x mapper_n.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_n.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_n.py\n",
    "#!/usr/bin/python2\n",
    "import sys\n",
    "counts_U = {}\n",
    "counts_L = {}\n",
    "\n",
    "for intermediate_file in sys.argv:\n",
    "    with open(intermediate_file, 'rU') as infile:\n",
    "        # intermediate files are word <tab> count per line\n",
    "        for line in infile.readlines():\n",
    "            word_count = line.split('\\t')\n",
    "            # starting with an uppercase\n",
    "            word = word_count[0]\n",
    "            if len(word_count) == 2 and (word[0] == word[0].upper()):\n",
    "                try:\n",
    "                    counts_U[word_count[0]] += int(word_count[1])\n",
    "                except KeyError:\n",
    "                    counts_U[word_count[0]] = int(word_count[1])\n",
    "                    \n",
    "            elif len(word_count) == 2:\n",
    "                try:\n",
    "                    counts_L[word_count[0]] += int(word_count[1])\n",
    "                except KeyError:\n",
    "                    counts_L[word_count[0]] = int(word_count[1]) \n",
    "                \n",
    "for word in counts_U:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts_U[word]))\n",
    "    \n",
    "for word in counts_L:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts_L[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Case\n",
      "Alice\t385\n",
      "How\t12\n",
      "The\t104\n",
      "We\t3\n",
      "Lower Case\n",
      "dog\t2\n",
      "the\t1680\n",
      "we\t24\n"
     ]
    }
   ],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x reducer_n.py\n",
    "\n",
    "# Set the execution permissions of the pWordCount.sh bash shell script\n",
    "!chmod a+x pWordCount_n.sh\n",
    "\n",
    "!./pWordCount_n.sh 4 \"Alice the we The We How dog\" \n",
    "print(\"Upper Case\")\n",
    "!cat *.outputU\n",
    "\n",
    "print(\"Lower Case\")\n",
    "!cat *.outputL\n",
    "\n",
    "### For example, I input \"Alice\", \"the\",  \"we\", \"The\", \"We\", \"How\", \"dog\" \n",
    "### they are sorted as alphabetical order for Upper case begin (saved .outputU file),\n",
    "### and Lower case begin (saved .outputL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This example illustrates and compares the bias-variance decomposition \n",
    "## of the expected mean squared error of a single estimator against \n",
    "## a bagging ensemble. In regression, the expected mean squared error of \n",
    "## an estimator can be decomposed in terms of bias, variance and noise.\n",
    "## On average over datasets of the regression problem, the bias term\n",
    "## measures the average amount by which the predictions of the estimator\n",
    "## differ from the predictions of the best possible estimator for the\n",
    "## problem (i.e., the Bayes model). \n",
    "## Following generated figures show that the larger the variance, \n",
    "## the more sensitive are the predictions for x to small changes \n",
    "## in the training set. The bias term corresponds to the difference \n",
    "## between the average prediction of the estimator (in cyan) and\n",
    "## the best possible model (in dark blue). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shihyu/anaconda3/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/home/shihyu/anaconda3/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree: 0.0255 (error) = 0.0003 (bias^2)  + 0.0152 (var) + 0.0098 (noise)\n",
      "Bagging(Tree): 0.0196 (error) = 0.0004 (bias^2)  + 0.0092 (var) + 0.0098 (noise)\n"
     ]
    }
   ],
   "source": [
    "#####Below example came from scikit, but we modify a little bit as using \n",
    "## 4-th power instead of square error measures. But, still can observe \n",
    "## varaince and bias issue. \n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Settings\n",
    "n_repeat = 50       # Number of iterations for computing expectations\n",
    "n_train = 50        # Size of the training set\n",
    "n_test = 1000       # Size of the test set\n",
    "noise = 0.1         # Standard deviation of the noise\n",
    "np.random.seed(0)\n",
    "\n",
    "# Change this for exploring the bias-variance decomposition of other\n",
    "# estimators. This should work well for estimators with high variance (e.g.,\n",
    "# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n",
    "# linear models).\n",
    "estimators = [(\"Tree\", DecisionTreeRegressor()),\n",
    "              (\"Bagging(Tree)\", BaggingRegressor(DecisionTreeRegressor()))]\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise, n_repeat=1):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X)\n",
    "\n",
    "    if n_repeat == 1:\n",
    "        y = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "    else:\n",
    "        y = np.zeros((n_samples, n_repeat))\n",
    "\n",
    "        for i in range(n_repeat):\n",
    "            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(n_repeat):\n",
    "    X, y = generate(n_samples=n_train, noise=noise)\n",
    "    X_train.append(X)\n",
    "    y_train.append(y)\n",
    "\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\n",
    "\n",
    "# Loop over estimators to compare\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros((n_test, n_repeat))\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        estimator.fit(X_train[i], y_train[i])\n",
    "        y_predict[:, i] = estimator.predict(X_test)\n",
    "\n",
    "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "    y_error = np.zeros(n_test)\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        for j in range(n_repeat):\n",
    "            y_error += (y_test[:, j] - y_predict[:, i]) ** 4\n",
    "\n",
    "    y_error /= (n_repeat * n_repeat)\n",
    "\n",
    "    y_noise = np.var(y_test, axis=1)\n",
    "    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 4\n",
    "    y_var = np.var(y_predict, axis=1)\n",
    "\n",
    "    print(\"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \"\n",
    "          \" + {3:.4f} (var) + {4:.4f} (noise)\".format(name,\n",
    "                                                      np.mean(y_error),\n",
    "                                                      np.mean(y_bias),\n",
    "                                                      np.mean(y_var),\n",
    "                                                      np.mean(y_noise)))\n",
    "\n",
    "    # Plot figures\n",
    "    plt.subplot(2, n_estimators, n + 1)\n",
    "    plt.plot(X_test, f(X_test), \"b\", label=\"$f(x)$\")\n",
    "    plt.plot(X_train[0], y_train[0], \".b\", label=\"LS ~ $y = f(x)+noise$\")\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        if i == 0:\n",
    "            plt.plot(X_test, y_predict[:, i], \"r\", label=\"$\\^y(x)$\")\n",
    "        else:\n",
    "            plt.plot(X_test, y_predict[:, i], \"r\", alpha=0.05)\n",
    "\n",
    "    plt.plot(X_test, np.mean(y_predict, axis=1), \"c\",\n",
    "             label=\"$\\mathbb{E}_{LS} \\^y(x)$\")\n",
    "\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.title(name)\n",
    "\n",
    "    if n == 0:\n",
    "        plt.legend(loc=\"upper left\", prop={\"size\": 11})\n",
    "\n",
    "    plt.subplot(2, n_estimators, n_estimators + n + 1)\n",
    "    plt.plot(X_test, y_error, \"r\", label=\"$error(x)$\")\n",
    "    plt.plot(X_test, y_bias, \"b\", label=\"$bias^2(x)$\"),\n",
    "    plt.plot(X_test, y_var, \"g\", label=\"$variance(x)$\"),\n",
    "    plt.plot(X_test, y_noise, \"c\", label=\"$noise(x)$\")\n",
    "\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.ylim([0, 0.1])\n",
    "\n",
    "    if n == 0:\n",
    "        plt.legend(loc=\"upper left\", prop={\"size\": 11})\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
