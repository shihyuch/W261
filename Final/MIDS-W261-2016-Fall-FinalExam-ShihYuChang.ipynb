{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.11 (default, Dec  6 2015 18:08:32)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ['SPARK_HOME'] = '/home/cloudera/Downloads/spark-2.0.1-bin-hadoop2.6/'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Ttest_indResult(statistic=1.4142505069117979, pvalue=0.15728991985619109),\n",
       " Ttest_indResult(statistic=1.3867952958906298, pvalue=0.1655057375790788))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import scipy.stats\n",
    "\n",
    "control=[.5, .5, 3, 3, 3, 3, 3, 3, 4]\n",
    "control.extend([0]*(100000-len(control)))\n",
    "control=np.asarray(control)\n",
    "\n",
    "# treatment=[ 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "treatment=[1.5, .5, 0, 3, 4]\n",
    "treatment.extend([0]*(100000-len(treatment)))\n",
    "treatment=np.asarray(treatment)\n",
    "\n",
    "#nb 2-sided test\n",
    "scipy.stats.ttest_ind(control, treatment), scipy.stats.ttest_ind(control.astype(bool), treatment.astype(bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 6))]\n"
     ]
    }
   ],
   "source": [
    "RDD1 = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "RDD2 = sc.parallelize([(3, 9), (3, 6)])\n",
    "print RDD1.join(RDD2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawSales = sc.textFile('beerSales.txt', use_unicode=False)\n",
    "sales = (rawSales\n",
    "         .map(lambda x: x.split('\\t'))\n",
    "         .filter(lambda x: x[0] != 'Week')\n",
    "         .map(lambda x: float(x[5]))\n",
    "        )\n",
    "\n",
    "sales18Pk = sales.collect()\n",
    "\n",
    "# Mean model is just mean of cases18pk sales\n",
    "meanModel = sum(sales18Pk)/len(sales18Pk)\n",
    "mm = sc.broadcast(meanModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.44422333\n"
     ]
    }
   ],
   "source": [
    "mape = (sales\n",
    "        .map(lambda x: 100*abs(x - mm.value) / x)\n",
    "       )\n",
    "\n",
    "print mape.sum() / mape.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0844994130751715, 4.584967478670572, 4.248495242049359, 3.9512437185814275, 4.1588830833596715]\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "logSales = (sales\n",
    "            .map(lambda x: log(x))\n",
    "           )\n",
    "print logSales.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logSales18Pk = logSales.collect()\n",
    "meanModelLog = sum(logSales18Pk)/len(logSales18Pk)\n",
    "mml = sc.broadcast(meanModelLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.5849342896\n"
     ]
    }
   ],
   "source": [
    "mapeLog = (logSales\n",
    "           .map(lambda x: 100*abs(x - mml.value) / x)\n",
    "          )\n",
    "\n",
    "print mapeLog.sum() / mapeLog.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(6.08449941308, [2.99473177322,2.64617479738,2.72063731661]), LabeledPoint(4.58496747867, [2.99473177322,2.92584614609,2.72063731661]), LabeledPoint(4.24849524205, [2.99473177322,2.92584614609,2.62972823433]), LabeledPoint(3.95124371858, [2.99473177322,2.92584614609,2.55178617863]), LabeledPoint(4.15888308336, [2.99473177322,2.92584614609,2.5771819259]), LabeledPoint(4.27666611902, [2.99473177322,2.92584614609,2.72063731661]), LabeledPoint(3.85014760171, [2.99473177322,2.92584614609,2.63332665491]), LabeledPoint(4.44265125649, [3.00071981507,2.93012651646,2.66861613186]), LabeledPoint(4.07753744391, [3.00171434523,2.93119375242,2.62684014568]), LabeledPoint(4.14313472639, [3.00221123965,2.93119375242,2.67414864943])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "from math import log\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [x for x in line.split('\\t')]\n",
    "    if values[0] != 'Week':\n",
    "        return LabeledPoint(log(float(values[5])), [log(float(values[1])), log(float(values[2])), log(float(values[3]))])\n",
    "\n",
    "data = sc.textFile('beerSales.txt')\n",
    "parsedData = data.map(parsePoint).filter(lambda x: x != None)\n",
    "\n",
    "print parsedData.take(10)\n",
    "\n",
    "# Build the model\n",
    "model = LinearRegressionWithSGD.train(parsedData, intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24146308056\n"
     ]
    }
   ],
   "source": [
    "valuesAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "\n",
    "mape18 = valuesAndPreds.map(lambda (v, p): 100 * abs(v - p) / v).reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "print mape18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
