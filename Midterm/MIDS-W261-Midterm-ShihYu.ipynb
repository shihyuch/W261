{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Fall, 2016, Midterm\n",
    "# Name: Shih Yu Chang\n",
    "# Email: sychang@ischool.berkeley.edu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing kltext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile kltext.txt\n",
    "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\n",
    "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise similarity using K-L divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\r\n",
      "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."
     ]
    }
   ],
   "source": [
    "!cat kltext.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "import numpy as np\n",
    "class kldivergence(MRJob):\n",
    "    \n",
    "    # process each string character by character\n",
    "    # the relative frequency of each character emitting Pr(character|str)\n",
    "    # for input record 1.abcbe\n",
    "    # emit \"a\"    [1, 0.2]\n",
    "    # emit \"b\"    [1, 0.4] etc...\n",
    "    def mapper1(self, _, line):\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        for l in letter_list:\n",
    "            if count.has_key(l):\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            yield key, [index, count[key]*1.0/len(letter_list)]\n",
    "\n",
    "    # on a component i calculate (e.g., \"b\")\n",
    "   \n",
    "    #  (P(i) log (P(i) / Q(i))\n",
    "    #\n",
    "    def reducer1(self, key, values):\n",
    "        p = 0\n",
    "        q = 0\n",
    "        for v in values:\n",
    "            if v[0] == 1:  #String 1\n",
    "                p = v[1]\n",
    "            else:          # String 2\n",
    "                q = v[1]\n",
    "        sim = np.log(p/q) * p\n",
    "        \n",
    "        yield None, sim\n",
    "\n",
    "\n",
    "    #Aggegate components            \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum = kl_sum + value\n",
    "        yield \"KLDivergence\", kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [self.mr(mapper=self.mapper1,\n",
    "                        reducer=self.reducer1),\n",
    "                \n",
    "                self.mr(reducer=self.reducer2)\n",
    "               \n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KLDivergence', 0.08088278445318145)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from mrjob.job import MRJob\n",
    "from kldivergence import kldivergence\n",
    "\n",
    "#dont forget to save kltext.txt (see earlier cell)\n",
    "mr_job = kldivergence(args=['kltext.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence_smooth.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence_smooth.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "import numpy as np\n",
    "class kldivergence_smooth(MRJob):\n",
    "    \n",
    "    # process each string character by character\n",
    "    # the relative frequency of each character emitting Pr(character|str)\n",
    "    # for input record 1.abcbe\n",
    "    # emit \"a\"    [1, (1+1)/(5+24)]\n",
    "    # emit \"b\"    [1, (2+1)/(5+24) etc...\n",
    "    def mapper1(self, _, line):\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        \n",
    "        # (ni+1)/(n+24)\n",
    "        \n",
    "        for l in letter_list:\n",
    "            if count.has_key(l):\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            yield key, [index, (count[key] + 1) * 1.0 / (len(letter_list) + 24)]\n",
    "\n",
    "    \n",
    "    def reducer1(self, key, values):\n",
    "        p = 0\n",
    "        q = 0\n",
    "        for v in values:\n",
    "            if v[0] == 1:\n",
    "                p = v[1]\n",
    "            else:\n",
    "                q = v[1]\n",
    "                \n",
    "        sim = np.log(p/q) * p\n",
    "        \n",
    "        yield None, sim\n",
    "\n",
    "    # Aggregate components             \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum = kl_sum + value\n",
    "        yield \"KLDivergence\", kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [self.mr(mapper=self.mapper1,\n",
    "                        reducer=self.reducer1),\n",
    "                self.mr(reducer=self.reducer2)\n",
    "               \n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence_smooth.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KLDivergence', 0.06726997279170038)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from kldivergence_smooth import kldivergence_smooth\n",
    "mr_job = kldivergence_smooth(args=['kltext.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=3    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, mapper=self.mapper,combiner = self.combiner,reducer=self.reducer)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        print \"Current path:\", os.path.dirname(os.path.realpath(__file__))\n",
    "        \n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        #open('Centroids.txt', 'w').close()\n",
    "        \n",
    "        print \"Centroids: \", self.centroid_points\n",
    "        \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        yield int(MinDist(D,self.centroid_points)), (D[0],D[1],1)\n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        sumx = sumy = num = 0\n",
    "        for x,y,n in inputdata:\n",
    "            num = num + n\n",
    "            sumx = sumx + x\n",
    "            sumy = sumy + y\n",
    "        yield idx,(sumx,sumy,num)\n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        num = [0]*self.k \n",
    "        for i in range(self.k):\n",
    "            centroids.append([0,0])\n",
    "        for x, y, n in inputdata:\n",
    "            num[idx] = num[idx] + n\n",
    "            centroids[idx][0] = centroids[idx][0] + x\n",
    "            centroids[idx][1] = centroids[idx][1] + y\n",
    "        centroids[idx][0] = centroids[idx][0]/num[idx]\n",
    "        centroids[idx][1] = centroids[idx][1]/num[idx]\n",
    "\n",
    "        yield idx,(centroids[idx][0],centroids[idx][1])\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration1:\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003230.821415/job_local_dir/0/mapper/0\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003230.821415/job_local_dir/0/mapper/1\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "0 [-3.344726378997632, 0.3375985510805805]\n",
      "1 [5.379067911319121, 0.15446805295171434]\n",
      "2 [0.24288276270220563, 5.350519186138149]\n",
      "\n",
      "\n",
      "iteration2:\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003231.043216/job_local_dir/0/mapper/0\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003231.043216/job_local_dir/0/mapper/1\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "0 [-3.344726378997632, 0.3375985510805805]\n",
      "1 [5.379067911319121, 0.15446805295171434]\n",
      "2 [0.24288276270220563, 5.350519186138149]\n",
      "\n",
      "\n",
      "iteration3:\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003231.267926/job_local_dir/0/mapper/0\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003231.267926/job_local_dir/0/mapper/1\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "0 [-3.344726378997632, 0.3375985510805805]\n",
      "1 [5.379067911319121, 0.15446805295171434]\n",
      "2 [0.24288276270220563, 5.350519186138149]\n",
      "\n",
      "\n",
      "iteration4:\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003231.462295/job_local_dir/0/mapper/0\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003231.462295/job_local_dir/0/mapper/1\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "0 [-3.344726378997632, 0.3375985510805805]\n",
      "1 [5.379067911319121, 0.15446805295171434]\n",
      "2 [0.24288276270220563, 5.350519186138149]\n",
      "\n",
      "\n",
      "iteration5:\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003231.700523/job_local_dir/0/mapper/0\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003231.700523/job_local_dir/0/mapper/1\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "0 [-3.344726378997632, 0.3375985510805805]\n",
      "1 [5.379067911319121, 0.15446805295171434]\n",
      "2 [0.24288276270220563, 5.350519186138149]\n",
      "\n",
      "\n",
      "iteration6:\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003231.903864/job_local_dir/0/mapper/0\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003231.903864/job_local_dir/0/mapper/1\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "0 [-3.344726378997632, 0.3375985510805805]\n",
      "1 [5.379067911319121, 0.15446805295171434]\n",
      "2 [0.24288276270220563, 5.350519186138149]\n",
      "\n",
      "\n",
      "iteration7:\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003232.202383/job_local_dir/0/mapper/0\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003232.202383/job_local_dir/0/mapper/1\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "0 [-3.344726378997632, 0.3375985510805805]\n",
      "1 [5.379067911319121, 0.15446805295171434]\n",
      "2 [0.24288276270220563, 5.350519186138149]\n",
      "\n",
      "\n",
      "iteration8:\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003232.412122/job_local_dir/0/mapper/0\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003232.412122/job_local_dir/0/mapper/1\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "0 [-3.344726378997632, 0.3375985510805805]\n",
      "1 [5.379067911319121, 0.15446805295171434]\n",
      "2 [0.24288276270220563, 5.350519186138149]\n",
      "\n",
      "\n",
      "iteration9:\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003232.602461/job_local_dir/0/mapper/0\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003232.602461/job_local_dir/0/mapper/1\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "0 [-3.344726378997632, 0.3375985510805805]\n",
      "1 [5.379067911319121, 0.15446805295171434]\n",
      "2 [0.24288276270220563, 5.350519186138149]\n",
      "\n",
      "\n",
      "iteration10:\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003232.811920/job_local_dir/0/mapper/0\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "Current path: /tmp/Kmeans.cloudera.20161020.003232.811920/job_local_dir/0/mapper/1\n",
      "Centroids:  [[0.0, 0.0], [6.0, 3.0], [3.0, 6.0]]\n",
      "0 [-3.344726378997632, 0.3375985510805805]\n",
      "1 [5.379067911319121, 0.15446805295171434]\n",
      "2 [0.24288276270220563, 5.350519186138149]\n",
      "\n",
      "\n",
      "Centroids\n",
      "\n",
      "[[-3.344726378997632, 0.3375985510805805], [5.379067911319121, 0.15446805295171434], [0.24288276270220563, 5.350519186138149]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import random, array\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "mr_job = MRKmeans(args=['Kmeandata.csv', '--file', 'Centroids.txt'])\n",
    "\n",
    "#Geneate initial centroids\n",
    "centroid_points = [[0,0],[6,3],[3,6]]\n",
    "k = 3\n",
    "with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# Update centroids iteratively\n",
    "for i in range(10):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i+1)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            centroid_points[key] = value\n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "print \"Centroids\\n\"\n",
    "print centroid_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5932559652\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from numpy import argmin, array, random\n",
    "\n",
    "#Euclidean norm\n",
    "def norm(x):\n",
    "    return (x[0]**2 + x[1]**2)**0.5\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def smallestDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff**2\n",
    "    \n",
    "    distances = (diffsq.sum(axis = 1))**0.5\n",
    "    # Get the nearest centroid for each instance\n",
    "    min_idx = argmin(distances)\n",
    "    return distances[min_idx]\n",
    "\n",
    "data = []\n",
    "\n",
    "centroids = [[-4.5,0.0],[4.5,0.0],[0.0,4.5]]\n",
    "\n",
    "num = 0.0\n",
    "den = 0.0\n",
    "\n",
    "with open('Kmeandata.csv', 'r') as infile:\n",
    "    for line in csv.reader(infile):\n",
    "        point = [float(line[0]), float(line[1])]\n",
    "        weight = 1/norm(point)\n",
    "        num += smallestDist(point, centroids) * weight\n",
    "        den += weight\n",
    "\n",
    "print num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cloudera\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
