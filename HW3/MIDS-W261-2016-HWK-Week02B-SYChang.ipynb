{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "#### Assignement:  week \\#2\n",
    "#### Shih Yu Chang\n",
    "### Due: 2016-09-13, 8AM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *HW2.0.*\n",
    "- What is a race condition in the context of parallel computation? Give an example.\n",
    "- What is MapReduce?\n",
    "- How does it differ from Hadoop?\n",
    "- Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A race condition refers to the ssituation where the output is dependent on the sequence or timing of other uncontrollable events. For example, when a few paralle threads have interaction with some common resource, due to the lack of coordination and synchronization, the execution can cause invalid state of the resource, which can randomly affect the result of each thread. \n",
    "\n",
    "For example, consider the following logic, where B is a global counter variable:\n",
    "\n",
    "temp = B\n",
    "temp = temp + 1\n",
    "B = temp\n",
    "It will increase the value of B by one. If it is run twice sequentially, B will be increased by two. However, if two threads are running it at the same time, there is a chance that B will only be increased by one, not two.\n",
    "\n",
    "- MapReduce is a programming framework for processing large data sets with a parallel, distributed algorithm on a cluster. It has two phases:\n",
    " - Map(): performs filtering and sorting\n",
    " - Reduce(): performs a summary operation\n",
    " \n",
    " \n",
    "- Hadoop is an open source project which provides an implementation of the MapReduce functionality. It has two components:\n",
    " - HDFS: Hadoop distributed file system (Storage)\n",
    " - MapReduce implementation (Calculation)\n",
    " \n",
    "Therefore, MapReduce is a calculating component of Hadoop \n",
    "\n",
    "- Hadoop is based on Functional Programing languages. key features of functional languages include (1) such language can accept other functions as arguments; (2) it is a declarative programming paradigm, which means programming is done with expressions. \n",
    " - Example is given below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count alphabeta numbers in each word of sentence: \n",
      "{'and': 3, 'both': 4, 'Run': 3, 'Multinomial': 11, 'Bernoulli': 9, 'Naive': 5, 'algorithms': 10, 'Bayes': 5, 'the': 3}\n"
     ]
    }
   ],
   "source": [
    "# function to count character in a word\n",
    "def char_count(word):\n",
    "    return len(word)\n",
    "\n",
    "# apply the function on a sentence\n",
    "sentence = 'Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms'\n",
    "print('Count alphabeta numbers in each word of sentence: ')\n",
    "print({word:char_count(word) for word in sentence.split()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1.1 Lower case and upper case sort. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  163k  100  163k    0     0  25509      0  0:00:06  0:00:06 --:--:--  107k\n"
     ]
    }
   ],
   "source": [
    "!curl 'http://www.gutenberg.org/cache/epub/11/pg11.txt' -o alicesTExtFilename.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pWordCount_n.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pWordCount_n.sh\n",
    "#!/bin/bash\n",
    "## pWordCount.sh\n",
    "## Author: James G. Shanahan\n",
    "## Usage: pWordCount.sh m wordlist testFile.txt\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##       inputFile = a text input file\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "\n",
    "\n",
    "if [ $# -eq 0 ]\n",
    "  then\n",
    "    echo \"No arguments supplied\"\n",
    "    echo \"To run use\"  \n",
    "    echo \"     pWordCount.sh m wordlist inputFile\"\n",
    "    echo \"Input:\"\n",
    "    echo \"      number of processes (maps), e.g., 4\"\n",
    "    echo \"      wordlist = a space-separated list of words in quotes, e.g., 'the and of'\"\n",
    "    echo \"      inputFile = a text input file\"\n",
    "    exit\n",
    "fi\n",
    "    \n",
    "    \n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a text file  \n",
    "data=\"alicesTExtFilename.txt\"\n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper_n.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "    \n",
    "## sorting keys (words) by alphbetaical order      \n",
    "    \n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer_n.py $countfiles | grep -e \"^[[:upper:]]\" | sort -k1,1 > $data.outputU\n",
    "./reducer_n.py $countfiles | grep -e \"^[[:lower:]]\" | sort -k1,1 > $data.outputL\n",
    "\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "rm $data.chunk.*\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_n.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_n.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Given a file and list of words, read lines and count occurrences of words\n",
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "filename = sys.argv[1]\n",
    "\n",
    "## Words in the word list are space delimited\n",
    "wordlist = sys.argv[2].split(' ')\n",
    "counts = {}\n",
    "\n",
    "\n",
    "with open (filename, \"rU\") as myfile:\n",
    "    for text in myfile.readlines():  \n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in wordlist:\n",
    "                try:\n",
    "                    counts[word] += 1\n",
    "                except:\n",
    "                    counts[word] = 1\n",
    "\n",
    "for word in counts:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts[word]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_n.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_n.py\n",
    "#!/usr/bin/python2\n",
    "import sys\n",
    "counts_U = {}\n",
    "counts_L = {}\n",
    "\n",
    "for intermediate_file in sys.argv:\n",
    "    with open(intermediate_file, 'rU') as infile:\n",
    "        # intermediate files are word <tab> count per line\n",
    "        for line in infile.readlines():\n",
    "            word_count = line.split('\\t')\n",
    "            # starting with an uppercase\n",
    "            word = word_count[0]\n",
    "            if len(word_count) == 2 and (word[0] == word[0].upper()):\n",
    "                try:\n",
    "                    counts_U[word_count[0]] += int(word_count[1])\n",
    "                except KeyError:\n",
    "                    counts_U[word_count[0]] = int(word_count[1])\n",
    "                    \n",
    "            elif len(word_count) == 2:\n",
    "                try:\n",
    "                    counts_L[word_count[0]] += int(word_count[1])\n",
    "                except KeyError:\n",
    "                    counts_L[word_count[0]] = int(word_count[1]) \n",
    "                \n",
    "for word in counts_U:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts_U[word]))\n",
    "    \n",
    "for word in counts_L:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts_L[word]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For example, I input \"Alice\", \"the\",  \"we\", \"The\", \"We\", \"How\", \"dog\" \n",
    "### they are sorted as alphabetical order for Upper case begin (saved .outputU file),\n",
    "### and Lower case begin (saved .outputL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Case\n",
      "Alice\t398\n",
      "ALICE\t5\n",
      "How\t27\n",
      "HOW\t1\n",
      "The\t119\n",
      "THE\t13\n",
      "We\t11\n",
      "WE\t1\n",
      "Lower Case\n",
      "dog\t3\n",
      "how\t44\n",
      "the\t1686\n",
      "we\t31\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper_n.py\n",
    "!chmod a+x reducer_n.py\n",
    "!chmod a+x pWordCount_n.sh\n",
    "!./pWordCount_n.sh 4 \"Alice the we The We How dog\" \n",
    "print(\"Upper Case\")\n",
    "!cat *.outputU\n",
    "\n",
    "print(\"Lower Case\")\n",
    "!cat *.outputL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1.2 TOTAL SORT using multiple reducers [OPTITIONAL for this week; will be covered in next live session]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cloudera\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1.3 How many times does the word alice occur in the book?\n",
    "Write a MapReduce job to determine this. Please pay attention to what you use for a  key and value as output from your mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pWordCount_13.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pWordCount_13.sh\n",
    "#!/bin/bash\n",
    "## pWordCount.sh\n",
    "## Author: James G. Shanahan\n",
    "## Usage: pWordCount.sh m wordlist testFile.txt\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##       inputFile = a text input file\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "\n",
    "\n",
    "if [ $# -eq 0 ]\n",
    "  then\n",
    "    echo \"No arguments supplied\"\n",
    "    echo \"To run use\"  \n",
    "    echo \"     pWordCount.sh m wordlist inputFile\"\n",
    "    echo \"Input:\"\n",
    "    echo \"      number of processes (maps), e.g., 4\"\n",
    "    echo \"      wordlist = a space-separated list of words in quotes, e.g., 'the and of'\"\n",
    "    echo \"      inputFile = a text input file\"\n",
    "    exit\n",
    "fi\n",
    "    \n",
    "    \n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a text file  \n",
    "data=\"alicesTExtFilename.txt\"\n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper13.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "    \n",
    "## sorting keys (words) by alphbetaical order      \n",
    "    \n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer13.py $countfiles | sort -k1,1 > output13\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper13.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper13.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "filename = sys.argv[1]\n",
    "\n",
    "## Words in the word list are space delimited\n",
    "wordlist = sys.argv[2].lower().split(' ')\n",
    "counts = {}\n",
    "\n",
    " \n",
    "words = re.findall('\\w+', open(filename).read())\n",
    "for word in words:\n",
    "            if word.lower() in wordlist:\n",
    "                try:\n",
    "                    counts[word.lower()] += 1\n",
    "                except:\n",
    "                    counts[word.lower()] = 1\n",
    "\n",
    "for word in counts:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts[word]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer13.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer13.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "counts = {}\n",
    "\n",
    "for intermediate_file in sys.argv:\n",
    "    with open(intermediate_file, 'rU') as infile:\n",
    "        # intermediate files are word <tab> count per line\n",
    "        for line in infile.readlines():\n",
    "            word_count = line.split('\\t')\n",
    "            if len(word_count) == 2:\n",
    "                try:\n",
    "                    counts[word_count[0]] += int(word_count[1])\n",
    "                except KeyError:\n",
    "                    counts[word_count[0]] = int(word_count[1])\n",
    "for word in counts:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper13.py\n",
    "!chmod a+x reducer13.py\n",
    "!chmod a+x pWordCount_13.sh\n",
    "!./pWordCount_13.sh 4 \"Alice\"\n",
    "!cat output13\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.1 WORDCOUNT\n",
    "\n",
    "Using the Enron data from and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count in both SPAM and HAM classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper21.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper21.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# extract words to count from first positional argument, making them all lower case\n",
    "wordlist = []\n",
    "if len(sys.argv) > 1:\n",
    "    for word in sys.argv[1].strip().split():\n",
    "        wordlist.append(word.lower())\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Get the id, label, subject and body, respectively\n",
    "        email_id, label, subject, body = line.split('\\t')\n",
    "    except ValueError:\n",
    "        # if there are only 3 fields in the input, field 3 is default as body\n",
    "        email_id, label, body = line.split('\\t')\n",
    "        subject = ''\n",
    " \n",
    "    # extract words from the combined subject and body text\n",
    "    for word in WORD_RE.findall(subject + ' ' + body):\n",
    "        if len(wordlist) > 0:\n",
    "            if word.lower() in wordlist:\n",
    "                print('{0}\\t{1}'.format(word.lower(), 1))\n",
    "        else:\n",
    "            # otherwise count all words\n",
    "            print('{0}\\t{1}'.format(word.lower(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer21.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer21.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "tmp_word = None\n",
    "tmp_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    if tmp_word == word:\n",
    "        tmp_count += count\n",
    "    else:\n",
    "        if tmp_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (tmp_word, tmp_count)\n",
    "        # update current word    \n",
    "        tmp_count = count\n",
    "        tmp_word = word\n",
    "\n",
    "# do not forget to output the last word if required\n",
    "if tmp_word == word:\n",
    "    print '%s\\t%s' % (tmp_word, tmp_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper21.py\n",
    "!chmod a+x reducer21.py\n",
    "!cat enronemail_1h.txt | ./mapper21.py \"assistance\" | sort -k1,1 | ./reducer21.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/shihyu/enronemail_1h.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/shihyu\n",
    "!hdfs dfs -put enronemail_1h.txt /user/shihyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results21\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob6452960282347316161.jar tmpDir=null\n",
      "16/09/09 21:27:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 21:27:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 21:27:54 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/09 21:27:54 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/09 21:27:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0016\n",
      "16/09/09 21:27:54 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0016\n",
      "16/09/09 21:27:54 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0016/\n",
      "16/09/09 21:27:54 INFO mapreduce.Job: Running job: job_1473444507507_0016\n",
      "16/09/09 21:28:01 INFO mapreduce.Job: Job job_1473444507507_0016 running in uber mode : false\n",
      "16/09/09 21:28:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/09 21:28:09 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/09 21:28:10 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/09 21:28:15 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/09 21:28:15 INFO mapreduce.Job: Job job_1473444507507_0016 completed successfully\n",
      "16/09/09 21:28:15 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=156\n",
      "\t\tFILE: Number of bytes written=355991\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217093\n",
      "\t\tHDFS: Number of bytes written=14\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11199\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3594\n",
      "\t\tTotal time spent by all map tasks (ms)=11199\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3594\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11199\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3594\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11467776\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3680256\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=130\n",
      "\t\tMap output materialized bytes=162\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=162\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=139\n",
      "\t\tCPU time spent (ms)=2120\n",
      "\t\tPhysical memory (bytes) snapshot=690331648\n",
      "\t\tVirtual memory (bytes) snapshot=4668510208\n",
      "\t\tTotal committed heap usage (bytes)=704118784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=14\n",
      "16/09/09 21:28:15 INFO streaming.StreamJob: Output directory: results21\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results21\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper \"/home/cloudera/mapper21.py 'assistance'\" \\\n",
    "-reducer /home/cloudera/reducer21.py \\\n",
    "-input /user/shihyu/enronemail_1h.txt \\\n",
    "-output results21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/results21/part-00000 ### Should be 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.2  \n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2.1) determine the top-10 occurring tokens (most frequent tokens) using a single reducer for the SPAM class and for the HAM class. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer22.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer22.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "\n",
    "tmp_word = None\n",
    "tmp_count = 0\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if tmp_word == word:\n",
    "        tmp_count += count\n",
    "    else:\n",
    "        if tmp_word:\n",
    "            # save count            \n",
    "            wordcount[tmp_word] = tmp_count\n",
    "        tmp_count = count\n",
    "        tmp_word = word\n",
    "\n",
    "if tmp_word == word:    \n",
    "    wordcount[tmp_word] = tmp_count\n",
    "    \n",
    "# sort count top get top n counts:\n",
    "n = 10\n",
    "sorted_count = sorted(wordcount.items(), key=operator.itemgetter(1))\n",
    "print 'Top %d counts from %d words:' %(n, len(sorted_count))\n",
    "for i in range(n):\n",
    "    print '%s\\t%d' %(sorted_count[-i-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 tokens of SPAM messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/shihyu/spam_enronemail_1h.txt': File exists\n",
      "Deleted results22_spam\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob9139697224943270396.jar tmpDir=null\n",
      "16/09/09 21:45:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 21:45:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 21:45:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/09 21:45:51 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/09 21:45:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0021\n",
      "16/09/09 21:45:52 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0021\n",
      "16/09/09 21:45:52 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0021/\n",
      "16/09/09 21:45:52 INFO mapreduce.Job: Running job: job_1473444507507_0021\n",
      "16/09/09 21:45:58 INFO mapreduce.Job: Job job_1473444507507_0021 running in uber mode : false\n",
      "16/09/09 21:45:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/09 21:46:07 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/09 21:46:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/09 21:46:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/09 21:46:14 INFO mapreduce.Job: Job job_1473444507507_0021 completed successfully\n",
      "16/09/09 21:46:14 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=180515\n",
      "\t\tFILE: Number of bytes written=716688\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=139858\n",
      "\t\tHDFS: Number of bytes written=108\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13432\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4041\n",
      "\t\tTotal time spent by all map tasks (ms)=13432\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4041\n",
      "\t\tTotal vcore-seconds taken by all map tasks=13432\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4041\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=13754368\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4137984\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=44\n",
      "\t\tMap output records=18690\n",
      "\t\tMap output bytes=143129\n",
      "\t\tMap output materialized bytes=180521\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3787\n",
      "\t\tReduce shuffle bytes=180521\n",
      "\t\tReduce input records=18690\n",
      "\t\tReduce output records=11\n",
      "\t\tSpilled Records=37380\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=139\n",
      "\t\tCPU time spent (ms)=3770\n",
      "\t\tPhysical memory (bytes) snapshot=733274112\n",
      "\t\tVirtual memory (bytes) snapshot=4686516224\n",
      "\t\tTotal committed heap usage (bytes)=679477248\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=139622\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=108\n",
      "16/09/09 21:46:14 INFO streaming.StreamJob: Output directory: results22_spam\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper21.py\n",
    "!chmod a+x reducer22.py\n",
    "!hdfs dfs -put spam_enronemail_1h.txt /user/shihyu\n",
    "!hdfs dfs -rm -r results22_spam\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper21.py \\\n",
    "-reducer /home/cloudera/reducer22.py \\\n",
    "-input /user/shihyu/spam_enronemail_1h.txt \\\n",
    "-output results22_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 counts from 3787 words:\t\r\n",
      "the\t698\r\n",
      "to\t565\r\n",
      "and\t390\r\n",
      "your\t356\r\n",
      "a\t346\r\n",
      "of\t336\r\n",
      "you\t332\r\n",
      "in\t235\r\n",
      "for\t203\r\n",
      "com\t151\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/results22_spam/part-00000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 tokens of HAM messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/shihyu/ham_enronemail_1h.txt': File exists\n",
      "rm: `results22_ham': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob6025724084762220367.jar tmpDir=null\n",
      "16/09/09 21:50:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 21:50:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 21:50:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/09 21:50:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/09 21:50:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0022\n",
      "16/09/09 21:50:38 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0022\n",
      "16/09/09 21:50:38 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0022/\n",
      "16/09/09 21:50:38 INFO mapreduce.Job: Running job: job_1473444507507_0022\n",
      "16/09/09 21:50:46 INFO mapreduce.Job: Job job_1473444507507_0022 running in uber mode : false\n",
      "16/09/09 21:50:46 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/09 21:50:54 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/09 21:50:55 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/09 21:51:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/09 21:51:01 INFO mapreduce.Job: Job job_1473444507507_0022 completed successfully\n",
      "16/09/09 21:51:01 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=137520\n",
      "\t\tFILE: Number of bytes written=630692\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=93864\n",
      "\t\tHDFS: Number of bytes written=106\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12500\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4040\n",
      "\t\tTotal time spent by all map tasks (ms)=12500\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4040\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12500\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4040\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12800000\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4136960\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=56\n",
      "\t\tMap output records=14221\n",
      "\t\tMap output bytes=109072\n",
      "\t\tMap output materialized bytes=137526\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2768\n",
      "\t\tReduce shuffle bytes=137526\n",
      "\t\tReduce input records=14221\n",
      "\t\tReduce output records=11\n",
      "\t\tSpilled Records=28442\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=182\n",
      "\t\tCPU time spent (ms)=3650\n",
      "\t\tPhysical memory (bytes) snapshot=695545856\n",
      "\t\tVirtual memory (bytes) snapshot=4660273152\n",
      "\t\tTotal committed heap usage (bytes)=704118784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=93630\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=106\n",
      "16/09/09 21:51:01 INFO streaming.StreamJob: Output directory: results22_ham\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper21.py\n",
    "!chmod a+x reducer22.py\n",
    "!hdfs dfs -put ham_enronemail_1h.txt /user/shihyu\n",
    "!hdfs dfs -rm -r results22_ham\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper21.py \\\n",
    "-reducer /home/cloudera/reducer22.py \\\n",
    "-input /user/shihyu/ham_enronemail_1h.txt \\\n",
    "-output results22_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 counts from 2768 words:\t\r\n",
      "the\t549\r\n",
      "to\t398\r\n",
      "ect\t382\r\n",
      "and\t278\r\n",
      "of\t230\r\n",
      "hou\t206\r\n",
      "a\t196\r\n",
      "in\t182\r\n",
      "for\t170\r\n",
      "on\t135\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/results22_ham/part-00000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.3 (Optional)  \n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2.1) determine the top-10 occurring tokens (most frequent tokens) using multiple reducers. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper23.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper23.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "sys.stderr.write(\"reporter:counter:Mapper,Script Count,1\\n\") \n",
    "for line in reader(sys.stdin):\n",
    "    sys.stderr.write(\"reporter:counter:Mapper,Line Count,1\\n\")    \n",
    "    try:\n",
    "        int(line[0]) #check if the ID field is an integer, skip the record if not\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    words = re.findall(WORD_RE, line[3])\n",
    "    for word in words:\n",
    "        print word.lower()+'\\t1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer23.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer23.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "tmp_word=''\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "sys.stderr.write(\"reporter:counter:Reducer,Script Count,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    line=line.strip().split('\\t') #Parse line into a list of fields\n",
    "    word,sub_count=line\n",
    "    if tmp_word==word:\n",
    "        count+=int(sub_count) #Extract chunk count from the second field of each incoming line\n",
    "    else:\n",
    "        if tmp_word:\n",
    "            sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\")\n",
    "            print tmp_word+'\\t'+str(count)\n",
    "        tmp_word=word\n",
    "        count=int(sub_count)\n",
    "if tmp_word:\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Line Count,1\\n\")\n",
    "    print tmp_word+'\\t'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted results23\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob6434113247504200456.jar tmpDir=null\n",
      "16/09/09 22:01:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 22:01:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 22:01:45 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/09 22:01:45 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/09 22:01:45 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/09 22:01:45 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/09 22:01:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0024\n",
      "16/09/09 22:01:45 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0024\n",
      "16/09/09 22:01:45 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0024/\n",
      "16/09/09 22:01:45 INFO mapreduce.Job: Running job: job_1473444507507_0024\n",
      "16/09/09 22:01:53 INFO mapreduce.Job: Job job_1473444507507_0024 running in uber mode : false\n",
      "16/09/09 22:01:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/09 22:02:01 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/09 22:02:02 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/09 22:02:10 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/09/09 22:02:11 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/09 22:02:11 INFO mapreduce.Job: Job job_1473444507507_0024 completed successfully\n",
      "16/09/09 22:02:11 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=12\n",
      "\t\tFILE: Number of bytes written=474206\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217093\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11754\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11915\n",
      "\t\tTotal time spent by all map tasks (ms)=11754\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11915\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11754\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=11915\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12036096\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=12200960\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=24\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=24\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=251\n",
      "\t\tCPU time spent (ms)=2670\n",
      "\t\tPhysical memory (bytes) snapshot=822874112\n",
      "\t\tVirtual memory (bytes) snapshot=6246965248\n",
      "\t\tTotal committed heap usage (bytes)=862978048\n",
      "\tMapper\n",
      "\t\tLine Count=100\n",
      "\t\tScript Count=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "16/09/09 22:02:11 INFO streaming.StreamJob: Output directory: results23\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper23.py\n",
    "!chmod a+x reducer23.py\n",
    "\n",
    "\n",
    "\n",
    "!hdfs dfs -rm -r results23\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-mapper /home/cloudera/mapper23.py \\\n",
    "-reducer /home/cloudera/reducer23.py \\\n",
    "-input /user/shihyu/enronemail_1h.txt \\\n",
    "-output results23\n",
    "!hdfs dfs -cat results23/* | head -10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3.1 Learn a Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChineseExample_Train.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChineseExample_Train.txt\n",
    "D1\t1\tChinese Beijing Chinese\n",
    "D2\t1\tChinese Chinese Shanghai\n",
    "D3\t1\tChinese Macao\n",
    "D4\t0\tTokyo Japan Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_t_ch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_t_ch.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use contenty\n",
    "    msg = line.split('\\t', 2)\n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isChina = msg[0], msg[1]\n",
    "    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        print '%s\\t%d\\t%s\\t%s' % (word, 1, isChina, msgID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t_ch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t_ch.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "tmp_word = None\n",
    "smooth_factor = 0 # no smoothing\n",
    "# because no smoothing, so count from zero for isChina and nonChina cases\n",
    "tmp_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "# key is words, values are word probability at isChina class and nonChina class, respectively\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper\n",
    "    word, count, isChina, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isChina = int(isChina)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # Used to count how many training items are at China category\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isChina\n",
    "\n",
    "    if tmp_word == word:        \n",
    "        tmp_count[isChina] += count\n",
    "    else:\n",
    "        if tmp_word:\n",
    "            # count finish so save it\n",
    "            wordcount[tmp_word] = tmp_count\n",
    "        # begin new count for new word\n",
    "        tmp_count = [smooth_factor, smooth_factor]\n",
    "        tmp_count[isChina] = count                \n",
    "        tmp_word = word\n",
    "\n",
    "# Last word count!\n",
    "if tmp_word == word:    \n",
    "    wordcount[tmp_word] = tmp_count\n",
    "    \n",
    "# calculate NB parameters, and write to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_China = sum(msgIDs.values())\n",
    "n_not_China = n_msg - n_China\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_not_China/n_msg, 1.0*n_China/n_msg)\n",
    "\n",
    "# conditional probabilities for each class\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Systems test for Chinese data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior_prob\t0.25\t0.75\r\n",
      "beijing\t0.0\t0.125\r\n",
      "chinese\t0.333333333333\t0.625\r\n",
      "tokyo\t0.333333333333\t0.0\r\n",
      "shanghai\t0.0\t0.125\r\n",
      "japan\t0.333333333333\t0.0\r\n",
      "macao\t0.0\t0.125\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper_t_ch.py\n",
    "!chmod a+x reducer_t_ch.py\n",
    "#Systems test the mapper and reducer\n",
    "!cat ChineseExample_Train.txt | ./mapper_t_ch.py | sort -k1,1 | ./reducer_t_ch.py > Chinese_probabilities.txt\n",
    "!cat Chinese_probabilities.txt\n",
    "### They agree with hand calculated at HW2.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3.2 Learn a multinomial naive Bayes model (with no smoothing) by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From formula \n",
    "\n",
    "$P(C|D) = \\frac{P(D|C)P(C)}{P(D)}$,\n",
    "\n",
    "where C represents category variable and D represents observation. Since our goal is to obtain $P(C|D)$, we have to \n",
    "evaluate $P(D|C)$ and $P(C)$ first. \n",
    "\n",
    "Priori and conditional probabilities tables for Chinese Data Set: \n",
    "\n",
    "|Prob| $c_1$ = China | $c_0$ = not China|\n",
    "|-----------------------|---------------|----------------|\n",
    "|Prior, $P(C = c_i)$ | 3/4 | 1/4 |\n",
    "|$P(Chinese | C = c_i)$ | 5/8 | 1/3 |\n",
    "|$P(Beijing | C = c_i)$ | 1/8 | 0 |\n",
    "|$P(Shanghai | C = c_i)$ | 1/8 | 0 |\n",
    "|$P(Macao | C = c_i)$ | 1/8 | 0 |\n",
    "|$P(Tokyo | C = c_i)$ | 0 | 1/3 |\n",
    "|$P(Japan | C = c_i)$ | 0 | 1/3 |\n",
    "\n",
    "\n",
    "Classifier:\n",
    "\n",
    "Given $D_5$ as observation, it can be classfied by comparing probabilities of $P(D_5 | C = c_0)$ and  $P(D_5 | C = c_1)$.\n",
    "Since \n",
    "\n",
    "%%latex\n",
    "\\begin{align}\n",
    "    P(D_5 | C = c_0) = P(Chinese | C = c_0)^3 P(Tokyo | C = c_0) P(Japan | C = c_0) = 1/243\n",
    "\\end{align}\n",
    "\n",
    "and \n",
    "\n",
    "%%latex\n",
    "\\begin{align}\n",
    "    P(D_5 | C = c_1) = P(Chinese | C = c_1)^3 P(Tokyo | C = c_1) P(Japan | C = c_1) = 0\n",
    "\\end{align}\n",
    "    \n",
    " Therefore, \n",
    " \n",
    " %%latex\n",
    "\\begin{align}\n",
    "    P(C = c_0 | D_5) = \\alpha * P(D_5 | C = c_0) P(C = c_0) = \\alpha * 1/243 * 1/4 \n",
    "\\end{align}\n",
    "\n",
    "and \n",
    "\n",
    "%%latex\n",
    "\\begin{align}\n",
    "    P(C = c_1 | D_5) = \\alpha * P(D_5 | C = c_1) P(C = c_1) = \\alpha * 0 * 3/4 \n",
    "\\end{align}\n",
    " \n",
    "   \n",
    "Finally, since $P(C = c_0 | D_5)$ is greter then $P(C = c_1 | D_5)$, $D_5$ will be classified as non China. This classification result is totally different from smoothing case that presented at attached file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3.3 Learn a multinomial naive Bayes model (with no smoothing) for SPAM filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model as SPAM_Model_MNB.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_t_em.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_t_em.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use email subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    if len(msg) <= 2:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]\n",
    "    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        print '%s\\t%d\\t%s\\t%s' % (word, 1, isSpam, msgID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t_em.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t_em.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "tmp_word = None\n",
    "smooth_factor = 0 # no smoothing\n",
    "# because no smoothing, so count from zero for isChina and nonChina cases\n",
    "tmp_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "# key is words, values are word probability at isChina class and nonChina class, respectively\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # Used to count how many training items are at SPAM emails category\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    if tmp_word == word:        \n",
    "        tmp_count[isSpam] += count\n",
    "    else:\n",
    "        if tmp_word:\n",
    "            # count finish so save it\n",
    "            wordcount[tmp_word] = tmp_count\n",
    "        # begin new count for new word\n",
    "        tmp_count = [smooth_factor, smooth_factor]\n",
    "        tmp_count[isSpam] = count                \n",
    "        tmp_word = word\n",
    "\n",
    "# Last word count!\n",
    "if tmp_word == word:    \n",
    "    wordcount[tmp_word] = tmp_count\n",
    "    \n",
    "# calculate NB parameters, and write to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_Spam = sum(msgIDs.values())\n",
    "n_Ham = n_msg - n_Spam\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_Ham/n_msg, 1.0*n_Spam/n_msg)\n",
    "\n",
    "# conditional probabilities for each class\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted prob_em\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob1804159925030180330.jar tmpDir=null\n",
      "16/09/09 22:31:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 22:31:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 22:31:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/09 22:31:32 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/09 22:31:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0026\n",
      "16/09/09 22:31:33 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0026\n",
      "16/09/09 22:31:33 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0026/\n",
      "16/09/09 22:31:33 INFO mapreduce.Job: Running job: job_1473444507507_0026\n",
      "16/09/09 22:31:40 INFO mapreduce.Job: Job job_1473444507507_0026 running in uber mode : false\n",
      "16/09/09 22:31:40 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/09 22:31:48 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/09 22:31:49 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/09 22:31:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/09 22:31:55 INFO mapreduce.Job: Job job_1473444507507_0026 completed successfully\n",
      "16/09/09 22:31:55 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1172006\n",
      "\t\tFILE: Number of bytes written=2699652\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217093\n",
      "\t\tHDFS: Number of bytes written=173923\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12593\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4298\n",
      "\t\tTotal time spent by all map tasks (ms)=12593\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4298\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12593\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4298\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12895232\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4401152\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=1105594\n",
      "\t\tMap output materialized bytes=1172012\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=1172012\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=5409\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=160\n",
      "\t\tCPU time spent (ms)=4150\n",
      "\t\tPhysical memory (bytes) snapshot=695779328\n",
      "\t\tVirtual memory (bytes) snapshot=4670259200\n",
      "\t\tTotal committed heap usage (bytes)=704118784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=173923\n",
      "16/09/09 22:31:55 INFO streaming.StreamJob: Output directory: prob_em\n",
      "mv: `/user/shihyu/SPAM_Model_MNB.tsv': File exists\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper_t_em.py\n",
    "!chmod a+x reducer_t_em.py\n",
    "!hdfs dfs -rm -r prob_em\n",
    "!rm SPAM_Model_MNB.tsv\n",
    "# run training job\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper_t_em.py \\\n",
    "-reducer /home/cloudera/reducer_t_em.py \\\n",
    "-input /user/shihyu/enronemail_1h.txt \\\n",
    "-output prob_em\n",
    "!hadoop fs -mv  prob_em/part-00000 /user/shihyu/SPAM_Model_MNB.tsv\n",
    "!hadoop fs -get /user/shihyu/SPAM_Model_MNB.tsv\n",
    "\n",
    "### output prob will be write in a file SPAM_Model_MNB.tsv.###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 and bottom 10 model entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "zxs\t0.0\t5.29464711177e-05\n",
      "zolam\t0.0\t0.000105892942235\n",
      "zo\t0.0\t0.000105892942235\n",
      "zk\t0.0\t5.29464711177e-05\n",
      "zinc\t0.0\t5.29464711177e-05\n",
      "zimin\t0.000349259569712\t0.0\n",
      "zesto\t0.0\t0.000529464711177\n",
      "zero\t0.000209555741827\t5.29464711177e-05\n",
      "zadorozhny\t0.00027940765577\t0.0\n",
      "zac\t0.0\t0.000105892942235\n"
     ]
    }
   ],
   "source": [
    "### Buttom 10\n",
    "!cat SPAM_Model_MNB.tsv | sort -k1,1 -r | head -n 10 > a\n",
    "!cat a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "\u0001\t0.00146689019279\t0.0\n",
      "0\t0.0\t0.000317678826706\n",
      "00\t0.00132718636491\t0.000741250595648\n",
      "000\t0.00174629784856\t0.00142955472018\n",
      "001\t0.000139703827885\t5.29464711177e-05\n",
      "0011\t6.98519139424e-05\t0.0\n",
      "00450\t0.0\t5.29464711177e-05\n",
      "0080\t0.0\t5.29464711177e-05\n",
      "01\t0.00125733445096\t0.000370625297824\n",
      "012\t0.00027940765577\t0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Top 10\n",
    "!cat SPAM_Model_MNB.tsv | sort -k1,1 | head -n 10 > a\n",
    "!cat a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.3.4 Classify Documents using the learnt Multinomial Naive Bayes model using Hadoop Streaming\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Unit test your classifier map reduce job using the Chinese example.\n",
    "Input $D_5$, output $0$, not China. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChineseExample_Test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChineseExample_Test.txt\n",
    "D1\t1\tChinese Beijing Chinese\n",
    "D2\t1\tChinese Chinese Shanghai\n",
    "D3\t1\tChinese Macao\n",
    "D4\t0\tTokyo Japan Chinese\n",
    "D5\t0\tChinese Chinese Chinese Tokyo Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/shihyu/Chinese_probabilities.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put Chinese_probabilities.txt /user/shihyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_c_ch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_c_ch.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string, subprocess\n",
    "# read the probabilities from Chinese_probabilities.txt\n",
    "prob = {}\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"/user/shihyu/Chinese_probabilities.txt\"], stdout=subprocess.PIPE)\n",
    "### Prepare key (word) values (conditional prob at each class) pair \n",
    "for line in cat.stdout:\n",
    "        word, p0, p1 = line.split()\n",
    "        prob[word] = [p0, p1]\n",
    "\n",
    "# get prior probability\n",
    "prior = prob['prior_prob']\n",
    "\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use content\n",
    "    msg = line.split('\\t', 2)\n",
    "    # skip strange message \n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isChina = msg[0], msg[1]    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "         \n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s\\t%s' % (msgID, prob[word][0], prob[word][1], isChina, prior[0], prior[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_c_ch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_c_ch.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "\n",
    "tmp_msg = None\n",
    "tmp_prob = [0, 0] \n",
    "tmp_truth = 0\n",
    "msgID = None\n",
    "n_error = 0\n",
    "n_msg = 0\n",
    "n_zero = [0, 0]\n",
    "\n",
    "\n",
    "print '%s\\t%s\\t%s' %('ID', 'TRUTH', 'PREDICTION')\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    #print line\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper_c.py\n",
    "    msgID, p0, p1, isChina, pr0, pr1 = line.split('\\t')\n",
    "    prob = [float(p0), float(p1)]\n",
    "    \n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:        \n",
    "        isChina = int(isChina)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # get posterior prob assume independence\n",
    "    if tmp_msg == msgID:        \n",
    "        # by considering zero prob \n",
    "        tmp_prob = np.sum([[math.log(x) if x>0 else float('-inf') for x in prob], tmp_prob], 0)\n",
    "    else:\n",
    "        if tmp_msg:\n",
    "            # count finish for current word, predict and print result\n",
    "            pred = np.argmax(tmp_prob)\n",
    "            n_error += pred != tmp_truth\n",
    "            n_msg += 1\n",
    "            n_zero[tmp_truth] += float('-inf') in tmp_prob\n",
    "            print '%s\\t%s\\t%s' %(tmp_msg, tmp_truth, pred)\n",
    "                    \n",
    "        # initialize new count for new word\n",
    "        prior = [math.log(float(pr0)), math.log(float(pr1))]\n",
    "        tmp_prob = np.sum([[math.log(x) if x>0 else float('-inf') for x in prob], prior], 0)\n",
    "        tmp_msg = msgID\n",
    "        tmp_truth = isChina\n",
    "        \n",
    "# do not forget to print the last msg\n",
    "if tmp_msg == msgID:\n",
    "    pred = np.argmax(tmp_prob)\n",
    "    n_error += pred != isChina\n",
    "    n_msg += 1\n",
    "    n_zero[tmp_truth] += float('-inf') in tmp_prob\n",
    "    print '%s\\t%s\\t%s' %(msgID, tmp_truth, pred)\n",
    "    \n",
    "# calculate the overall error rate\n",
    "print 'Error rate is: %.f' %(1.0*n_error/n_msg)\n",
    "print 'Number of messages with zero probability: China(%d), non China(%d)' %(n_zero[1], n_zero[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/shihyu/ChineseExample_Test.txt': File exists\n",
      "Deleted Chinese_classification_results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob2950033205685806109.jar tmpDir=null\n",
      "16/09/09 23:30:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 23:30:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/09 23:30:24 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/09 23:30:24 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/09 23:30:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0035\n",
      "16/09/09 23:30:25 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0035\n",
      "16/09/09 23:30:25 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0035/\n",
      "16/09/09 23:30:25 INFO mapreduce.Job: Running job: job_1473444507507_0035\n",
      "16/09/09 23:30:33 INFO mapreduce.Job: Job job_1473444507507_0035 running in uber mode : false\n",
      "16/09/09 23:30:33 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/09 23:30:47 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/09/09 23:30:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/09 23:30:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/09 23:30:55 INFO mapreduce.Job: Job job_1473444507507_0035 completed successfully\n",
      "16/09/09 23:30:56 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=573\n",
      "\t\tFILE: Number of bytes written=356873\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=453\n",
      "\t\tHDFS: Number of bytes written=139\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=26415\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4321\n",
      "\t\tTotal time spent by all map tasks (ms)=26415\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4321\n",
      "\t\tTotal vcore-seconds taken by all map tasks=26415\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4321\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=27048960\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4424704\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=535\n",
      "\t\tMap output materialized bytes=579\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=579\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=32\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=556\n",
      "\t\tCPU time spent (ms)=11750\n",
      "\t\tPhysical memory (bytes) snapshot=925163520\n",
      "\t\tVirtual memory (bytes) snapshot=4690018304\n",
      "\t\tTotal committed heap usage (bytes)=927989760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=215\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=139\n",
      "16/09/09 23:30:56 INFO streaming.StreamJob: Output directory: Chinese_classification_results\n"
     ]
    }
   ],
   "source": [
    "# clean up HDFS \n",
    "!chmod a+x mapper_c_ch.py\n",
    "!chmod a+x reducer_c_ch.py\n",
    "!hdfs dfs -put ChineseExample_Test.txt /user/shihyu\n",
    "\n",
    "!hdfs dfs -rm -r Chinese_classification_results\n",
    "# run classification job\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper_c_ch.py \\\n",
    "-reducer /home/cloudera/reducer_c_ch.py \\\n",
    "-input /user/shihyu/ChineseExample_Test.txt \\\n",
    "-output Chinese_classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\tTRUTH\tPREDICTION\r\n",
      "D1\t1\t1\r\n",
      "D2\t1\t1\r\n",
      "D3\t1\t1\r\n",
      "D4\t0\t0\r\n",
      "D5\t0\t0\r\n",
      "Error rate is: 0\t\r\n",
      "Number of messages with zero probability: China(3), non China(2)\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat Chinese_classification_results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification for emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_c_em.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_c_em.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string, subprocess\n",
    "# read the probability prob_em from HDFS\n",
    "prob = {}\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"/user/cloudera/prob_em/part-00000\"], stdout=subprocess.PIPE)\n",
    "### Prepare key (word) values (conditional prob at each class) pair \n",
    "for line in cat.stdout:    \n",
    "    word, p0, p1 = line.split()\n",
    "    prob[word] = [p0, p1]\n",
    "    print '%s\\n' % (word)\n",
    "\n",
    "# get prior probability\n",
    "prior = prob['prior_prob']\n",
    "\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    # skip bad message \n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "         \n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s\\t%s' % (msgID, prob[word][0], prob[word][1], isSpam, prior[0], prior[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_c_em.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_c_em.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "\n",
    "tmp_msg = None\n",
    "tmp_prob = [0, 0] \n",
    "tmp_truth = 0\n",
    "msgID = None\n",
    "n_error = 0\n",
    "n_msg = 0\n",
    "n_zero = [0, 0]\n",
    "\n",
    "print '%s\\t%s\\t%s' %('EMAIL ID', 'TRUTH', 'PREDICTION')\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    #print line\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    try:\n",
    "        msgID, p0, p1, isSpam, pr0, pr1 = line.split('\\t')\n",
    "    except ValueError:\n",
    "        \n",
    "        continue\n",
    "\n",
    "       \n",
    "        \n",
    "    prob = [float(p0), float(p1)]\n",
    "    \n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:        \n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    if tmp_msg == msgID:        \n",
    "        tmp_prob = np.sum([[math.log(x) if x>0 else float('-inf') for x in prob], tmp_prob], 0)\n",
    "    else:\n",
    "        if tmp_msg:\n",
    "            # count finish for current word, predict and print result\n",
    "            pred = np.argmax(tmp_prob)\n",
    "            n_error += pred != tmp_truth\n",
    "            n_msg += 1\n",
    "            n_zero[tmp_truth] += float('-inf') in tmp_prob\n",
    "            print '%s\\t%s\\t%s' %(tmp_msg, tmp_truth, pred)\n",
    "                    \n",
    "        # initialize new count for new word\n",
    "        prior = [math.log(float(pr0)), math.log(float(pr1))]\n",
    "        tmp_prob = np.sum([[math.log(x) if x>0 else float('-inf') for x in prob], prior], 0)\n",
    "        tmp_msg = msgID\n",
    "        tmp_truth = isSpam\n",
    "\n",
    "# do not forget to print the last msg result if needed!\n",
    "if tmp_msg == msgID:\n",
    "    pred = np.argmax(tmp_prob)\n",
    "    n_error += pred != isSpam\n",
    "    n_msg += 1\n",
    "    n_zero[tmp_truth] += float('-inf') in tmp_prob\n",
    "    print '%s\\t%s\\t%s' %(msgID, tmp_truth, pred)\n",
    "    \n",
    "# calculate the overall error rate\n",
    "print 'Error rate is : %.6f' %(1.0*n_error/n_msg)\n",
    "print 'Number of messages with zero probability: spam(%d), ham(%d)' %(n_zero[1], n_zero[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted prob_em\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob4653791537771942229.jar tmpDir=null\n",
      "16/09/10 05:47:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 05:47:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 05:47:19 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/10 05:47:19 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/10 05:47:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0049\n",
      "16/09/10 05:47:19 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0049\n",
      "16/09/10 05:47:19 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0049/\n",
      "16/09/10 05:47:19 INFO mapreduce.Job: Running job: job_1473444507507_0049\n",
      "16/09/10 05:47:27 INFO mapreduce.Job: Job job_1473444507507_0049 running in uber mode : false\n",
      "16/09/10 05:47:27 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/10 05:47:36 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/10 05:47:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/10 05:47:44 INFO mapreduce.Job: Job job_1473444507507_0049 completed successfully\n",
      "16/09/10 05:47:44 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1172006\n",
      "\t\tFILE: Number of bytes written=2699652\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217093\n",
      "\t\tHDFS: Number of bytes written=173923\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13838\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4599\n",
      "\t\tTotal time spent by all map tasks (ms)=13838\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4599\n",
      "\t\tTotal vcore-seconds taken by all map tasks=13838\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4599\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=14170112\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4709376\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=1105594\n",
      "\t\tMap output materialized bytes=1172012\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=1172012\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=5409\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=144\n",
      "\t\tCPU time spent (ms)=4170\n",
      "\t\tPhysical memory (bytes) snapshot=752009216\n",
      "\t\tVirtual memory (bytes) snapshot=4707004416\n",
      "\t\tTotal committed heap usage (bytes)=679477248\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=173923\n",
      "16/09/10 05:47:44 INFO streaming.StreamJob: Output directory: prob_em\n",
      "Deleted Emails_classification_results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob1113870619855326106.jar tmpDir=null\n",
      "16/09/10 05:47:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 05:47:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 05:47:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/10 05:47:51 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/09/10 05:47:51 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/10 05:47:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0050\n",
      "16/09/10 05:47:52 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0050\n",
      "16/09/10 05:47:52 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0050/\n",
      "16/09/10 05:47:52 INFO mapreduce.Job: Running job: job_1473444507507_0050\n",
      "16/09/10 05:47:59 INFO mapreduce.Job: Job job_1473444507507_0050 running in uber mode : false\n",
      "16/09/10 05:47:59 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/10 05:48:10 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/10 05:48:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/10 05:48:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/10 05:48:17 INFO mapreduce.Job: Job job_1473444507507_0050 completed successfully\n",
      "16/09/10 05:48:17 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2409826\n",
      "\t\tFILE: Number of bytes written=5175358\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217093\n",
      "\t\tHDFS: Number of bytes written=2778\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19651\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4672\n",
      "\t\tTotal time spent by all map tasks (ms)=19651\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4672\n",
      "\t\tTotal vcore-seconds taken by all map tasks=19651\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4672\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=20122624\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4784128\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=54839\n",
      "\t\tMap output bytes=2300142\n",
      "\t\tMap output materialized bytes=2409832\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5510\n",
      "\t\tReduce shuffle bytes=2409832\n",
      "\t\tReduce input records=54839\n",
      "\t\tReduce output records=103\n",
      "\t\tSpilled Records=109678\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=157\n",
      "\t\tCPU time spent (ms)=4010\n",
      "\t\tPhysical memory (bytes) snapshot=709365760\n",
      "\t\tVirtual memory (bytes) snapshot=4679450624\n",
      "\t\tTotal committed heap usage (bytes)=741867520\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2778\n",
      "16/09/10 05:48:17 INFO streaming.StreamJob: Output directory: Emails_classification_results\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper_t_em.py\n",
    "!chmod a+x reducer_t_em.py\n",
    "!hdfs dfs -rm -r prob_em\n",
    "\n",
    "# run training job\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper_t_em.py \\\n",
    "-reducer /home/cloudera/reducer_t_em.py \\\n",
    "-input /user/shihyu/enronemail_1h.txt \\\n",
    "-output prob_em\n",
    "\n",
    "# clean up HDFS \n",
    "!chmod a+x mapper_c_em.py\n",
    "!chmod a+x reducer_c_em.py\n",
    "\n",
    "!hdfs dfs -rm -r Emails_classification_results\n",
    "# run classification job\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper_c_em.py \\\n",
    "-reducer /home/cloudera/reducer_c_em.py \\\n",
    "-input /user/shihyu/enronemail_1h.txt \\\n",
    "-output Emails_classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL ID\tTRUTH\tPREDICTION\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Error rate is : 0\t\r\n",
      "Number of messages with zero probability: spam(44), ham(56)\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat Emails_classification_results/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the posterior probabilities. Since the NB posterior probability is: \n",
    "$$\n",
    "P(C=c_j\\mid X_1,X_2, ...,X_n)=\\frac{P(C=c_j)\\prod_i{P(X_i\\mid C=c_i)}}{\\sum_k{P(C=c_k)\\prod_i{P(X_i\\mid C=c_k)}}}\n",
    "$$\n",
    "where $C$ is email category and $X_i$ are wrods. The conditional probability of those words for the opposite category, $P(X_i\\mid C=c_k)$, are zero, and this will make one term on the denominator become zero. Therefore, posterior probability is 1 for the category that contains all the words from same category, and 0 for the other category. Hisogram is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f78f1abf090>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAFCCAYAAABilzUAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nns/qtEs+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78fd5efe90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 44 spam and 56 ham all has posterior probability of 1\n",
    "spam = (0,44)\n",
    "ham = (0,56)\n",
    "plt.figure(figsize=(15,5))\n",
    "p = plt.subplot(1, 2, 1)\n",
    "p.hist(ham,100)\n",
    "plt.title('Ham Probability Frequencies')\n",
    "\n",
    "p = plt.subplot(1, 2, 2)\n",
    "p.hist(spam,100)\n",
    "plt.title('Spam Probability Frequencies') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.4 Use Laplace plus-one smoothing  <a name=\"2.4\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. \n",
    "\n",
    "In addition, compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t_em_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t_em_s.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "tmp_word = None\n",
    "smooth_factor = 1 # because smoothing\n",
    "# because smoothing, so count from ONE \n",
    "tmp_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "# key is words, values are word probability at isChina class and nonChina class, respectively\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # Used to count how many training items are at SPAM emails category\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    if tmp_word == word:        \n",
    "        tmp_count[isSpam] += count\n",
    "    else:\n",
    "        if tmp_word:\n",
    "            # count finish so save it\n",
    "            wordcount[tmp_word] = tmp_count\n",
    "        # begin new count for new word\n",
    "        tmp_count = [smooth_factor, smooth_factor]\n",
    "        tmp_count[isSpam] = count                \n",
    "        tmp_word = word\n",
    "\n",
    "# Last word count!\n",
    "if tmp_word == word:    \n",
    "    wordcount[tmp_word] = tmp_count\n",
    "    \n",
    "# calculate NB parameters, and write to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_Spam = sum(msgIDs.values())\n",
    "n_Ham = n_msg - n_Spam\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_Ham/n_msg, 1.0*n_Spam/n_msg)\n",
    "\n",
    "# conditional probabilities for each class\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted prob_em\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob5515545378449372430.jar tmpDir=null\n",
      "16/09/10 06:03:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 06:03:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 06:03:37 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/10 06:03:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/10 06:03:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0053\n",
      "16/09/10 06:03:38 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0053\n",
      "16/09/10 06:03:38 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0053/\n",
      "16/09/10 06:03:38 INFO mapreduce.Job: Running job: job_1473444507507_0053\n",
      "16/09/10 06:03:44 INFO mapreduce.Job: Job job_1473444507507_0053 running in uber mode : false\n",
      "16/09/10 06:03:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/10 06:03:52 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/10 06:03:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/10 06:04:01 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/10 06:04:01 INFO mapreduce.Job: Job job_1473444507507_0053 completed successfully\n",
      "16/09/10 06:04:01 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1172006\n",
      "\t\tFILE: Number of bytes written=2699658\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217093\n",
      "\t\tHDFS: Number of bytes written=234964\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11916\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4755\n",
      "\t\tTotal time spent by all map tasks (ms)=11916\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4755\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11916\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4755\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12201984\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4869120\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=1105594\n",
      "\t\tMap output materialized bytes=1172012\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=1172012\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=5409\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=177\n",
      "\t\tCPU time spent (ms)=4130\n",
      "\t\tPhysical memory (bytes) snapshot=743137280\n",
      "\t\tVirtual memory (bytes) snapshot=4685901824\n",
      "\t\tTotal committed heap usage (bytes)=679477248\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=234964\n",
      "16/09/10 06:04:01 INFO streaming.StreamJob: Output directory: prob_em\n",
      "Deleted Emails_classification_results_s\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob2312837241767659670.jar tmpDir=null\n",
      "16/09/10 06:04:08 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 06:04:08 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 06:04:08 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/10 06:04:09 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/10 06:04:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0054\n",
      "16/09/10 06:04:09 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0054\n",
      "16/09/10 06:04:09 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0054/\n",
      "16/09/10 06:04:09 INFO mapreduce.Job: Running job: job_1473444507507_0054\n",
      "16/09/10 06:04:17 INFO mapreduce.Job: Job job_1473444507507_0054 running in uber mode : false\n",
      "16/09/10 06:04:17 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/10 06:04:30 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/10 06:04:31 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/10 06:04:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/10 06:04:39 INFO mapreduce.Job: Job job_1473444507507_0054 completed successfully\n",
      "16/09/10 06:04:39 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2560864\n",
      "\t\tFILE: Number of bytes written=5477440\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217093\n",
      "\t\tHDFS: Number of bytes written=2783\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22027\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5693\n",
      "\t\tTotal time spent by all map tasks (ms)=22027\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5693\n",
      "\t\tTotal vcore-seconds taken by all map tasks=22027\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5693\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=22555648\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5829632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=54839\n",
      "\t\tMap output bytes=2451180\n",
      "\t\tMap output materialized bytes=2560870\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5510\n",
      "\t\tReduce shuffle bytes=2560870\n",
      "\t\tReduce input records=54839\n",
      "\t\tReduce output records=103\n",
      "\t\tSpilled Records=109678\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=374\n",
      "\t\tCPU time spent (ms)=4870\n",
      "\t\tPhysical memory (bytes) snapshot=903626752\n",
      "\t\tVirtual memory (bytes) snapshot=4671840256\n",
      "\t\tTotal committed heap usage (bytes)=929562624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2783\n",
      "16/09/10 06:04:39 INFO streaming.StreamJob: Output directory: Emails_classification_results_s\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper_t_em.py\n",
    "!chmod a+x reducer_t_em_s.py\n",
    "!hdfs dfs -rm -r prob_em\n",
    "\n",
    "# run training job\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper_t_em.py \\\n",
    "-reducer /home/cloudera/reducer_t_em_s.py \\\n",
    "-input /user/shihyu/enronemail_1h.txt \\\n",
    "-output prob_em\n",
    "\n",
    "# clean up HDFS \n",
    "!chmod a+x mapper_c_em.py\n",
    "!chmod a+x reducer_c_em.py\n",
    "\n",
    "!hdfs dfs -rm -r Emails_classification_results_s\n",
    "# run classification job\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper_c_em.py \\\n",
    "-reducer /home/cloudera/reducer_c_em.py \\\n",
    "-input /user/shihyu/enronemail_1h.txt \\\n",
    "-output Emails_classification_results_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL ID\tTRUTH\tPREDICTION\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Error rate is : 0.020000\t\r\n",
      "Number of messages with zero probability: spam(0), ham(0)\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat Emails_classification_results_s/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW2.4 Discussion: Since smoothing essentially add noise to the data, and thus introduce some misclassification on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.5 Ignore rare words  <a name=\"2.5\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset. Report the error and the change in error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t_em_I.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t_em_I.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "\n",
    "tmp_word = None\n",
    "smooth_factor = 0 # no smoothing\n",
    "# because no smoothing, so count from zero for isChina and nonChina cases\n",
    "tmp_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "# key is words, values are word probability at isChina class and nonChina class, respectively\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # Used to count how many training items are at SPAM emails category\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    if tmp_word == word:        \n",
    "        tmp_count[isSpam] += count\n",
    "    else:\n",
    "        if tmp_word:\n",
    "            # count finish so save it\n",
    "            wordcount[tmp_word] = tmp_count\n",
    "        # begin new count for new word\n",
    "        tmp_count = [smooth_factor, smooth_factor]\n",
    "        tmp_count[isSpam] = count                \n",
    "        tmp_word = word\n",
    "\n",
    "# Last word count!\n",
    "if tmp_word == word:    \n",
    "    wordcount[tmp_word] = tmp_count\n",
    "    \n",
    "# calculate NB parameters, and write to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_Spam = sum(msgIDs.values())\n",
    "n_Ham = n_msg - n_Spam\n",
    "print '%s\\t%s\\t%s' %('prior_prob', 1.0*n_Ham/n_msg, 1.0*n_Spam/n_msg)\n",
    "\n",
    "# conditional probabilities for each class\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    # only emit probability when the count (spam and ham together) is greater or equal than 3\n",
    "    if sum(wordcount[key]) >= 3:\n",
    "        print '%s\\t%s\\t%s' %(key, value[0], value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_c_em_I.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_c_em_I.py\n",
    "#!/usr/bin/python\n",
    "import sys, re, string, subprocess\n",
    "# read the probability prob_em from HDFS\n",
    "prob = {}\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"/user/cloudera/prob_em/part-00000\"], stdout=subprocess.PIPE)\n",
    "### Prepare key (word) values (conditional prob at each class) pair \n",
    "for line in cat.stdout:    \n",
    "    word, p0, p1 = line.split()\n",
    "    prob[word] = [p0, p1]\n",
    "    print '%s\\n' % (word)\n",
    "\n",
    "# get prior probability\n",
    "prior = prob['prior_prob']\n",
    "\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    # skip bad message \n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    words = regex.sub(' ', msg[-1].lower())\n",
    "    # split the line into words\n",
    "    words = words.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output)\n",
    "        # which has probability from training step\n",
    "        if word in prob:\n",
    "            print '%s\\t%s\\t%s\\t%s\\t%s\\t%s' % (msgID, prob[word][0], prob[word][1], isSpam, prior[0], prior[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted prob_em\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob2597624802238306615.jar tmpDir=null\n",
      "16/09/10 06:39:40 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 06:39:41 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 06:39:41 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/10 06:39:41 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/10 06:39:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0059\n",
      "16/09/10 06:39:42 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0059\n",
      "16/09/10 06:39:42 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0059/\n",
      "16/09/10 06:39:42 INFO mapreduce.Job: Running job: job_1473444507507_0059\n",
      "16/09/10 06:39:48 INFO mapreduce.Job: Job job_1473444507507_0059 running in uber mode : false\n",
      "16/09/10 06:39:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/10 06:39:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/10 06:40:03 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/10 06:40:04 INFO mapreduce.Job: Job job_1473444507507_0059 completed successfully\n",
      "16/09/10 06:40:04 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1172006\n",
      "\t\tFILE: Number of bytes written=2699658\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217093\n",
      "\t\tHDFS: Number of bytes written=66986\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12983\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4557\n",
      "\t\tTotal time spent by all map tasks (ms)=12983\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4557\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12983\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4557\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=13294592\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4666368\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=1105594\n",
      "\t\tMap output materialized bytes=1172012\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=1172012\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=1877\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=206\n",
      "\t\tCPU time spent (ms)=3950\n",
      "\t\tPhysical memory (bytes) snapshot=652304384\n",
      "\t\tVirtual memory (bytes) snapshot=4667314176\n",
      "\t\tTotal committed heap usage (bytes)=598736896\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=66986\n",
      "16/09/10 06:40:04 INFO streaming.StreamJob: Output directory: prob_em\n",
      "Deleted Emails_classification_results_I\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob3767192195945412910.jar tmpDir=null\n",
      "16/09/10 06:40:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 06:40:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 06:40:11 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/10 06:40:11 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/10 06:40:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0060\n",
      "16/09/10 06:40:11 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0060\n",
      "16/09/10 06:40:11 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0060/\n",
      "16/09/10 06:40:11 INFO mapreduce.Job: Running job: job_1473444507507_0060\n",
      "16/09/10 06:40:21 INFO mapreduce.Job: Job job_1473444507507_0060 running in uber mode : false\n",
      "16/09/10 06:40:21 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/10 06:40:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/10 06:40:41 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/10 06:40:41 INFO mapreduce.Job: Job job_1473444507507_0060 completed successfully\n",
      "16/09/10 06:40:41 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2040312\n",
      "\t\tFILE: Number of bytes written=4436342\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217093\n",
      "\t\tHDFS: Number of bytes written=2785\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22089\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4460\n",
      "\t\tTotal time spent by all map tasks (ms)=22089\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4460\n",
      "\t\tTotal vcore-seconds taken by all map tasks=22089\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4460\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=22619136\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4567040\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=36250\n",
      "\t\tMap output bytes=1967806\n",
      "\t\tMap output materialized bytes=2040318\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1978\n",
      "\t\tReduce shuffle bytes=2040318\n",
      "\t\tReduce input records=36250\n",
      "\t\tReduce output records=103\n",
      "\t\tSpilled Records=72500\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=181\n",
      "\t\tCPU time spent (ms)=3620\n",
      "\t\tPhysical memory (bytes) snapshot=682422272\n",
      "\t\tVirtual memory (bytes) snapshot=4671741952\n",
      "\t\tTotal committed heap usage (bytes)=682622976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2785\n",
      "16/09/10 06:40:41 INFO streaming.StreamJob: Output directory: Emails_classification_results_I\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper_t_em.py\n",
    "!chmod a+x reducer_t_em_I.py\n",
    "!hdfs dfs -rm -r prob_em\n",
    "\n",
    "# run training job\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper_t_em.py \\\n",
    "-reducer /home/cloudera/reducer_t_em_I.py \\\n",
    "-input /user/shihyu/enronemail_1h.txt \\\n",
    "-output prob_em\n",
    "\n",
    "# clean up HDFS \n",
    "!chmod a+x mapper_c_em_I.py\n",
    "!chmod a+x reducer_c_em.py\n",
    "\n",
    "!hdfs dfs -rm -r Emails_classification_results_I\n",
    "# run classification job\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper_c_em_I.py \\\n",
    "-reducer /home/cloudera/reducer_c_em.py \\\n",
    "-input /user/shihyu/enronemail_1h.txt \\\n",
    "-output Emails_classification_results_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL ID\tTRUTH\tPREDICTION\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Error rate is : 0.010000\t\r\n",
      "Number of messages with zero probability: spam(43), ham(54)\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat Emails_classification_results_I/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.5 Discussion: \n",
    "Similar to smoothing, ignoring small frequency words will remove some that only belong to one category.\n",
    "Hence, the training error increases a bit. Compared to smoothing case, training error is smaller. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  HW2.6 Benchmark your code with the Python SciKit-Learn  <a name=\"2.6\"></a>\n",
    "[Back to Table of Contents](#TOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK- multinomial NB error rate is : 0.0000\n",
      "SK- Bernoulli   NB error rate is : 0.1800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# read file\n",
    "with open('enronemail_1h.txt', 'rU') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            email_id, label, subject, body = line.split('\\t')\n",
    "            X_train.append(subject + ' ' + body)\n",
    "        except ValueError:\n",
    "            email_id, label, body = line.split('\\t')\n",
    "            X_train.append(body)\n",
    "        # extract only words from the combined subject and body text\n",
    "        Y_train.append(int(label))\n",
    "\n",
    "# Use the TfidVectorizer to create the feature vectors\n",
    "vectorizer = TfidfVectorizer(token_pattern = \"[\\w']+\")\n",
    "vf = vectorizer.fit(X_train,Y_train)\n",
    "\n",
    "clf_M = MultinomialNB()\n",
    "clf_M.fit(vf.fit_transform(X_train), Y_train)\n",
    "training_error_mnb = 1.0 - clf_M.score(vf.fit_transform(X_train), Y_train)\n",
    "\n",
    "clf_B = BernoulliNB()\n",
    "clf_B.fit(vf.fit_transform(X_train), Y_train)\n",
    "training_error_bnb = 1.0 - clf_B.score(vf.fit_transform(X_train), Y_train)\n",
    "\n",
    "print 'SK- multinomial NB error rate is : %.4f' %training_error_mnb\n",
    "print 'SK- Bernoulli   NB error rate is : %.4f' %training_error_bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.6.1 Bernoulli Naive Bayes (OPTIONAL: note this exercise is a stretch HW and optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From SK-Learn, we have Multinomial NB with 0% error rate, and Bernoulli NB with 18% error rate. The difference \n",
    "is made by the properties of classifiers. Bernoulli NB classifier will focuses on the appearance of word, while Multinomial NB classifier will reflect the frequency of the word. At this example, Bernoulli classifier is better since, as for SPAM detection, the appearance of some words (keywords) is more important than the frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:red\">HW2.6.1 Results: </span>See execution above, performance summary:\n",
    "|   |Training Error   |   \n",
    "|---|:---:|\n",
    "| SK-Learn Multinomial NB | 0% |\n",
    "| SK-Learn Bernoulli NB  | 18% |\n",
    "| Multinomial NB (HW2.4)| 2%  |\n",
    "| Multinomial NB (HW2.5)| 1%  |\n",
    "\n",
    "### HW2.6 Discussion:\n",
    "  Since Bernoulli NB classifier emphaised on the appearance of word, while Multinomial NB classifier emphasizes on the frequency of the word, one would like to choose Bernoulli classifier for SPAM detection by focusing keywords appearing in SPAM emails. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  HW2.7 Preprocess the Entire Spam Dataset (OPTIONAL) <a name=\"2.7\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File train-Enron-1.txt was generated!\n",
      "rm: `/user/shihyu/train-Enron-1.txt': No such file or directory\n",
      "train-Enron-1.txt is uploaded to hdfs.\n"
     ]
    }
   ],
   "source": [
    "# Put the download folder at the same place of this notebook\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# function to read one folder into text\n",
    "def readmail(path, isSpam, handler):\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    for file_name in files:\n",
    "        id = '.'.join(file_name.split('.')[:-2])\n",
    "        f = open(path+file_name, 'r')\n",
    "        mail = f.read().split('\\n', 1)\n",
    "        subject = mail[0].strip('Subject:').strip()\n",
    "        body = ' '.join(mail[1].split()).strip()\n",
    "        row = '%s\\t%s\\t%s\\t%s' %(id, isSpam, subject, 'NA' if not body else body)\n",
    "        f.close()\n",
    "        handler.write(row + '\\n')\n",
    "\n",
    "# read two folder separately\n",
    "text_file = open(\"train-Enron-1.txt\", \"w\")\n",
    "readmail('./enron1-Training-Data-RAW/ham/', '0', text_file)\n",
    "readmail('./enron1-Training-Data-RAW/spam/', '1', text_file)\n",
    "text_file.close()\n",
    "print 'File train-Enron-1.txt was generated!'\n",
    "\n",
    "!hdfs dfs -rm /user/shihyu/train-Enron-1.txt\n",
    "!hdfs dfs -put train-Enron-1.txt /user/shihyu\n",
    "print 'train-Enron-1.txt is uploaded to hdfs.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.8 Build and evaluate a NB classifier on the  Entire Spam Dataset (OPTIONAL) <a name=\"2.8\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `Emails_8_classification_results': No such file or directory\n",
      "Deleted prob_em\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob2182033911936835284.jar tmpDir=null\n",
      "16/09/10 07:03:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 07:03:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 07:03:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/10 07:03:57 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/09/10 07:03:57 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/10 07:03:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0061\n",
      "16/09/10 07:03:57 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0061\n",
      "16/09/10 07:03:57 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0061/\n",
      "16/09/10 07:03:57 INFO mapreduce.Job: Running job: job_1473444507507_0061\n",
      "16/09/10 07:04:05 INFO mapreduce.Job: Job job_1473444507507_0061 running in uber mode : false\n",
      "16/09/10 07:04:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/10 07:04:15 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/10 07:04:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/10 07:04:26 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/10 07:04:26 INFO mapreduce.Job: Job job_1473444507507_0061 completed successfully\n",
      "16/09/10 07:04:27 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=28116899\n",
      "\t\tFILE: Number of bytes written=56589444\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5413593\n",
      "\t\tHDFS: Number of bytes written=576332\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16575\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8133\n",
      "\t\tTotal time spent by all map tasks (ms)=16575\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8133\n",
      "\t\tTotal vcore-seconds taken by all map tasks=16575\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8133\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=16972800\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=8328192\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5172\n",
      "\t\tMap output records=848224\n",
      "\t\tMap output bytes=26420445\n",
      "\t\tMap output materialized bytes=28116905\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=50535\n",
      "\t\tReduce shuffle bytes=28116905\n",
      "\t\tReduce input records=848224\n",
      "\t\tReduce output records=16086\n",
      "\t\tSpilled Records=1696448\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=172\n",
      "\t\tCPU time spent (ms)=8350\n",
      "\t\tPhysical memory (bytes) snapshot=761016320\n",
      "\t\tVirtual memory (bytes) snapshot=4687081472\n",
      "\t\tTotal committed heap usage (bytes)=704118784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5413367\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=576332\n",
      "16/09/10 07:04:27 INFO streaming.StreamJob: Output directory: prob_em\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob3630265869098973634.jar tmpDir=null\n",
      "16/09/10 07:04:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 07:04:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/10 07:04:30 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/10 07:04:30 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/10 07:04:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0062\n",
      "16/09/10 07:04:31 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0062\n",
      "16/09/10 07:04:31 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0062/\n",
      "16/09/10 07:04:31 INFO mapreduce.Job: Running job: job_1473444507507_0062\n",
      "16/09/10 07:04:39 INFO mapreduce.Job: Job job_1473444507507_0062 running in uber mode : false\n",
      "16/09/10 07:04:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/10 07:04:52 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "16/09/10 07:04:53 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "16/09/10 07:04:54 INFO mapreduce.Job:  map 71% reduce 0%\n",
      "16/09/10 07:04:55 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/10 07:05:05 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "16/09/10 07:05:08 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "16/09/10 07:05:11 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "16/09/10 07:05:14 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "16/09/10 07:05:17 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/09/10 07:05:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/10 07:05:19 INFO mapreduce.Job: Job job_1473444507507_0062 completed successfully\n",
      "16/09/10 07:05:19 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=71793746\n",
      "\t\tFILE: Number of bytes written=143943210\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5413593\n",
      "\t\tHDFS: Number of bytes written=133738\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24481\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=21964\n",
      "\t\tTotal time spent by all map tasks (ms)=24481\n",
      "\t\tTotal time spent by all reduce tasks (ms)=21964\n",
      "\t\tTotal vcore-seconds taken by all map tasks=24481\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=21964\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=25068544\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=22491136\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5172\n",
      "\t\tMap output records=870033\n",
      "\t\tMap output bytes=70053674\n",
      "\t\tMap output materialized bytes=71793752\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=21258\n",
      "\t\tReduce shuffle bytes=71793752\n",
      "\t\tReduce input records=870033\n",
      "\t\tReduce output records=5174\n",
      "\t\tSpilled Records=1740066\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=206\n",
      "\t\tCPU time spent (ms)=32550\n",
      "\t\tPhysical memory (bytes) snapshot=812158976\n",
      "\t\tVirtual memory (bytes) snapshot=4677476352\n",
      "\t\tTotal committed heap usage (bytes)=878182400\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5413367\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=133738\n",
      "16/09/10 07:05:19 INFO streaming.StreamJob: Output directory: Emails_8_classification_results\n"
     ]
    }
   ],
   "source": [
    "# clean up HDFS\n",
    "!hdfs dfs -rm -r Emails_8_classification_results\n",
    "!hdfs dfs -rm -r prob_em\n",
    "\n",
    "# run trainning job\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper_t_em.py \\\n",
    "-reducer /home/cloudera/reducer_t_em_I.py \\\n",
    "-input /user/shihyu/train-Enron-1.txt \\\n",
    "-output prob_em\n",
    "\n",
    "# run classification job\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mapper_c_em_I.py \\\n",
    "-reducer /home/cloudera/reducer_c_em.py \\\n",
    "-input /user/shihyu/train-Enron-1.txt \\\n",
    "-output Emails_8_classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat Emails_8_classification_results/part-00000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the result is too long, so I do not show total result about entries. Since Hadoop is executed successfully, for this train-Enron-1.txt data set, rrror rate is : 0.008509, and the Number of messages with zero probability are spam(1360), ham(3590).\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.8.1 OPTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK- multinomial NB error rate is : 0.0200\n",
      "SK- Bernoulli   NB error rate is : 0.1900\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import csv, re, string\n",
    "import numpy as np\n",
    "\n",
    "# read email message, and organize training data\n",
    "f = open('train-Enron-1.txt', 'r')\n",
    "txt = f.read().strip()\n",
    "f.close()\n",
    "emails = txt.split('\\n')\n",
    "train_label = [msg.strip().split('\\t', 2)[1] for msg in emails]\n",
    "train_data = [msg.strip().split('\\t', 2)[-1] for msg in emails]\n",
    "# removing the funky characters\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "train_data = [' '.join(regex.sub(' ', msg).split()).decode('latin-1') for msg in train_data]\n",
    "\n",
    "# read test data\n",
    "with open('enronemail_1h.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    emails = list(reader)\n",
    "test_label = [msg[1] for msg in emails]\n",
    "test_data = [msg[2]+msg[3] if len(msg)==4 else msg[2] for msg in emails]\n",
    "    \n",
    "\n",
    "# feature vectorization\n",
    "Vectorizeration = CountVectorizer()\n",
    "Train = Vectorizeration.fit_transform(train_data) \n",
    "Test = Vectorizeration.transform(test_data)\n",
    "\n",
    "# multinomial Naive Bayes Classifier from sklearn\n",
    "clf_M = MultinomialNB()\n",
    "clf_M.fit(Train, train_label)\n",
    "pred_mnb = clf_M.predict(Test)\n",
    "training_error_mnb = 1.0*sum(pred_mnb != test_label) / len(test_label)\n",
    "\n",
    "# Bernoulli Naive Bayes Classifier from sklearn\n",
    "clf_B = BernoulliNB()\n",
    "clf_B.fit(Train, train_label)\n",
    "pred_bnb = clf_B.predict(Test)\n",
    "training_error_bnb = 1.0*sum(pred_bnb != test_label) / len(test_label)\n",
    "\n",
    "print 'SK- multinomial NB error rate is : %.4f' %training_error_mnb\n",
    "print 'SK- Bernoulli   NB error rate is : %.4f' %training_error_bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.8.1 Discussion: Performance comparison shown by following table.\n",
    "|   |Training Error   |   \n",
    "|---|:---:|\n",
    "| SK-Learn Multinomial NB | 0.02 |\n",
    "| SK-Learn Bernoulli NB  | 0.19 |\n",
    "| Multinomial NB (HW2.4)| 0.02  |\n",
    "| Multinomial NB (HW2.5)| 0.01  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
