{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261, Machine Learning at Scale\n",
    "--------\n",
    "#### Assignement:  week \\#3\n",
    "#### Shih Yu Chang\n",
    "### Due: 2016-09-20, 8AM PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.0.\n",
    "1. How do you merge  two sorted  lists/arrays of records of the form [key, value]?\n",
    "1. Where is this  used in Hadoop MapReduce? [Hint within the shuffle]\n",
    "1. What is  a combiner function in the context of Hadoop? \n",
    "1. Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "1. What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###*HW3.0.* Q&A\n",
    "\n",
    "1.2\n",
    "Merge sort is a sorting method which combines two sorted lists into a single sorted list of items. Merge sort benefits from distributed computing environment by sorting of the child lists. The merging of child lists into a single sorted list can be finished in linear time. Merge sorting is used in the shuffle stage of Hadoop to rearrange keys prior to sending them to the reducer. Key-value pairs from different mappers are sorted at their mappers, and then distributed across the reducers in a sorted form.\n",
    "\n",
    "3.\n",
    "Combiners are used for local aggregation during the mapper processes of Hadoop. They are run when the incomplete output from the mapper becomes too large to fit within memory. The combiner is responsible for reducing the data size so that the mapper can run faster by keeping data in memory and so that the network operations overhead in the partitioner is kept as small as possible. Depending on the size and scope of the problem, Hadoop will run combiners any number of times including zero with no input from the user. For this reason, it is important that the combiner is able to receive records in the same format of the mapper's output and emit data in the same format. The combining operation must also be associative and commutative so that the variable number of runs will not affect the final result.\n",
    "\n",
    "4.\n",
    "Combiners can be used in large word-count operations. A typical mapper output for a word-count problem will, in general, be much greater than the size of the document since it emits each individual word and the counter associated with it. Transferring this data across the network will downgrade performance of this operation, as well as making the subsequent sorting operation take much longer. Adding a combiner can reduce the size of the mapper output.\n",
    "\n",
    "5.\n",
    "Shuffle happens after all mapper tasks complete, but before reducer tasks start. All key-value pairs are sorted by key, and the same key is guaranteed to be delivered to the same reducer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1 consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/shihyu/Consumer_Complaints.csv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/shihyu\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/shihyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r1.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    line=line.strip()\n",
    "    #Since product at second field, extract from index 1 \n",
    "    product=line.split(',')[1] \n",
    "    # emit product name as key, no need for value \n",
    "    print \"%s\\t%s\" %(product, 'na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce3r1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r1.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    product = line.split('\\t')[0].strip()\n",
    "    try:\n",
    "        #Iterate the counter depending on the product\n",
    "        if product.lower()=='debt collection':\n",
    "            sys.stderr.write(\"reporter:counter:Debt,Total,1\\n\")\n",
    "        if product.lower()=='mortgage':\n",
    "            sys.stderr.write(\"reporter:counter:Mortgage,Total,1\\n\")\n",
    "        else:\n",
    "            sys.stderr.write(\"reporter:counter:Others,Total,1\\n\")\n",
    "    except:\n",
    "        # must be a header record so skip it\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s1\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob5972900019385505293.jar tmpDir=null\n",
      "16/09/12 20:01:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/12 20:01:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/12 20:01:28 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/12 20:01:28 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/12 20:01:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0065\n",
      "16/09/12 20:01:29 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0065\n",
      "16/09/12 20:01:29 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0065/\n",
      "16/09/12 20:01:29 INFO mapreduce.Job: Running job: job_1473444507507_0065\n",
      "16/09/12 20:01:37 INFO mapreduce.Job: Job job_1473444507507_0065 running in uber mode : false\n",
      "16/09/12 20:01:37 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/12 20:01:48 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/12 20:01:49 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/12 20:01:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/12 20:01:57 INFO mapreduce.Job: Job job_1473444507507_0065 completed successfully\n",
      "16/09/12 20:01:58 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5817067\n",
      "\t\tFILE: Number of bytes written=11989780\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910820\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18726\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6190\n",
      "\t\tTotal time spent by all map tasks (ms)=18726\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6190\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18726\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6190\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19175424\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6338560\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=5191235\n",
      "\t\tMap output materialized bytes=5817073\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=5817073\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=331\n",
      "\t\tCPU time spent (ms)=6280\n",
      "\t\tPhysical memory (bytes) snapshot=816939008\n",
      "\t\tVirtual memory (bytes) snapshot=4693041152\n",
      "\t\tTotal committed heap usage (bytes)=881328128\n",
      "\tDebt\n",
      "\t\tTotal=44372\n",
      "\tMortgage\n",
      "\t\tTotal=125752\n",
      "\tOthers\n",
      "\t\tTotal=187161\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "16/09/12 20:01:58 INFO streaming.StreamJob: Output directory: result3s1\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mappe3r1.py\n",
    "!chmod a+x reduce3r1.py\n",
    "!hdfs dfs -rm -r result3s1\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-mapper /home/cloudera/mappe3r1.py \\\n",
    "-reducer /home/cloudera/reduce3r1.py \\\n",
    "-input /user/shihyu/Consumer_Complaints.csv \\\n",
    "-output result3s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above results show that there are 44372 debt records, 125752 mortgage records, and 187161 other records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single record dataset:  foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/shihyu/testfil3e2a.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "#Create a test file that we can use to test our code\n",
    "! echo \"foo foo quux labs foo bar quux\" > testfil3e2a.txt\n",
    "!hdfs dfs -mkdir -p /user/shihyu\n",
    "!hdfs dfs -put testfil3e2a.txt /user/shihyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r2a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r2a.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "#counter\n",
    "count = 0\n",
    "#Increment script call counter once when the file runs\n",
    "sys.stderr.write(\"reporter:counter:Mapper2a,Script Calls,1\\n\") \n",
    "for line in sys.stdin:\n",
    "    #Increment line call counter when we process a new line\n",
    "    sys.stderr.write(\"reporter:counter:Mapper2a,Line Calls,1\\n\")    \n",
    "    line=line.strip()\n",
    "    words=line.split()\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce3r2a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r2a.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "tmp_word=''\n",
    "#Counter for the chosen word\n",
    "count = 0 \n",
    "\n",
    "#Increment script call counter once when the file runs\n",
    "sys.stderr.write(\"reporter:counter:Reducer2a,Script Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    #Parse line \n",
    "    line=line.strip().split('\\t')\n",
    "    word,tmp_count=line\n",
    "    if tmp_word==word:\n",
    "        count+=int(tmp_count) \n",
    "    else:\n",
    "        if tmp_word:\n",
    "            \n",
    "            #Increment line call counter when we emit a new word \n",
    "            sys.stderr.write(\"reporter:counter:Reducer2a,Line Calls,1\\n\")\n",
    "            print tmp_word+'\\t'+str(count)\n",
    "        tmp_word=word\n",
    "        count=int(tmp_count)\n",
    "\n",
    "#Do not forget  to emit final record \n",
    "if tmp_word:\n",
    "    sys.stderr.write(\"reporter:counter:Reducer2a,Line Calls,1\\n\")\n",
    "    print tmp_word+'\\t'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s2a\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob6115026488106076061.jar tmpDir=null\n",
      "16/09/12 22:07:41 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/12 22:07:42 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/12 22:07:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/12 22:07:42 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/09/12 22:07:42 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/09/12 22:07:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/12 22:07:42 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/12 22:07:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0069\n",
      "16/09/12 22:07:43 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0069\n",
      "16/09/12 22:07:43 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0069/\n",
      "16/09/12 22:07:43 INFO mapreduce.Job: Running job: job_1473444507507_0069\n",
      "16/09/12 22:07:51 INFO mapreduce.Job: Job job_1473444507507_0069 running in uber mode : false\n",
      "16/09/12 22:07:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/12 22:07:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/12 22:08:11 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/09/12 22:08:13 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/09/12 22:08:15 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/12 22:08:15 INFO mapreduce.Job: Job job_1473444507507_0069 completed successfully\n",
      "16/09/12 22:08:15 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=592813\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=142\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3852\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=47084\n",
      "\t\tTotal time spent by all map tasks (ms)=3852\n",
      "\t\tTotal time spent by all reduce tasks (ms)=47084\n",
      "\t\tTotal vcore-seconds taken by all map tasks=3852\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=47084\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=3944448\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=48214016\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=429\n",
      "\t\tCPU time spent (ms)=5290\n",
      "\t\tPhysical memory (bytes) snapshot=909508608\n",
      "\t\tVirtual memory (bytes) snapshot=7822495744\n",
      "\t\tTotal committed heap usage (bytes)=844627968\n",
      "\tMapper2a\n",
      "\t\tLine Calls=1\n",
      "\t\tScript Calls=1\n",
      "\tReducer2a\n",
      "\t\tLine Calls=4\n",
      "\t\tScript Calls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/09/12 22:08:15 INFO streaming.StreamJob: Output directory: result3s2a\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mappe3r2a.py\n",
    "!chmod a+x reduce3r2a.py\n",
    "!hdfs dfs -rm -r result3s2a\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/mappe3r2a.py \\\n",
    "-reducer /home/cloudera/reduce3r2a.py \\\n",
    "-input /user/shihyu/testfil3e2a.txt \\\n",
    "-output result3s2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\t1\r\n",
      "foo\t3\r\n",
      "labs\t1\r\n",
      "quux\t2\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat result3s2a/part-00000 | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since there are four different words, the value of mapper counter is one and the value of reducer counter is four."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mulitple mappers and reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mappe3r2b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r2b.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "count = 0 \n",
    "sys.stderr.write(\"reporter:counter:Mapper2b,Script Count,1\\n\") \n",
    "for line in reader(sys.stdin):\n",
    "    sys.stderr.write(\"reporter:counter:Mapper2b,Line Count,1\\n\")    \n",
    "    # Considering words in compaints issue \n",
    "    words = re.findall(WORD_RE, line[3])\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reduce3r2b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r2b.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "tmp_word=''\n",
    "#Counter for the chosen word\n",
    "count = 0 \n",
    "\n",
    "#Increment script call counter once when the file runs\n",
    "sys.stderr.write(\"reporter:counter:Reducer2a,Script Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    #Parse line \n",
    "    line=line.strip().split('\\t')\n",
    "    word,tmp_count=line\n",
    "    if tmp_word==word:\n",
    "        count+=int(tmp_count) \n",
    "    else:\n",
    "        if tmp_word:\n",
    "            \n",
    "            #Increment line call counter when we emit a new word \n",
    "            sys.stderr.write(\"reporter:counter:Reducer2a,Line Calls,1\\n\")\n",
    "            print tmp_word+'\\t'+str(count)\n",
    "        tmp_word=word\n",
    "        count=int(tmp_count)\n",
    "\n",
    "#Do not forget  to emit final record \n",
    "if tmp_word:\n",
    "    sys.stderr.write(\"reporter:counter:Reducer2a,Line Calls,1\\n\")\n",
    "    print tmp_word+'\\t'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `result3s2b': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob8882929842593617907.jar tmpDir=null\n",
      "16/09/12 22:28:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/12 22:28:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/12 22:28:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/12 22:28:55 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/12 22:28:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/12 22:28:55 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/12 22:28:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0070\n",
      "16/09/12 22:28:56 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0070\n",
      "16/09/12 22:28:56 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0070/\n",
      "16/09/12 22:28:56 INFO mapreduce.Job: Running job: job_1473444507507_0070\n",
      "16/09/12 22:29:05 INFO mapreduce.Job: Job job_1473444507507_0070 running in uber mode : false\n",
      "16/09/12 22:29:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/12 22:29:20 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "16/09/12 22:29:22 INFO mapreduce.Job:  map 79% reduce 0%\n",
      "16/09/12 22:29:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/12 22:29:34 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/09/12 22:29:35 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/12 22:29:35 INFO mapreduce.Job: Job job_1473444507507_0070 completed successfully\n",
      "16/09/12 22:29:35 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16121345\n",
      "\t\tFILE: Number of bytes written=32716908\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910820\n",
      "\t\tHDFS: Number of bytes written=2342\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=28489\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19523\n",
      "\t\tTotal time spent by all map tasks (ms)=28489\n",
      "\t\tTotal time spent by all reduce tasks (ms)=19523\n",
      "\t\tTotal vcore-seconds taken by all map tasks=28489\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=19523\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=29172736\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=19991552\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348309\n",
      "\t\tMap output bytes=13424715\n",
      "\t\tMap output materialized bytes=16121357\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=188\n",
      "\t\tReduce shuffle bytes=16121357\n",
      "\t\tReduce input records=1348309\n",
      "\t\tReduce output records=188\n",
      "\t\tSpilled Records=2696618\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=413\n",
      "\t\tCPU time spent (ms)=19520\n",
      "\t\tPhysical memory (bytes) snapshot=990117888\n",
      "\t\tVirtual memory (bytes) snapshot=6244900864\n",
      "\t\tTotal committed heap usage (bytes)=1105199104\n",
      "\tMapper2b\n",
      "\t\tLine Count=312913\n",
      "\t\tScript Count=2\n",
      "\tReducer2a\n",
      "\t\tLine Calls=188\n",
      "\t\tScript Calls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2342\n",
      "16/09/12 22:29:35 INFO streaming.StreamJob: Output directory: result3s2b\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mappe3r2b.py\n",
    "!chmod a+x reduce3r2b.py\n",
    "!hdfs dfs -rm -r result3s2b\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-mapper /home/cloudera/mappe3r2b.py \\\n",
    "-reducer /home/cloudera/reduce3r2b.py \\\n",
    "-input /user/shihyu/Consumer_Complaints.csv \\\n",
    "-output result3s2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APR\t3431\r\n",
      "Account\t16555\r\n",
      "Applied\t139\r\n",
      "Arbitration\t168\r\n",
      "Bankruptcy\t222\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat result3s2b/part-00000 | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper is called twice, and reducer is also called twice. Line count for mapper is 312913, and line count for reducer is 188. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer Complaints Dataset using a Mapper, Reducer, and standalone combiner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `result3s2c': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob2891191680511399467.jar tmpDir=null\n",
      "16/09/13 07:22:26 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 07:22:26 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 07:22:27 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/13 07:22:27 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/13 07:22:27 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/13 07:22:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/13 07:22:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0071\n",
      "16/09/13 07:22:27 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0071\n",
      "16/09/13 07:22:27 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0071/\n",
      "16/09/13 07:22:27 INFO mapreduce.Job: Running job: job_1473444507507_0071\n",
      "16/09/13 07:22:36 INFO mapreduce.Job: Job job_1473444507507_0071 running in uber mode : false\n",
      "16/09/13 07:22:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/13 07:22:50 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "16/09/13 07:22:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/13 07:23:03 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/09/13 07:23:04 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/13 07:23:04 INFO mapreduce.Job: Job job_1473444507507_0071 completed successfully\n",
      "16/09/13 07:23:04 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4971\n",
      "\t\tFILE: Number of bytes written=485524\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910820\n",
      "\t\tHDFS: Number of bytes written=2342\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=30438\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14295\n",
      "\t\tTotal time spent by all map tasks (ms)=30438\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14295\n",
      "\t\tTotal vcore-seconds taken by all map tasks=30438\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=14295\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=31168512\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=14638080\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348309\n",
      "\t\tMap output bytes=13424715\n",
      "\t\tMap output materialized bytes=4983\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=1348309\n",
      "\t\tCombine output records=347\n",
      "\t\tReduce input groups=188\n",
      "\t\tReduce shuffle bytes=4983\n",
      "\t\tReduce input records=347\n",
      "\t\tReduce output records=188\n",
      "\t\tSpilled Records=694\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=315\n",
      "\t\tCPU time spent (ms)=17960\n",
      "\t\tPhysical memory (bytes) snapshot=961658880\n",
      "\t\tVirtual memory (bytes) snapshot=6246400000\n",
      "\t\tTotal committed heap usage (bytes)=1034420224\n",
      "\tMapper2b\n",
      "\t\tLine Count=312913\n",
      "\t\tScript Count=2\n",
      "\tReducer2a\n",
      "\t\tLine Calls=535\n",
      "\t\tScript Calls=6\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2342\n",
      "16/09/13 07:23:04 INFO streaming.StreamJob: Output directory: result3s2c\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mappe3r2b.py\n",
    "!chmod a+x reduce3r2b.py\n",
    "!hdfs dfs -rm -r result3s2c\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-mapper /home/cloudera/mappe3r2b.py \\\n",
    "-combiner /home/cloudera/reduce3r2b.py \\\n",
    "-reducer /home/cloudera/reduce3r2b.py \\\n",
    "-input /user/shihyu/Consumer_Complaints.csv \\\n",
    "-output result3s2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APR\t3431\r\n",
      "Account\t16555\r\n",
      "Applied\t139\r\n",
      "Arbitration\t168\r\n",
      "Bankruptcy\t222\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat result3s2c/part-00000 | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, when we add the combiner, we see that it runs four times, in addition to the two map and reduce tasks. Since we use the reducer as a combiner, we have seen 2 map tasks and 6 reduce tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a single reducer: Top 50 most frequent terms and bottom 10 tokens (least frequent items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r2d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r2d.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "total_words = 0\n",
    "\n",
    "count = 0 \n",
    "sys.stderr.write(\"reporter:counter:Mapper2d,Script Count,1\\n\") \n",
    "for line in reader(sys.stdin):\n",
    "    sys.stderr.write(\"reporter:counter:Mapper2d,Line Count,1\\n\")    \n",
    "    # Considering words in compaints issue \n",
    "    words = re.findall(WORD_RE, line[3])\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "        total_words = total_words + 1\n",
    "        \n",
    "#Also print out total words        \n",
    "        \n",
    "print '%s\\t%s' % ('!!Total', str(total_words))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce3r2d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r2d.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "tmp_word=''\n",
    "#Counter for the chosen word\n",
    "count = 0 \n",
    "total_num_words=0\n",
    "\n",
    "#Increment script call counter once when the file runs\n",
    "sys.stderr.write(\"reporter:counter:Reducer2d,Script Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    #Parse line \n",
    "    line=line.strip().split('\\t')\n",
    "    word,tmp_count=line\n",
    "    tmp_count=int(tmp_count)\n",
    "    \n",
    "    if word=='!!Total':\n",
    "        total_num_words = tmp_count\n",
    "        continue\n",
    "    \n",
    "    if tmp_word==word:\n",
    "        count+=int(tmp_count) \n",
    "    else:\n",
    "        if tmp_word:\n",
    "            \n",
    "            #Increment line call counter when we emit a new word \n",
    "            sys.stderr.write(\"reporter:counter:Reducer2d,Line Calls,1\\n\")\n",
    "            print tmp_word+'\\t'+str(count)+'\\t'+str((count+0.0)/(total_num_words + 0.0))\n",
    "          \n",
    "        tmp_word=word\n",
    "        count=int(tmp_count)\n",
    "\n",
    "#Do not forget  to emit final record \n",
    "if tmp_word:\n",
    "    sys.stderr.write(\"reporter:counter:Reducer2d,Line Calls,1\\n\")\n",
    "    print tmp_word+'\\t'+str(count)+'\\t'+str((count+0.0)/(total_num_words + 0.0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sorting via the Hadoop Shuffle using break tie mapper/reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing break_tie.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile break_tie.py  \n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s2d\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob4950492321238404515.jar tmpDir=null\n",
      "16/09/13 10:08:47 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 10:08:47 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 10:08:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/13 10:08:48 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/13 10:08:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/13 10:08:48 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/13 10:08:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0076\n",
      "16/09/13 10:08:49 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0076\n",
      "16/09/13 10:08:49 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0076/\n",
      "16/09/13 10:08:49 INFO mapreduce.Job: Running job: job_1473444507507_0076\n",
      "16/09/13 10:08:58 INFO mapreduce.Job: Job job_1473444507507_0076 running in uber mode : false\n",
      "16/09/13 10:08:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/13 10:09:14 INFO mapreduce.Job:  map 14% reduce 0%\n",
      "16/09/13 10:09:17 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/09/13 10:09:18 INFO mapreduce.Job:  map 36% reduce 0%\n",
      "16/09/13 10:09:20 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "16/09/13 10:09:22 INFO mapreduce.Job:  map 72% reduce 0%\n",
      "16/09/13 10:09:25 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/09/13 10:09:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/13 10:09:39 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "16/09/13 10:09:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/13 10:09:41 INFO mapreduce.Job: Job job_1473444507507_0076 completed successfully\n",
      "16/09/13 10:09:41 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16121373\n",
      "\t\tFILE: Number of bytes written=32598398\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910820\n",
      "\t\tHDFS: Number of bytes written=5508\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=53152\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15674\n",
      "\t\tTotal time spent by all map tasks (ms)=53152\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15674\n",
      "\t\tTotal vcore-seconds taken by all map tasks=53152\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=15674\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=54427648\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=16050176\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348311\n",
      "\t\tMap output bytes=13424745\n",
      "\t\tMap output materialized bytes=16121379\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=189\n",
      "\t\tReduce shuffle bytes=16121379\n",
      "\t\tReduce input records=1348311\n",
      "\t\tReduce output records=188\n",
      "\t\tSpilled Records=2696622\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1192\n",
      "\t\tCPU time spent (ms)=32390\n",
      "\t\tPhysical memory (bytes) snapshot=1173905408\n",
      "\t\tVirtual memory (bytes) snapshot=4689772544\n",
      "\t\tTotal committed heap usage (bytes)=1346895872\n",
      "\tMapper2d\n",
      "\t\tLine Count=312913\n",
      "\t\tScript Count=2\n",
      "\tReducer2d\n",
      "\t\tLine Calls=188\n",
      "\t\tScript Calls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5508\n",
      "16/09/13 10:09:41 INFO streaming.StreamJob: Output directory: result3s2d\n"
     ]
    }
   ],
   "source": [
    "# Generate Hadoop results without sorting\n",
    "!chmod a+x mappe3r2d.py\n",
    "!chmod a+x reduce3r2d.py\n",
    "!hdfs dfs -rm -r result3s2d\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/mappe3r2d.py \\\n",
    "-reducer /home/cloudera/reduce3r2d.py \\\n",
    "-input /user/shihyu/Consumer_Complaints.csv \\\n",
    "-output result3s2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APR\t3431\t0.00514780239401\r\n",
      "ATM\t2422\t0.0036339193816\r\n",
      "Account\t16555\t0.0248387842124\r\n",
      "Advertising\t1193\t0.00178995285807\r\n",
      "Application\t8868\t0.0133053662577\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat result3s2d/* | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s2d_sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob4914841511058435259.jar tmpDir=null\n",
      "16/09/13 10:10:07 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 10:10:07 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 10:10:08 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/13 10:10:08 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/13 10:10:08 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/13 10:10:08 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/09/13 10:10:08 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/09/13 10:10:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/13 10:10:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0077\n",
      "16/09/13 10:10:09 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0077\n",
      "16/09/13 10:10:09 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0077/\n",
      "16/09/13 10:10:09 INFO mapreduce.Job: Running job: job_1473444507507_0077\n",
      "16/09/13 10:10:20 INFO mapreduce.Job: Job job_1473444507507_0077 running in uber mode : false\n",
      "16/09/13 10:10:20 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/13 10:10:31 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/13 10:10:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/13 10:10:42 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/13 10:10:42 INFO mapreduce.Job: Job job_1473444507507_0077 completed successfully\n",
      "16/09/13 10:10:43 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5890\n",
      "\t\tFILE: Number of bytes written=368821\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8500\n",
      "\t\tHDFS: Number of bytes written=5508\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18116\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7683\n",
      "\t\tTotal time spent by all map tasks (ms)=18116\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7683\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18116\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7683\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18550784\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7867392\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=188\n",
      "\t\tMap output records=188\n",
      "\t\tMap output bytes=5508\n",
      "\t\tMap output materialized bytes=5896\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=188\n",
      "\t\tReduce shuffle bytes=5896\n",
      "\t\tReduce input records=188\n",
      "\t\tReduce output records=188\n",
      "\t\tSpilled Records=376\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=272\n",
      "\t\tCPU time spent (ms)=3760\n",
      "\t\tPhysical memory (bytes) snapshot=671367168\n",
      "\t\tVirtual memory (bytes) snapshot=4693913600\n",
      "\t\tTotal committed heap usage (bytes)=663748608\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8262\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5508\n",
      "16/09/13 10:10:43 INFO streaming.StreamJob: Output directory: result3s2d_sorted\n"
     ]
    }
   ],
   "source": [
    "### Begin sorting\n",
    "!chmod a+x break_tie.py\n",
    "!hdfs dfs -rm -r result3s2d_sorted\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options='-k2,2nr' \\\n",
    "-mapper /home/cloudera/break_tie.py \\\n",
    "-reducer /home/cloudera/break_tie.py \\\n",
    "-input result3s2d \\\n",
    "-output result3s2d_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Most Common Words:\n",
      "Word | Frequency | Relative Frequency\n",
      "Loan\t107254\t0.160921713193\n",
      "collection\t70487\t0.105757256586\n",
      "modification\t70487\t0.105757256586\n",
      "foreclosure\t70487\t0.105757256586\n",
      "account\t40893\t0.0613550228208\n",
      "or\t40508\t0.0607773766763\n",
      "credit\t40483\t0.0607398671864\n",
      "payments\t39993\t0.0600046811843\n",
      "escrow\t36767\t0.0551644566075\n",
      "servicing\t36767\t0.0551644566075\n",
      "report\t34903\t0.0523677490405\n",
      "Incorrect\t29133\t0.0437105587714\n",
      "on\t29069\t0.0436145344772\n",
      "information\t29069\t0.0436145344772\n",
      "debt\t26531\t0.0398065710625\n",
      "not\t18477\t0.027722513796\n",
      "owed\t17972\t0.0269648221\n",
      "Cont'd\t17972\t0.0269648221\n",
      "attempts\t17972\t0.0269648221\n",
      "collect\t17972\t0.0269648221\n",
      "Account\t16555\t0.0248387842124\n",
      "and\t16448\t0.0246782435956\n",
      "closing\t16205\t0.0243136513538\n",
      "management\t16205\t0.0243136513538\n",
      "opening\t16205\t0.0243136513538\n",
      "Credit\t14768\t0.0221576058743\n",
      "of\t13983\t0.0209798078914\n",
      "loan\t12376\t0.0185686978806\n",
      "my\t10731\t0.0161005734451\n",
      "withdrawals\t10555\t0.0158365066362\n",
      "Deposits\t10555\t0.0158365066362\n",
      "Problems\t9484\t0.0142296000888\n",
      "Application\t8868\t0.0133053662577\n",
      "Communication\t8671\t0.0130097914772\n",
      "tactics\t8671\t0.0130097914772\n",
      "originator\t8625\t0.0129407740158\n",
      "mortgage\t8625\t0.0129407740158\n",
      "broker\t8625\t0.0129407740158\n",
      "to\t8401\t0.0126046889863\n",
      "Billing\t8158\t0.0122400967445\n",
      "Other\t7886\t0.0118319934944\n",
      "Disclosure\t7655\t0.0114854058077\n",
      "verification\t7655\t0.0114854058077\n",
      "disputes\t6938\t0.0104096336373\n",
      "reporting\t6559\t0.00984098977041\n",
      "lease\t6337\t0.00950790550009\n",
      "the\t6248\t0.00937437171604\n",
      "funds\t5663\t0.00849664965236\n",
      "low\t5663\t0.00849664965236\n",
      "by\t5663\t0.00849664965236\n",
      "cat: Unable to write to output stream.\n",
      "10 Least Common Words:\n",
      "Word | Frequency | Relative Frequency\n",
      "Payment\t92\t0.000138034922835\n",
      "credited\t92\t0.000138034922835\n",
      "Convenience\t75\t0.000112528469703\n",
      "checks\t75\t0.000112528469703\n",
      "amt\t71\t0.000106526951319\n",
      "day\t71\t0.000106526951319\n",
      "wrong\t71\t0.000106526951319\n",
      "disclosures\t64\t9.60242941464e-05\n",
      "missing\t64\t9.60242941464e-05\n",
      "Issue\t1\t1.50037959604e-06\n"
     ]
    }
   ],
   "source": [
    "! echo \"50 Most Common Words:\" # !hdfs dfs -cat result3s2c/part-00000 | head -5\n",
    "! echo \"Word | Frequency | Relative Frequency\"\n",
    "!hdfs dfs -cat result3s2d_sorted/* | head -50\n",
    "! echo \"10 Least Common Words:\"\n",
    "! echo \"Word | Frequency | Relative Frequency\"\n",
    "!hdfs dfs -cat result3s2d_sorted/* | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1  \n",
    "Using **2 reducers**: What are the top **50 most frequent terms** in your word count analysis? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r21_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r21_t.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "total_words = 0 \n",
    "sys.stderr.write(\"reporter:counter:Mapper21,Script Count,1\\n\") \n",
    "for line in reader(sys.stdin):\n",
    "    sys.stderr.write(\"reporter:counter:Mapper21,Line Count,1\\n\")    \n",
    "    # Considering words in compaints issue \n",
    "    words = re.findall(WORD_RE, line[3])\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "        if word[0].lower() == 'a':\n",
    "            print '%s\\t%s' % ('*', 1)\n",
    "        else: \n",
    "            print '%s\\t%s' % ('#', 1)\n",
    "        total_words = total_words + 1\n",
    "            \n",
    "print '%s\\t%s' % ('!!Total', str(total_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combine3r21.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combine3r21.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read(file, separator='\\t'):\n",
    "    for line in file:\n",
    "        yield line.rstrip().split(separator, 1)\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "data = read(sys.stdin, separator='\\t')\n",
    "# groupby groups multiple word-count pairs by word,\n",
    "for tmp_word, group in groupby(data, itemgetter(0)):\n",
    "    try:\n",
    "        total_count = sum(int(count) for tmp_word, count in group)\n",
    "        sys.stderr.write(\"reporter:counter:Code Call Counters,combiner pairs,1\\n\")\n",
    "        sys.stdout.write(\"{0}{1}{2}\\n\".format(tmp_word, '\\t', total_count))\n",
    "        if tmp_word == '*':\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,combiner total flags,1\\n\")\n",
    "    except ValueError:\n",
    "        sys.stderr.write(\"reporter:counter:Code Call Counters,combiner skipped pairs,1\\n\")            \n",
    "        # count was not a number, so silently discard this item\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce3r21_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r21_t.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read(file, separator='\\t'):\n",
    "    for line in file:\n",
    "        yield line.rstrip().split(separator, 1)\n",
    "\n",
    "total = 1\n",
    "total_first = True\n",
    "total_words = 0\n",
    "    \n",
    "# input comes from STDIN (standard input)\n",
    "data = read(sys.stdin, separator='\\t')\n",
    "\n",
    "\n",
    "#Increment script call counter once when the file runs\n",
    "sys.stderr.write(\"reporter:counter:Reducer21,Script Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    #Parse line \n",
    "    line=line.strip().split('\\t')\n",
    "    word,tmp_count=line\n",
    "    tmp_count=int(tmp_count)\n",
    "    if word=='!!Total':\n",
    "        total_words = tmp_count\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "for tmp_word, group in groupby(data, itemgetter(0)):\n",
    "    try:\n",
    "        total_count = sum(int(count) for tmp_word, count in group)\n",
    "        \n",
    "        if tmp_word == '*':\n",
    "            total = total_count\n",
    "            sys.stderr.write(\"reporter:counter:Reducer total indicators,1\\n\")\n",
    "            sys.stderr.write(\"reporter:counter:Reducer word count,{0}\\n\".format(total))\n",
    "            if total_first:\n",
    "                sys.stderr.write(\"reporter:counter:Reducer recvd total first,1\\n\")\n",
    "        elif tmp_word == '#':\n",
    "            total =total_count\n",
    "            sys.stderr.write(\"reporter:counter:Reducer total indicators,1\\n\")\n",
    "            sys.stderr.write(\"reporter:counter:Reducer word count,{0}\\n\".format(total))\n",
    "            if total_first:\n",
    "                sys.stderr.write(\"reporter:counter:Reducer recvd total first,1\\n\")           \n",
    "        else:\n",
    "            #total = total_1 + total_2\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,reducer processed,1\\n\")\n",
    "            print tmp_word+'\\t'+str(total_count)+'\\t'+str(float(total_count)/float(total_words)) \n",
    "\n",
    "            total_first = False\n",
    "                \n",
    "    except ValueError:\n",
    "        sys.stderr.write(\"reporter:counter:Code Call Counters,reducer skipped pairs,1\\n\")            \n",
    "          \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s21_t\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob4180188453315537109.jar tmpDir=null\n",
      "16/09/13 20:24:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 20:24:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 20:24:18 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/13 20:24:18 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/13 20:24:18 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/13 20:24:18 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/13 20:24:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0105\n",
      "16/09/13 20:24:19 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0105\n",
      "16/09/13 20:24:19 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0105/\n",
      "16/09/13 20:24:19 INFO mapreduce.Job: Running job: job_1473444507507_0105\n",
      "16/09/13 20:24:30 INFO mapreduce.Job: Job job_1473444507507_0105 running in uber mode : false\n",
      "16/09/13 20:24:30 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/13 20:24:46 INFO mapreduce.Job:  map 9% reduce 0%\n",
      "16/09/13 20:24:48 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "16/09/13 20:24:49 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "16/09/13 20:24:52 INFO mapreduce.Job:  map 34% reduce 0%\n",
      "16/09/13 20:24:55 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "16/09/13 20:24:58 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "16/09/13 20:25:02 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "16/09/13 20:25:05 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/09/13 20:25:06 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/09/13 20:25:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/13 20:25:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/13 20:25:21 INFO mapreduce.Job: Job job_1473444507507_0105 completed successfully\n",
      "16/09/13 20:25:21 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5047\n",
      "\t\tFILE: Number of bytes written=485704\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910820\n",
      "\t\tHDFS: Number of bytes written=2868\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=68922\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=20634\n",
      "\t\tTotal time spent by all map tasks (ms)=68922\n",
      "\t\tTotal time spent by all reduce tasks (ms)=20634\n",
      "\t\tTotal vcore-seconds taken by all map tasks=68922\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=20634\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=70576128\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=21129216\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=2696620\n",
      "\t\tMap output bytes=18817981\n",
      "\t\tMap output materialized bytes=5059\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=2696620\n",
      "\t\tCombine output records=353\n",
      "\t\tReduce input groups=191\n",
      "\t\tReduce shuffle bytes=5059\n",
      "\t\tReduce input records=353\n",
      "\t\tReduce output records=97\n",
      "\t\tSpilled Records=706\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=609\n",
      "\t\tCPU time spent (ms)=33630\n",
      "\t\tPhysical memory (bytes) snapshot=1049341952\n",
      "\t\tVirtual memory (bytes) snapshot=6254194688\n",
      "\t\tTotal committed heap usage (bytes)=962068480\n",
      "\tCode Call Counters\n",
      "\t\tcombiner pairs=353\n",
      "\t\tcombiner total flags=2\n",
      "\t\treducer processed=97\n",
      "\tMapper21\n",
      "\t\tLine Count=312913\n",
      "\t\tScript Count=2\n",
      "\tReducer21\n",
      "\t\tScript Calls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2868\n",
      "16/09/13 20:25:21 INFO streaming.StreamJob: Output directory: result3s21_t\n"
     ]
    }
   ],
   "source": [
    "# Generate Hadoop results without sorting\n",
    "!chmod a+x mappe3r21_t.py\n",
    "!chmod a+x combine3r21.py\n",
    "!chmod a+x reduce3r21_t.py\n",
    "!hdfs dfs -rm -r result3s21_t\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-mapper /home/cloudera/mappe3r21_t.py \\\n",
    "-combiner /home/cloudera/combine3r21.py \\\n",
    "-reducer /home/cloudera/reduce3r21_t.py \\\n",
    "-input /user/shihyu/Consumer_Complaints.csv \\\n",
    "-output result3s21_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Most Common Words with 2 reducers:\n",
      "Word | Frequency | Relative Frequency\n",
      "!!Total\t681811\t1.02297531275\n",
      "ATM\t2422\t0.0036339193816\n",
      "Advertising\t1193\t0.00178995285807\n",
      "Application\t8868\t0.0133053662577\n",
      "Balance\t597\t0.000895726618835\n",
      "Cancelling\t2795\t0.00419356097093\n",
      "Charged\t878\t0.00131733328532\n",
      "Collection\t1907\t0.00286122388964\n",
      "Communication\t8671\t0.0130097914772\n",
      "Customer\t2734\t0.00410203781557\n",
      "Dealing\t1944\t0.0029167379347\n",
      "Embezzlement\t3276\t0.00491524355662\n",
      "Forbearance\t350\t0.000525132858613\n",
      "Fraud\t3842\t0.00576445840798\n",
      "Getting\t291\t0.000436610462447\n"
     ]
    }
   ],
   "source": [
    "! echo \"50 Most Common Words with 2 reducers:\" # !hdfs dfs -cat result3s2c/part-00000 | head -5\n",
    "! echo \"Word | Frequency | Relative Frequency\"\n",
    "!hdfs dfs -cat result3s21_t/* | head -15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s21_t_sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob8926523487061108341.jar tmpDir=null\n",
      "16/09/13 20:32:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 20:32:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 20:32:39 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/09/13 20:32:39 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/09/13 20:32:39 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/09/13 20:32:39 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/13 20:32:39 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/09/13 20:32:39 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/09/13 20:32:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/13 20:32:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0106\n",
      "16/09/13 20:32:40 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0106\n",
      "16/09/13 20:32:40 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0106/\n",
      "16/09/13 20:32:40 INFO mapreduce.Job: Running job: job_1473444507507_0106\n",
      "16/09/13 20:32:52 INFO mapreduce.Job: Job job_1473444507507_0106 running in uber mode : false\n",
      "16/09/13 20:32:52 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/13 20:33:06 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/09/13 20:33:10 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/09/13 20:33:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/13 20:33:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/13 20:33:18 INFO mapreduce.Job: Job job_1473444507507_0106 completed successfully\n",
      "16/09/13 20:33:18 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3068\n",
      "\t\tFILE: Number of bytes written=482229\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4665\n",
      "\t\tHDFS: Number of bytes written=2868\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=42946\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8876\n",
      "\t\tTotal time spent by all map tasks (ms)=42946\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8876\n",
      "\t\tTotal vcore-seconds taken by all map tasks=42946\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8876\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=43976704\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=9089024\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=97\n",
      "\t\tMap output records=97\n",
      "\t\tMap output bytes=2868\n",
      "\t\tMap output materialized bytes=3080\n",
      "\t\tInput split bytes=363\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=97\n",
      "\t\tReduce shuffle bytes=3080\n",
      "\t\tReduce input records=97\n",
      "\t\tReduce output records=97\n",
      "\t\tSpilled Records=194\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=843\n",
      "\t\tCPU time spent (ms)=4690\n",
      "\t\tPhysical memory (bytes) snapshot=964501504\n",
      "\t\tVirtual memory (bytes) snapshot=6268620800\n",
      "\t\tTotal committed heap usage (bytes)=860356608\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4302\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2868\n",
      "16/09/13 20:33:18 INFO streaming.StreamJob: Output directory: result3s21_t_sorted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Begin sorting\n",
    "!chmod a+x break_tie.py\n",
    "!hdfs dfs -rm -r result3s21_t_sorted\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options='-k2,2nr' \\\n",
    "-mapper /home/cloudera/break_tie.py \\\n",
    "-reducer /home/cloudera/break_tie.py \\\n",
    "-input result3s21_t \\\n",
    "-output result3s21_t_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Most Common Words with 2 reducers:\n",
      "Word | Frequency | Relative Frequency\n",
      "!!Total\t681811\t1.02297531275\n",
      "Loan\t107254\t0.160921713193\n",
      "collection\t70487\t0.105757256586\n",
      "modification\t70487\t0.105757256586\n",
      "servicing\t36767\t0.0551644566075\n",
      "report\t34903\t0.0523677490405\n",
      "information\t29069\t0.0436145344772\n",
      "attempts\t17972\t0.0269648221\n",
      "collect\t17972\t0.0269648221\n",
      "opening\t16205\t0.0243136513538\n",
      "loan\t12376\t0.0185686978806\n",
      "my\t10731\t0.0161005734451\n",
      "withdrawals\t10555\t0.0158365066362\n",
      "Problems\t9484\t0.0142296000888\n",
      "Application\t8868\t0.0133053662577\n",
      "Communication\t8671\t0.0130097914772\n",
      "mortgage\t8625\t0.0129407740158\n",
      "originator\t8625\t0.0129407740158\n",
      "Other\t7886\t0.0118319934944\n",
      "reporting\t6559\t0.00984098977041\n",
      "lease\t6337\t0.00950790550009\n",
      "low\t5663\t0.00849664965236\n",
      "funds\t5663\t0.00849664965236\n",
      "Managing\t5006\t0.00751090025777\n",
      "Improper\t4966\t0.00745088507392\n",
      "investigation\t4858\t0.00728884407755\n",
      "card\t4405\t0.00660917212055\n",
      "score\t4357\t0.00653715389994\n",
      "get\t4357\t0.00653715389994\n",
      "costs\t4350\t0.00652665124276\n",
      "interest\t4238\t0.00635860872801\n",
      "Taking\t4206\t0.00631059658093\n",
      "when\t4095\t0.00614405444577\n",
      "Fraud\t3842\t0.00576445840798\n",
      "are\t3821\t0.00573295043646\n",
      "pay\t3821\t0.00573295043646\n",
      "contact\t3710\t0.0055664083013\n",
      "statements\t3621\t0.00543287451725\n",
      "info\t3553\t0.00533084870472\n",
      "sharing\t3489\t0.00523482441058\n",
      "rate\t3431\t0.00514780239401\n",
      "money\t3365\t0.00504877734067\n",
      "Identity\t3276\t0.00491524355662\n",
      "Embezzlement\t3276\t0.00491524355662\n",
      "receiving\t3226\t0.00484022457682\n",
      "sending\t3226\t0.00484022457682\n",
      "fee\t3198\t0.00479821394813\n",
      "illegal\t2964\t0.00444712512266\n",
      "action\t2964\t0.00444712512266\n",
      "Cancelling\t2795\t0.00419356097093\n"
     ]
    }
   ],
   "source": [
    "! echo \"50 Most Common Words with 2 reducers:\" # !hdfs dfs -cat result3s2c/part-00000 | head -5\n",
    "! echo \"Word | Frequency | Relative Frequency\"\n",
    "!hdfs dfs -cat result3s21_t_sorted/* | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. Shopping Cart Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/shihyu/ProductPurchaseData.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "# put ProductPurchaseData.txt data \n",
    "!hdfs dfs -mkdir -p /user/shihyu\n",
    "!hdfs dfs -put ProductPurchaseData.txt /user/shihyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r3.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "product_count=0\n",
    "# cart index from 1\n",
    "cart_id = 0\n",
    "for line in sys.stdin:\n",
    "    line=line.strip()\n",
    "    products=line.split() #split on whitespace\n",
    "    cart_id = cart_id + 1  \n",
    "    for product in products:\n",
    "        product_count+=1        \n",
    "        print product+' '+str(cart_id)+' 1 '+str(len(products))\n",
    "                          \n",
    "\n",
    "\n",
    "#Emit total with special key for order inversion\n",
    "print '**Total '+'0'+' '+str(product_count)+' 0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce3r3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r3.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "tmp_product=None\n",
    "# counter for a paricular product\n",
    "count = 0 \n",
    "largest_basket_id=0\n",
    "largest_basket_size=0\n",
    "unique_products=0\n",
    "total_product_count=0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    #Parse line into fields\n",
    "    try:\n",
    "         product,cart_id,product_count,cart_total = line.strip().split(' ')\n",
    "    except ValueError:     \n",
    "        continue\n",
    "    \n",
    "   \n",
    "    \n",
    "    cart_total=int(cart_total)\n",
    "    \n",
    "    #Extract total products\n",
    "    if product=='**Total': \n",
    "        total_product_count+=int(product_count)\n",
    "        continue\n",
    "        \n",
    "    #Updated largest cart size and ID\n",
    "    if cart_total>largest_basket_size: \n",
    "        largest_basket_size=cart_total\n",
    "        largest_basket_id=cart_id\n",
    "  \n",
    "    if tmp_product==product:\n",
    "        count+=int(product_count)\n",
    "    else:\n",
    "        if tmp_product and tmp_product!='**Total':\n",
    "            print tmp_product+'\\t'+str(count)+'\\t'+ str((count + 0.0)/(total_product_count + 0.0))\n",
    "            unique_products+=1\n",
    "        tmp_product=product\n",
    "        count=int(product_count)\n",
    "# Dont forget last one        \n",
    "if tmp_product:\n",
    "    print tmp_product+'\\t'+str(count)+'\\t'+ str((count + 0.0)/(total_product_count + 0.0))\n",
    "    unique_products+=1\n",
    "    \n",
    "#Print aggregated stats separately with special key to make them easy to find\n",
    "print '*Largest Size Cart' + '\\t' + str(largest_basket_id) + '\\t '+ str(largest_basket_size)\n",
    "print '*Unique Products'+'\\t'+str(unique_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s3\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob3340586247327220801.jar tmpDir=null\n",
      "16/09/13 22:25:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 22:25:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 22:25:05 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/13 22:25:06 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/13 22:25:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/13 22:25:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/13 22:25:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0116\n",
      "16/09/13 22:25:06 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0116\n",
      "16/09/13 22:25:07 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0116/\n",
      "16/09/13 22:25:07 INFO mapreduce.Job: Running job: job_1473444507507_0116\n",
      "16/09/13 22:25:18 INFO mapreduce.Job: Job job_1473444507507_0116 running in uber mode : false\n",
      "16/09/13 22:25:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/13 22:25:34 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/13 22:25:35 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/13 22:25:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/13 22:25:46 INFO mapreduce.Job: Job job_1473444507507_0116 completed successfully\n",
      "16/09/13 22:25:46 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8406183\n",
      "\t\tFILE: Number of bytes written=17168009\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462851\n",
      "\t\tHDFS: Number of bytes written=368686\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=26298\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9015\n",
      "\t\tTotal time spent by all map tasks (ms)=26298\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9015\n",
      "\t\tTotal vcore-seconds taken by all map tasks=26298\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=9015\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=26929152\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=9231360\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380826\n",
      "\t\tMap output bytes=7644525\n",
      "\t\tMap output materialized bytes=8406189\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=380577\n",
      "\t\tReduce shuffle bytes=8406189\n",
      "\t\tReduce input records=380826\n",
      "\t\tReduce output records=12594\n",
      "\t\tSpilled Records=761652\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=285\n",
      "\t\tCPU time spent (ms)=11040\n",
      "\t\tPhysical memory (bytes) snapshot=830959616\n",
      "\t\tVirtual memory (bytes) snapshot=4695478272\n",
      "\t\tTotal committed heap usage (bytes)=744488960\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368686\n",
      "16/09/13 22:25:46 INFO streaming.StreamJob: Output directory: result3s3\n"
     ]
    }
   ],
   "source": [
    "# Generate Hadoop results without sorting\n",
    "!chmod a+x mappe3r3.py\n",
    "!chmod a+x reduce3r3.py\n",
    "!hdfs dfs -rm -r result3s3\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/mappe3r3.py \\\n",
    "-reducer /home/cloudera/reduce3r3.py \\\n",
    "-input /user/shihyu/ProductPurchaseData.txt \\\n",
    "-output result3s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s3_sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob5072414457235328786.jar tmpDir=null\n",
      "16/09/13 22:26:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 22:26:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/13 22:26:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/13 22:26:04 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/13 22:26:04 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/13 22:26:04 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/09/13 22:26:04 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/09/13 22:26:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/13 22:26:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0117\n",
      "16/09/13 22:26:05 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0117\n",
      "16/09/13 22:26:05 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0117/\n",
      "16/09/13 22:26:05 INFO mapreduce.Job: Running job: job_1473444507507_0117\n",
      "16/09/13 22:26:15 INFO mapreduce.Job: Job job_1473444507507_0117 running in uber mode : false\n",
      "16/09/13 22:26:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/13 22:26:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/13 22:26:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/13 22:26:46 INFO mapreduce.Job: Job job_1473444507507_0117 completed successfully\n",
      "16/09/13 22:26:46 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=393881\n",
      "\t\tFILE: Number of bytes written=1144797\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=373018\n",
      "\t\tHDFS: Number of bytes written=368686\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=28896\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11081\n",
      "\t\tTotal time spent by all map tasks (ms)=28896\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11081\n",
      "\t\tTotal vcore-seconds taken by all map tasks=28896\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=11081\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=29589504\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=11346944\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12594\n",
      "\t\tMap output records=12594\n",
      "\t\tMap output bytes=368687\n",
      "\t\tMap output materialized bytes=393887\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12594\n",
      "\t\tReduce shuffle bytes=393887\n",
      "\t\tReduce input records=12594\n",
      "\t\tReduce output records=12594\n",
      "\t\tSpilled Records=25188\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1151\n",
      "\t\tCPU time spent (ms)=8610\n",
      "\t\tPhysical memory (bytes) snapshot=915202048\n",
      "\t\tVirtual memory (bytes) snapshot=4678574080\n",
      "\t\tTotal committed heap usage (bytes)=861929472\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=372782\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368686\n",
      "16/09/13 22:26:46 INFO streaming.StreamJob: Output directory: result3s3_sorted\n"
     ]
    }
   ],
   "source": [
    "### Begin sorting\n",
    "!chmod a+x break_tie.py\n",
    "!hdfs dfs -rm -r result3s3_sorted\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options='-k2,2nr' \\\n",
    "-mapper /home/cloudera/break_tie.py \\\n",
    "-reducer /home/cloudera/break_tie.py \\\n",
    "-input result3s3 \\\n",
    "-output result3s3_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Most Common Products:\n",
      "Product | Frequency | Relative Frequency\n",
      "*Unique Products\t12592\n",
      "*Largest Size Cart\t6914\t 37\n",
      "DAI62779\t6667\t0.0175067747831\n",
      "FRO40251\t3881\t0.010191059387\n",
      "ELE17451\t3875\t0.0101753040775\n",
      "GRO73461\t3602\t0.00945843749344\n",
      "SNA80324\t3044\t0.00799319370628\n",
      "ELE32164\t2851\t0.0074863979161\n",
      "DAI75645\t2736\t0.00718442114993\n",
      "SNA45677\t2455\t0.0064465474865\n",
      "FRO31317\t2330\t0.0061183118711\n",
      "DAI85309\t2293\t0.00602115412894\n",
      "ELE26917\t2292\t0.00601852824402\n",
      "FRO80039\t2233\t0.00586360103355\n",
      "GRO21487\t2115\t0.00555374661261\n",
      "SNA99873\t2083\t0.00546971829507\n",
      "GRO59710\t2004\t0.00526227338613\n",
      "GRO71621\t1920\t0.00504169905258\n",
      "FRO85978\t1918\t0.00503644728273\n",
      "GRO30386\t1840\t0.00483162825872\n",
      "ELE74009\t1816\t0.00476860702057\n",
      "GRO56726\t1784\t0.00468457870302\n",
      "DAI63921\t1773\t0.00465569396887\n",
      "GRO46854\t1756\t0.00461105392517\n",
      "ELE66600\t1713\t0.00449814087347\n",
      "DAI83733\t1712\t0.00449551498855\n",
      "FRO32293\t1702\t0.00446925613932\n",
      "ELE66810\t1697\t0.0044561267147\n",
      "SNA55762\t1646\t0.00432220658362\n",
      "DAI22177\t1627\t0.00427231477008\n",
      "FRO78087\t1531\t0.00402022981745\n",
      "ELE99737\t1516\t0.0039808415436\n",
      "ELE34057\t1489\t0.00390994265067\n",
      "GRO94758\t1489\t0.00390994265067\n",
      "FRO35904\t1436\t0.00377077074974\n",
      "FRO53271\t1420\t0.00372875659097\n",
      "SNA93860\t1407\t0.00369462008697\n",
      "SNA90094\t1390\t0.00364998004327\n",
      "GRO38814\t1352\t0.00355019641619\n",
      "ELE56788\t1345\t0.00353181522173\n",
      "GRO61133\t1321\t0.00346879398357\n",
      "ELE74482\t1316\t0.00345566455896\n",
      "DAI88807\t1316\t0.00345566455896\n",
      "ELE59935\t1311\t0.00344253513434\n",
      "SNA96271\t1295\t0.00340052097557\n",
      "DAI43223\t1290\t0.00338739155095\n",
      "ELE91337\t1289\t0.00338476566603\n",
      "GRO15017\t1275\t0.0033480032771\n",
      "DAI31081\t1261\t0.00331124088818\n",
      "GRO81087\t1220\t0.00320357960633\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! echo \"50 Most Common Products:\" \n",
    "! echo \"Product | Frequency | Relative Frequency\"\n",
    "!hdfs dfs -cat result3s3_sorted/* | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**According to these results, we have 12592 unique products browsed.  The largest cart has 37 products, and the most commonly browsed product was DAI62779, which was selected by buyers 6667 times for a relative frequency of 0.0175.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get all products from the session\n",
    "    products = line.strip().split(' ')\n",
    "    cart_size = len(products)\n",
    "    if cart_size==0:\n",
    "        continue\n",
    "    \n",
    "    # sort products the pair is lexicographically sound\n",
    "    products.sort()\n",
    "    \n",
    "    # set pairs of products\n",
    "    pairs = [[products[i], products[j]] for i in range(cart_size) for j in range(i+1, cart_size)]\n",
    "    \n",
    "    # dummy record for total products count\n",
    "    print '%s,%s' %('*', 1)\n",
    "    \n",
    "    # emit product pairs\n",
    "    for pair in pairs:\n",
    "        print '%s_%s,%s' %(pair[0], pair[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combine3r4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combine3r4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "\n",
    "tmp_pair = None\n",
    "tmp_count = 0\n",
    "\n",
    "for line in sys.stdin:       \n",
    "    # get all products from each line \n",
    "    pair, count = line.strip().split(',', 1)\n",
    "    \n",
    "    # skip bad value \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # accumulate counts for whatever keys it receives\n",
    "    if tmp_pair == pair:\n",
    "        tmp_count += count\n",
    "    else:\n",
    "        # previous pair finishes streaming, emit results\n",
    "        if tmp_pair:            \n",
    "            print '%s,%s' %(tmp_pair, tmp_count)\n",
    "        # set new pair\n",
    "        tmp_pair = pair\n",
    "        tmp_count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce3r4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "\n",
    "total_products = 0\n",
    "min_support = 100\n",
    "tmp_pair = None\n",
    "tmp_count = 0\n",
    "\n",
    "for line in sys.stdin:       \n",
    "    # get all products from the session\n",
    "    pair, count = line.strip().split(',', 1)\n",
    "    \n",
    "    # skip bad count\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # get total sessions/baskets\n",
    "    if pair == '*':\n",
    "        total_products += count\n",
    "        continue\n",
    "        \n",
    "    # get pair count\n",
    "    if tmp_pair == pair:\n",
    "        tmp_count += count\n",
    "    else:\n",
    "        # previous pair finishes \n",
    "        if tmp_pair and tmp_count > min_support:\n",
    "            # emit\n",
    "            print '%s,%s,%s' %(tmp_pair, tmp_count, str(float(tmp_count)/float(total_products)))\n",
    "        # reset new pair\n",
    "        tmp_pair = pair\n",
    "        tmp_count = count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s4\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob7527278688606732722.jar tmpDir=null\n",
      "16/09/14 19:05:07 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/14 19:05:08 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/14 19:05:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/14 19:05:09 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/14 19:05:09 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/14 19:05:09 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/14 19:05:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0122\n",
      "16/09/14 19:05:10 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0122\n",
      "16/09/14 19:05:10 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0122/\n",
      "16/09/14 19:05:10 INFO mapreduce.Job: Running job: job_1473444507507_0122\n",
      "16/09/14 19:05:23 INFO mapreduce.Job: Job job_1473444507507_0122 running in uber mode : false\n",
      "16/09/14 19:05:23 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/14 19:05:42 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "16/09/14 19:05:44 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "16/09/14 19:05:45 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "16/09/14 19:05:47 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/09/14 19:05:55 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/09/14 19:05:59 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/14 19:06:09 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/09/14 19:06:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/14 19:06:11 INFO mapreduce.Job: Job job_1473444507507_0122 completed successfully\n",
      "16/09/14 19:06:12 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=23650909\n",
      "\t\tFILE: Number of bytes written=47658484\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462851\n",
      "\t\tHDFS: Number of bytes written=52179\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=64749\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12714\n",
      "\t\tTotal time spent by all map tasks (ms)=64749\n",
      "\t\tTotal time spent by all reduce tasks (ms)=12714\n",
      "\t\tTotal vcore-seconds taken by all map tasks=64749\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=12714\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=66302976\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=13019136\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2565158\n",
      "\t\tMap output bytes=53370702\n",
      "\t\tMap output materialized bytes=23650915\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=2565158\n",
      "\t\tCombine output records=1026710\n",
      "\t\tReduce input groups=980736\n",
      "\t\tReduce shuffle bytes=23650915\n",
      "\t\tReduce input records=1026710\n",
      "\t\tReduce output records=1311\n",
      "\t\tSpilled Records=2053420\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1497\n",
      "\t\tCPU time spent (ms)=41460\n",
      "\t\tPhysical memory (bytes) snapshot=1183547392\n",
      "\t\tVirtual memory (bytes) snapshot=4695547904\n",
      "\t\tTotal committed heap usage (bytes)=1208483840\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=52179\n",
      "16/09/14 19:06:12 INFO streaming.StreamJob: Output directory: result3s4\n"
     ]
    }
   ],
   "source": [
    "# Generate Hadoop results without sorting\n",
    "!chmod a+x mappe3r4.py\n",
    "!chmod a+x combine3r4.py\n",
    "!chmod a+x reduce3r4.py\n",
    "!hdfs dfs -rm -r result3s4\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/mappe3r4.py \\\n",
    "-combiner /home/cloudera/combine3r4.py \\\n",
    "-reducer /home/cloudera/reduce3r4.py \\\n",
    "-input /user/shihyu/ProductPurchaseData.txt \\\n",
    "-output result3s4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mappe3r4_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r4_s.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # just emit\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reduce3r4_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r4_s.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "n = 0\n",
    "top = 50\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # parse mappe3r4_s output  \n",
    "    pair, count, relative_freq = line.strip().split(',', 2)\n",
    "    n += 1\n",
    "    if n <= top:\n",
    "        w1, w2 = pair.split('_')\n",
    "        print '%s\\t%s\\t%s\\t%s' %(w1, w2, count, relative_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s4_sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob6464004570836655768.jar tmpDir=null\n",
      "16/09/14 19:52:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/14 19:53:00 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/14 19:53:01 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/14 19:53:01 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/14 19:53:01 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/14 19:53:01 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/09/14 19:53:01 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/09/14 19:53:01 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/09/14 19:53:01 INFO Configuration.deprecation: map.output.key.value.fields.spec is deprecated. Instead, use mapreduce.fieldsel.map.output.key.value.fields.spec\n",
      "16/09/14 19:53:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/14 19:53:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0125\n",
      "16/09/14 19:53:02 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0125\n",
      "16/09/14 19:53:02 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0125/\n",
      "16/09/14 19:53:02 INFO mapreduce.Job: Running job: job_1473444507507_0125\n",
      "16/09/14 19:53:18 INFO mapreduce.Job: Job job_1473444507507_0125 running in uber mode : false\n",
      "16/09/14 19:53:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/14 19:53:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/14 19:53:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/14 19:53:43 INFO mapreduce.Job: Job job_1473444507507_0125 completed successfully\n",
      "16/09/14 19:53:43 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=54807\n",
      "\t\tFILE: Number of bytes written=467528\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=56511\n",
      "\t\tHDFS: Number of bytes written=1898\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24389\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7963\n",
      "\t\tTotal time spent by all map tasks (ms)=24389\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7963\n",
      "\t\tTotal vcore-seconds taken by all map tasks=24389\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7963\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=24974336\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=8154112\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1311\n",
      "\t\tMap output records=1311\n",
      "\t\tMap output bytes=52179\n",
      "\t\tMap output materialized bytes=54813\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1311\n",
      "\t\tReduce shuffle bytes=54813\n",
      "\t\tReduce input records=1311\n",
      "\t\tReduce output records=50\n",
      "\t\tSpilled Records=2622\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=383\n",
      "\t\tCPU time spent (ms)=4700\n",
      "\t\tPhysical memory (bytes) snapshot=672772096\n",
      "\t\tVirtual memory (bytes) snapshot=4677021696\n",
      "\t\tTotal committed heap usage (bytes)=706215936\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56275\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1898\n",
      "16/09/14 19:53:43 INFO streaming.StreamJob: Output directory: result3s4_sorted\n"
     ]
    }
   ],
   "source": [
    "### Begin sorting\n",
    "!chmod a+x mappe3r4_s.py\n",
    "!chmod a+x reduce3r4_s.py\n",
    "!hdfs dfs -rm -r result3s4_sorted\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=',' \\\n",
    "-D map.output.key.value.fields.spec=0-1:2- \\\n",
    "-D mapred.text.key.comparator.options='-k2,2nr -k1,1' \\\n",
    "-mapper /home/cloudera/mappe3r4_s.py \\\n",
    "-reducer /home/cloudera/reduce3r4_s.py \\\n",
    "-input result3s4 \\\n",
    "-output result3s4_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Most Common Pairs of Products:\n",
      "Product Pairs| Frequency | Relative Frequency\n",
      "DAI62779\tELE17451\t1592\t0.0511880646925\n",
      "FRO40251\tSNA80324\t1412\t0.0454004694383\n",
      "DAI75645\tFRO40251\t1254\t0.0403202469374\n",
      "FRO40251\tGRO85051\t1213\t0.0390019613517\n",
      "DAI62779\tGRO73461\t1139\t0.0366226166361\n",
      "DAI75645\tSNA80324\t1130\t0.0363332368734\n",
      "DAI62779\tFRO40251\t1070\t0.0344040384554\n",
      "DAI62779\tSNA80324\t923\t0.0296775023311\n",
      "DAI62779\tDAI85309\t918\t0.0295167357963\n",
      "ELE32164\tGRO59710\t911\t0.0292916626475\n",
      "DAI62779\tDAI75645\t882\t0.0283592167454\n",
      "FRO40251\tGRO73461\t882\t0.0283592167454\n",
      "DAI62779\tELE92920\t877\t0.0281984502106\n",
      "FRO40251\tFRO92469\t835\t0.026848011318\n",
      "DAI62779\tELE32164\t832\t0.0267515513971\n",
      "DAI75645\tGRO73461\t712\t0.0228931545609\n",
      "DAI43223\tELE32164\t711\t0.022861001254\n",
      "DAI62779\tGRO30386\t709\t0.02279669464\n",
      "ELE17451\tFRO40251\t697\t0.0224108549564\n",
      "DAI85309\tELE99737\t659\t0.0211890292917\n",
      "DAI62779\tELE26917\t650\t0.020899649529\n",
      "GRO21487\tGRO73461\t631\t0.0202887366966\n",
      "DAI62779\tSNA45677\t604\t0.0194205974084\n",
      "ELE17451\tSNA80324\t597\t0.0191955242597\n",
      "DAI62779\tGRO71621\t595\t0.0191312176457\n",
      "DAI62779\tSNA55762\t593\t0.0190669110318\n",
      "DAI62779\tDAI83733\t586\t0.018841837883\n",
      "ELE17451\tGRO73461\t580\t0.0186489180412\n",
      "GRO73461\tSNA80324\t562\t0.0180701585158\n",
      "DAI62779\tGRO59710\t561\t0.0180380052088\n",
      "DAI62779\tFRO80039\t550\t0.0176843188322\n",
      "DAI75645\tELE17451\t547\t0.0175878589113\n",
      "DAI62779\tSNA93860\t537\t0.0172663258416\n",
      "DAI55148\tDAI62779\t526\t0.016912639465\n",
      "DAI43223\tGRO59710\t512\t0.0164624931674\n",
      "ELE17451\tELE32164\t511\t0.0164303398605\n",
      "DAI62779\tSNA18336\t506\t0.0162695733256\n",
      "ELE32164\tGRO73461\t486\t0.0156265071863\n",
      "DAI62779\tFRO78087\t482\t0.0154978939584\n",
      "DAI85309\tELE17451\t482\t0.0154978939584\n",
      "DAI62779\tGRO94758\t479\t0.0154014340375\n",
      "DAI62779\tGRO21487\t471\t0.0151442075817\n",
      "GRO85051\tSNA80324\t471\t0.0151442075817\n",
      "ELE17451\tGRO30386\t468\t0.0150477476608\n",
      "FRO85978\tSNA95666\t463\t0.014886981126\n",
      "DAI62779\tFRO19221\t462\t0.014854827819\n",
      "DAI62779\tGRO46854\t461\t0.0148226745121\n",
      "DAI43223\tDAI62779\t459\t0.0147583678981\n",
      "ELE92920\tSNA18336\t455\t0.0146297546703\n",
      "DAI88079\tFRO40251\t446\t0.0143403749076\n"
     ]
    }
   ],
   "source": [
    "! echo \"50 Most Common Pairs of Products:\" \n",
    "! echo \"Product Pairs| Frequency | Relative Frequency\"\n",
    "!hdfs dfs -cat result3s4_sorted/* | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.5: Stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r5.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# mapper counter\n",
    "sys.stderr.write(\"reporter:counter:HW3_5, Mapper_counter,1\\n\")\n",
    "\n",
    "# Associative array\n",
    "A = {}\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get all products from the session\n",
    "    products = line.strip().split(' ')\n",
    "    product_size = len(products)\n",
    "    if product_size==0:\n",
    "        continue\n",
    "    \n",
    "    #  lexicographically sort \n",
    "    products.sort()\n",
    "    \n",
    "    # get pairs of products\n",
    "    pairs = [[products[i], products[j]] for i in range(product_size) for j in range(i+1, product_size)]\n",
    "    \n",
    "    # emit dummy record for total count\n",
    "    print '%s\\t%s' %('*', 1)\n",
    "    \n",
    "    # prepare associative arrays\n",
    "    for w1, w2 in pairs:\n",
    "               \n",
    "        if w1 not in A:\n",
    "            # if w1 is new, add to associative array \n",
    "            A[w1] = {}\n",
    "            A[w1][w2] = 1            \n",
    "        elif w2 not in A[w1]:\n",
    "            # w1 is not new, but it doesn't have key for w2\n",
    "            A[w1][w2] = 1\n",
    "        else:\n",
    "            # both are there, increase it\n",
    "            A[w1][w2] += 1\n",
    "        \n",
    "# emit associative arrays\n",
    "for a in A:\n",
    "    print '%s\\t%s' %(a, str(A[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce3r5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r5.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "# function to combine associative array\n",
    "def elementSum(A1, A2):    \n",
    "    # make sure A1 is the long one\n",
    "    if len(A1)<len(A2):\n",
    "        A0 = A2\n",
    "        A2 = A1\n",
    "        A1 = A0\n",
    "    # merge shorter one into longer one \n",
    "    for a in A2:\n",
    "        if a not in A1:\n",
    "            A1[a] = A2[a]\n",
    "        else:\n",
    "            A1[a] += A2[a]        \n",
    "    # return\n",
    "    return A1\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# increase counter for reducer called\n",
    "sys.stderr.write(\"reporter:counter:HW3_5,Reducer_counter,1\\n\")\n",
    "\n",
    "min_support = 100\n",
    "tmp_word = None\n",
    "tmp_Array = None\n",
    "total_products = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # parse keyword and the associative array\n",
    "    word, Array = line.strip().split('\\t', 1)\n",
    "    \n",
    "    # get total basket\n",
    "    if word == '*':\n",
    "        total_products += int(Array)\n",
    "        continue\n",
    "    \n",
    "    # get array into variable\n",
    "    cmdStr = 'Array = ' + Array\n",
    "    exec cmdStr\n",
    "        \n",
    "    # merge the associative array\n",
    "    if tmp_word == word:\n",
    "        tmp_aArray = elementSum(tmp_Array, Array)           \n",
    "    else:\n",
    "        # finish one word merge\n",
    "        if tmp_word:\n",
    "            # get the top pairs with heap\n",
    "            for p in tmp_Array:\n",
    "                if tmp_Array[p] > min_support:                    \n",
    "             \n",
    "                    print '%s,%s,%s,%s' %(tmp_word, p, tmp_Array[p], str(float(tmp_Array[p])/float(total_products)))\n",
    "        # reset for a new word\n",
    "        tmp_word = word\n",
    "        tmp_Array = Array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r5_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r5_s.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_5,Mapper_s_counter,1\\n\")\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # just emit\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce3r5_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r5_s.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_5,Reducer_s_counter,1\\n\")\n",
    "\n",
    "n = 0\n",
    "top = 50\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # parse mapper output  \n",
    "    n += 1\n",
    "    if n <= top:        \n",
    "        print line.strip().replace(',', '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s5\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob1235062254260049591.jar tmpDir=null\n",
      "16/09/14 22:44:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/14 22:44:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/14 22:44:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/14 22:44:53 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/14 22:44:53 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/14 22:44:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/14 22:44:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0133\n",
      "16/09/14 22:44:54 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0133\n",
      "16/09/14 22:44:54 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0133/\n",
      "16/09/14 22:44:54 INFO mapreduce.Job: Running job: job_1473444507507_0133\n",
      "16/09/14 22:45:07 INFO mapreduce.Job: Job job_1473444507507_0133 running in uber mode : false\n",
      "16/09/14 22:45:07 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/14 22:45:25 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "16/09/14 22:45:27 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "16/09/14 22:45:28 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/09/14 22:45:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/14 22:45:43 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "16/09/14 22:45:46 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/14 22:45:46 INFO mapreduce.Job: Job job_1473444507507_0133 completed successfully\n",
      "16/09/14 22:45:46 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15864355\n",
      "\t\tFILE: Number of bytes written=32084353\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462851\n",
      "\t\tHDFS: Number of bytes written=29770\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39154\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14291\n",
      "\t\tTotal time spent by all map tasks (ms)=39154\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14291\n",
      "\t\tTotal vcore-seconds taken by all map tasks=39154\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=14291\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=40093696\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=14633984\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=48041\n",
      "\t\tMap output bytes=15749537\n",
      "\t\tMap output materialized bytes=15864361\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12012\n",
      "\t\tReduce shuffle bytes=15864361\n",
      "\t\tReduce input records=48041\n",
      "\t\tReduce output records=748\n",
      "\t\tSpilled Records=96082\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1508\n",
      "\t\tCPU time spent (ms)=19230\n",
      "\t\tPhysical memory (bytes) snapshot=934572032\n",
      "\t\tVirtual memory (bytes) snapshot=4699709440\n",
      "\t\tTotal committed heap usage (bytes)=924844032\n",
      "\tHW3_5\n",
      "\t\t Mapper_counter=2\n",
      "\t\tReducer_counter=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=29770\n",
      "16/09/14 22:45:46 INFO streaming.StreamJob: Output directory: result3s5\n"
     ]
    }
   ],
   "source": [
    "# Generate Hadoop results without sorting\n",
    "!chmod a+x mappe3r5.py\n",
    "!chmod a+x reduce3r5.py\n",
    "!hdfs dfs -rm -r result3s5\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/mappe3r5.py \\\n",
    "-reducer /home/cloudera/reduce3r5.py \\\n",
    "-input /user/shihyu/ProductPurchaseData.txt \\\n",
    "-output result3s5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI16732,FRO78087,106,0.00340825053857\t\r\n",
      "DAI22177,DAI62779,129,0.00414777659882\t\r\n",
      "DAI22534,DAI62779,123,0.00395485675702\t\r\n",
      "DAI22896,GRO21487,114,0.00366547699431\t\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat result3s5/* | head -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s5_sorted\n",
      "16/09/15 07:04:49 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/09/15 07:04:49 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/09/15 07:04:49 INFO Configuration.deprecation: map.output.key.value.fields.spec is deprecated. Instead, use mapreduce.fieldsel.map.output.key.value.fields.spec\n",
      "16/09/15 07:04:49 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/09/15 07:04:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/15 07:04:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob5655302219594596207.jar tmpDir=null\n",
      "16/09/15 07:04:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/15 07:04:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/15 07:04:52 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/15 07:04:52 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/15 07:04:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0143\n",
      "16/09/15 07:04:53 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0143\n",
      "16/09/15 07:04:53 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0143/\n",
      "16/09/15 07:04:53 INFO mapreduce.Job: Running job: job_1473444507507_0143\n",
      "16/09/15 07:05:04 INFO mapreduce.Job: Job job_1473444507507_0143 running in uber mode : false\n",
      "16/09/15 07:05:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/15 07:05:15 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/15 07:05:27 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/15 07:05:27 INFO mapreduce.Job: Job job_1473444507507_0143 completed successfully\n",
      "16/09/15 07:05:27 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=31272\n",
      "\t\tFILE: Number of bytes written=424364\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=34102\n",
      "\t\tHDFS: Number of bytes written=1897\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19212\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8826\n",
      "\t\tTotal time spent by all map tasks (ms)=19212\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8826\n",
      "\t\tTotal vcore-seconds taken by all map tasks=19212\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8826\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19673088\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=9037824\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=748\n",
      "\t\tMap output records=748\n",
      "\t\tMap output bytes=29770\n",
      "\t\tMap output materialized bytes=31278\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=748\n",
      "\t\tReduce shuffle bytes=31278\n",
      "\t\tReduce input records=748\n",
      "\t\tReduce output records=50\n",
      "\t\tSpilled Records=1496\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=447\n",
      "\t\tCPU time spent (ms)=4920\n",
      "\t\tPhysical memory (bytes) snapshot=698474496\n",
      "\t\tVirtual memory (bytes) snapshot=4687409152\n",
      "\t\tTotal committed heap usage (bytes)=639107072\n",
      "\tHW3_5\n",
      "\t\tMapper_s_counter=2\n",
      "\t\tReducer_s_counter=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=33866\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1897\n",
      "16/09/15 07:05:27 INFO streaming.StreamJob: Output directory: result3s5_sorted\n"
     ]
    }
   ],
   "source": [
    "### Begin sorting\n",
    "!chmod a+x mappe3r5_s.py\n",
    "!chmod a+x reduce3r5_s.py\n",
    "!hdfs dfs -rm -r result3s5_sorted\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=',' \\\n",
    "-D map.output.key.value.fields.spec=0-2:3- \\\n",
    "-D mapred.text.key.comparator.options='-k3,3nr -k1,1 -k2,2' \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mappe3r5_s.py,reduce3r5_s.py \\\n",
    "-mapper mappe3r5_s.py \\\n",
    "-reducer reduce3r5_s.py \\\n",
    "-input result3s5 \\\n",
    "-output result3s5_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Most Common Pairs of Products by Strip:\n",
      "Product Pairs| Frequency | Relative Frequency\n",
      "FRO40251\tSNA80324\t1412\t0.0454004694383\n",
      "DAI75645\tFRO40251\t1254\t0.0403202469374\n",
      "FRO40251\tGRO85051\t1213\t0.0390019613517\n",
      "DAI75645\tSNA80324\t1130\t0.0363332368734\n",
      "ELE32164\tGRO59710\t911\t0.0292916626475\n",
      "DAI62779\tELE17451\t902\t0.0290022828848\n",
      "FRO40251\tGRO73461\t882\t0.0283592167454\n",
      "DAI62779\tGRO73461\t844\t0.0271373910807\n",
      "FRO40251\tFRO92469\t835\t0.026848011318\n",
      "DAI75645\tGRO73461\t712\t0.0228931545609\n",
      "DAI43223\tELE32164\t711\t0.022861001254\n",
      "DAI62779\tFRO40251\t658\t0.0211568759847\n",
      "DAI62779\tDAI85309\t614\t0.0197421304781\n",
      "DAI62779\tSNA80324\t598\t0.0192276775666\n",
      "DAI62779\tDAI75645\t560\t0.0180058519019\n",
      "DAI75645\tELE17451\t547\t0.0175878589113\n",
      "DAI55148\tDAI62779\t526\t0.016912639465\n",
      "DAI43223\tGRO59710\t512\t0.0164624931674\n",
      "ELE32164\tGRO73461\t486\t0.0156265071863\n",
      "DAI62779\tSNA55762\t463\t0.014886981126\n",
      "FRO85978\tSNA95666\t463\t0.014886981126\n",
      "DAI43223\tDAI62779\t459\t0.0147583678981\n",
      "ELE92920\tSNA18336\t455\t0.0146297546703\n",
      "ELE17451\tFRO40251\t453\t0.0145654480563\n",
      "DAI62779\tFRO19221\t449\t0.0144368348285\n",
      "DAI88079\tFRO40251\t446\t0.0143403749076\n",
      "FRO73056\tGRO44993\t438\t0.0140831484518\n",
      "ELE17451\tSNA80324\t428\t0.0137616153821\n",
      "GRO38814\tGRO73461\t427\t0.0137294620752\n",
      "ELE17451\tGRO73461\t409\t0.0131507025498\n",
      "DAI62779\tELE32164\t406\t0.0130542426289\n",
      "DAI75645\tGRO85051\t395\t0.0127005562522\n",
      "FRO31317\tGRO73461\t395\t0.0127005562522\n",
      "DAI62779\tSNA45677\t392\t0.0126040963313\n",
      "DAI62779\tGRO30386\t389\t0.0125076364104\n",
      "GRO46854\tGRO73461\t389\t0.0125076364104\n",
      "GRO30386\tGRO73461\t380\t0.0122182566477\n",
      "FRO40251\tGRO21487\t375\t0.0120574901129\n",
      "DAI62779\tELE26917\t371\t0.011928876885\n",
      "DAI62779\tDAI83733\t363\t0.0116716504292\n",
      "DAI62779\tGRO71621\t357\t0.0114787305874\n",
      "ELE74482\tSNA99873\t357\t0.0114787305874\n",
      "FRO92469\tSNA80324\t352\t0.0113179640526\n",
      "FRO85978\tGRO73461\t344\t0.0110607375969\n",
      "DAI55148\tFRO40251\t343\t0.0110285842899\n",
      "DAI55148\tSNA80324\t339\t0.010899971062\n",
      "DAI62779\tFRO80039\t327\t0.0105141313784\n",
      "DAI43223\tELE17451\t326\t0.0104819780714\n",
      "ELE74482\tFRO31317\t317\t0.0101925983087\n",
      "DAI62779\tELE99737\t315\t0.0101282916948\n"
     ]
    }
   ],
   "source": [
    "! echo \"50 Most Common Pairs of Products by Strip:\" \n",
    "! echo \"Product Pairs| Frequency | Relative Frequency\"\n",
    "!hdfs dfs -cat result3s5_sorted/part-0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW3.5 Results\n",
    "- 2 mappers, 1 reducer\n",
    "- with the same configure, the execution time is reduced to 15 sec.  from 25 sec. of pair approach, about 32% improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.6 Computing Relative Frequencies on 100K WikiPedia pages (93Meg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/shihyu/wikitext_100k.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/shihyu\n",
    "!hdfs dfs -put wikitext_100k.txt /user/shihyu\n",
    "# hdfs -cat \\usr\\cloudera\\output\\part-r-0000 >\\somewhere\\results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairs Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r6_pair.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r6_pair.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# mapper counter\n",
    "sys.stderr.write(\"reporter:counter:HW3_6_Pair, Mapper_counter,1\\n\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "#cleanedHost = re.sub(r\"[^a-zA-Z0-9]+\", \"\", host)\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Get format\n",
    "        author, bbb, ccc, ddd, eee, body = line.split('\\t', -1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # get all words from the session (subject + ' ' + body)\n",
    "    # for word in WORD_RE.findall(subject + ' ' + body):\n",
    "    #clean_body = re.sub(r\"[^a-zA-Z0-9]+\", \"\", body)\n",
    "    #  re.sub(r\"[^A-Za-z]\", \" \", body.strip())\n",
    "    #clean_eee = re.sub(r\"[^a-zA-Z0-9]+\", \"\", eee)\n",
    "    # words = re.split(r\"[^A-Za-z]\", line.strip())\n",
    "    \n",
    "    ### workable\n",
    "    #line = re.sub(r\"[^A-Za-z]\", \" \", body.strip())\n",
    "    #words = line.split()\n",
    "    \n",
    "    #clean_author = re.sub(r\"[^A-Za-z]\", \" \", author.strip())\n",
    "    #clean_bbb = re.sub(r\"[^A-Za-z]\", \" \", bbb.strip())\n",
    "    #clean_ccc = re.sub(r\"[^A-Za-z]\", \" \", ccc.strip())\n",
    "    #clean_ddd = re.sub(r\"[^A-Za-z]\", \" \", ddd.strip())\n",
    "    clean_eee = re.sub(r\"[^A-Za-z]\", \" \", eee.strip())\n",
    "    clean_body = re.sub(r\"[^A-Za-z]\", \" \", body.strip())\n",
    "       \n",
    "    words = clean_body + ' ' + clean_eee# + ' ' + clean_ddd + ' ' + clean_ccc + ' ' + clean_bbb\n",
    "    words = words.split()\n",
    "    \n",
    "    #line = re.sub(r\"[^A-Za-z]\", \" \", body.strip()) + ' ' + re.sub(r\"[^A-Za-z]\", \" \", eee.strip())\n",
    "    #words = line.split()\n",
    "    \n",
    "    \n",
    "    cart_size = len(words)\n",
    "    if cart_size==0:\n",
    "        continue\n",
    "    \n",
    "    # sort words the pair is lexicographically sound\n",
    "    words.sort()\n",
    "    \n",
    "    # set pairs of words\n",
    "    pairs = [[words[i], words[j]] for i in range(cart_size) for j in range(i+1, cart_size)]\n",
    "    \n",
    "    # dummy record for total products count\n",
    "    print '%s,%s' %('*', 1)\n",
    "    \n",
    "    # emit product pairs\n",
    "    for pair in pairs:\n",
    "        print '%s_%s,%s' %(pair[0], pair[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combine3r6_pair.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combine3r6_pair.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# combiner counter\n",
    "sys.stderr.write(\"reporter:counter:HW3_6_Pair, Combiner_ounter,1\\n\")\n",
    "\n",
    "tmp_pair = None\n",
    "tmp_count = 0\n",
    "\n",
    "for line in sys.stdin:       \n",
    "    # get all products from each line \n",
    "    pair, count = line.strip().split(',', 1)\n",
    "    \n",
    "    # skip bad value \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # accumulate counts for whatever keys it receives\n",
    "    if tmp_pair == pair:\n",
    "        tmp_count += count\n",
    "    else:\n",
    "        # previous pair finishes streaming, emit results\n",
    "        if tmp_pair:            \n",
    "            print '%s,%s' %(tmp_pair, tmp_count)\n",
    "        # set new pair\n",
    "        tmp_pair = pair\n",
    "        tmp_count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce3r6_pair.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r6_pair.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# reducer counter\n",
    "sys.stderr.write(\"reporter:counter:HW3_6_Pair, Reducer_counter,1\\n\")\n",
    "\n",
    "\n",
    "total_words = 0\n",
    "min_support = 100\n",
    "tmp_pair = None\n",
    "tmp_count = 0\n",
    "\n",
    "for line in sys.stdin:       \n",
    "    # get all products from the session\n",
    "    pair, count = line.strip().split(',', 1)\n",
    "    \n",
    "    # skip bad count\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # get total sessions/baskets\n",
    "    if pair == '*':\n",
    "        total_words += count\n",
    "        continue\n",
    "        \n",
    "    # get pair count\n",
    "    if tmp_pair == pair:\n",
    "        tmp_count += count\n",
    "    else:\n",
    "        # previous pair finishes \n",
    "        if tmp_pair and tmp_count > min_support:\n",
    "            # emit\n",
    "            print '%s,%s,%s' %(tmp_pair, tmp_count, str(float(tmp_count)/float(total_words)))\n",
    "        # reset new pair\n",
    "        tmp_pair = pair\n",
    "        tmp_count = count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s6_pair\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob690816109107824484.jar tmpDir=null\n",
      "16/09/17 14:25:09 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/17 14:25:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/17 14:25:15 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/17 14:25:16 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/17 14:25:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/17 14:25:16 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/17 14:25:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0211\n",
      "16/09/17 14:25:18 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0211\n",
      "16/09/17 14:25:19 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0211/\n",
      "16/09/17 14:25:19 INFO mapreduce.Job: Running job: job_1473444507507_0211\n",
      "16/09/17 14:27:42 INFO mapreduce.Job: Job job_1473444507507_0211 running in uber mode : false\n",
      "16/09/17 14:27:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/17 14:28:09 INFO mapreduce.Job:  map 1% reduce 0%\n",
      "16/09/17 14:29:18 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "16/09/17 14:29:24 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "16/09/17 14:29:46 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "16/09/17 14:29:59 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "16/09/17 14:30:02 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/09/17 14:30:23 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/09/17 14:31:05 INFO mapreduce.Job:  map 87% reduce 0%\n",
      "16/09/17 14:31:09 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "16/09/17 14:31:12 INFO mapreduce.Job:  map 96% reduce 0%\n",
      "16/09/17 14:31:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/17 14:31:40 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "16/09/17 14:31:44 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "16/09/17 14:31:47 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "16/09/17 14:31:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/17 14:31:52 INFO mapreduce.Job: Job job_1473444507507_0211 completed successfully\n",
      "16/09/17 14:31:52 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=23902990\n",
      "\t\tFILE: Number of bytes written=37599996\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=92446758\n",
      "\t\tHDFS: Number of bytes written=237386\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=295395\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=35601\n",
      "\t\tTotal time spent by all map tasks (ms)=295395\n",
      "\t\tTotal time spent by all reduce tasks (ms)=35601\n",
      "\t\tTotal vcore-seconds taken by all map tasks=295395\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=35601\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=302484480\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=36455424\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=7530422\n",
      "\t\tMap output bytes=116059510\n",
      "\t\tMap output materialized bytes=13340307\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=8064826\n",
      "\t\tCombine output records=1216161\n",
      "\t\tReduce input groups=680716\n",
      "\t\tReduce shuffle bytes=13340307\n",
      "\t\tReduce input records=681757\n",
      "\t\tReduce output records=8324\n",
      "\t\tSpilled Records=1897918\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1206\n",
      "\t\tCPU time spent (ms)=161780\n",
      "\t\tPhysical memory (bytes) snapshot=961126400\n",
      "\t\tVirtual memory (bytes) snapshot=4708667392\n",
      "\t\tTotal committed heap usage (bytes)=969932800\n",
      "\tHW3_6_Pair\n",
      "\t\t Combiner_ounter=5\n",
      "\t\t Mapper_counter=2\n",
      "\t\t Reducer_counter=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92446532\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=237386\n",
      "16/09/17 14:31:52 INFO streaming.StreamJob: Output directory: result3s6_pair\n"
     ]
    }
   ],
   "source": [
    "# Generate Hadoop results without sorting\n",
    "!chmod a+x mappe3r6_pair.py\n",
    "!chmod a+x combine3r6_pair.py\n",
    "!chmod a+x reduce3r6_pair.py\n",
    "!hdfs dfs -rm -r result3s6_pair\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/mappe3r6_pair.py \\\n",
    "-combiner /home/cloudera/combine3r6_pair.py \\\n",
    "-reducer /home/cloudera/reduce3r6_pair.py \\\n",
    "-input /user/shihyu/wikitext_100k.txt \\\n",
    "-output result3s6_pair\n",
    "\n",
    "#!hdfs -cat /user/cloudera/result3s6_pair/part-00000 > /user/cloudera/rfpairs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC_the,103,5.72222222222\t\r\n",
      "ALTERNATIVE_the,103,5.72222222222\t\r\n",
      "AL_the,103,5.72222222222\t\r\n",
      "AZ_Gallery,108,6.0\t\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat result3s6_pair/* | head -4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s6_pair_sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob6382017384913746716.jar tmpDir=null\n",
      "16/09/17 14:37:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/17 14:37:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/17 14:37:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/17 14:37:31 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/09/17 14:37:31 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/17 14:37:31 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/17 14:37:31 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/09/17 14:37:31 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/09/17 14:37:31 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/09/17 14:37:31 INFO Configuration.deprecation: map.output.key.value.fields.spec is deprecated. Instead, use mapreduce.fieldsel.map.output.key.value.fields.spec\n",
      "16/09/17 14:37:31 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/17 14:37:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0212\n",
      "16/09/17 14:37:32 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0212\n",
      "16/09/17 14:37:32 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0212/\n",
      "16/09/17 14:37:32 INFO mapreduce.Job: Running job: job_1473444507507_0212\n",
      "16/09/17 14:37:46 INFO mapreduce.Job: Job job_1473444507507_0212 running in uber mode : false\n",
      "16/09/17 14:37:46 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/17 14:38:03 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/17 14:38:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/17 14:38:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/17 14:38:15 INFO mapreduce.Job: Job job_1473444507507_0212 completed successfully\n",
      "16/09/17 14:38:15 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=254040\n",
      "\t\tFILE: Number of bytes written=866024\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=241728\n",
      "\t\tHDFS: Number of bytes written=1588\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=30495\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8398\n",
      "\t\tTotal time spent by all map tasks (ms)=30495\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8398\n",
      "\t\tTotal vcore-seconds taken by all map tasks=30495\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8398\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=31226880\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=8599552\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8324\n",
      "\t\tMap output records=8324\n",
      "\t\tMap output bytes=237386\n",
      "\t\tMap output materialized bytes=254046\n",
      "\t\tInput split bytes=246\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8324\n",
      "\t\tReduce shuffle bytes=254046\n",
      "\t\tReduce input records=8324\n",
      "\t\tReduce output records=50\n",
      "\t\tSpilled Records=16648\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=358\n",
      "\t\tCPU time spent (ms)=7260\n",
      "\t\tPhysical memory (bytes) snapshot=751456256\n",
      "\t\tVirtual memory (bytes) snapshot=4700057600\n",
      "\t\tTotal committed heap usage (bytes)=679477248\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=241482\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1588\n",
      "16/09/17 14:38:15 INFO streaming.StreamJob: Output directory: result3s6_pair_sorted\n",
      "align\tcenter\t249001\t13833.3888889\n",
      "align\talign\t124251\t6902.83333333\n",
      "center\tcenter\t124251\t6902.83333333\n",
      "align\tparty\t33932\t1885.11111111\n"
     ]
    }
   ],
   "source": [
    "### Begin sorting\n",
    "!chmod a+x mappe3r4_s.py\n",
    "!chmod a+x reduce3r4_s.py\n",
    "!hdfs dfs -rm -r result3s6_pair_sorted\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=',' \\\n",
    "-D map.output.key.value.fields.spec=0-1:2- \\\n",
    "-D mapred.text.key.comparator.options='-k2,2nr -k1,1' \\\n",
    "-mapper /home/cloudera/mappe3r4_s.py \\\n",
    "-reducer /home/cloudera/reduce3r4_s.py \\\n",
    "-input result3s6_pair \\\n",
    "-output result3s6_pair_sorted\n",
    "\n",
    "!hdfs dfs -cat result3s6_pair_sorted/* | head -4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: /user/cloudera/rfpairs.txt: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs -cat /user/cloudera/result3s6_pair_sorted/* > /user/cloudera/rfpairs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mappe3r6_Strip.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# mapper counter\n",
    "sys.stderr.write(\"reporter:counter:HW3_6_Strip, Mapper_counter,1\\n\")\n",
    "\n",
    "# Associative array\n",
    "A = {}\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get all products from the session\n",
    "    products = line.strip().split(' ')\n",
    "    product_size = len(products)\n",
    "    if product_size==0:\n",
    "        continue\n",
    "    \n",
    "    #  lexicographically sort \n",
    "    products.sort()\n",
    "    \n",
    "    # get pairs of products\n",
    "    pairs = [[products[i], products[j]] for i in range(product_size) for j in range(i+1, product_size)]\n",
    "    \n",
    "    # emit dummy record for total count\n",
    "    print '%s\\t%s' %('*', 1)\n",
    "    \n",
    "    # prepare associative arrays\n",
    "    for w1, w2 in pairs:\n",
    "               \n",
    "        if w1 not in A:\n",
    "            # if w1 is new, add to associative array \n",
    "            A[w1] = {}\n",
    "            A[w1][w2] = 1            \n",
    "        elif w2 not in A[w1]:\n",
    "            # w1 is not new, but it doesn't have key for w2\n",
    "            A[w1][w2] = 1\n",
    "        else:\n",
    "            # both are there, increase it\n",
    "            A[w1][w2] += 1\n",
    "        \n",
    "# emit associative arrays\n",
    "for a in A:\n",
    "    print '%s\\t%s' %(a, str(A[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison for Pairs and Strip Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.7 Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Answer:\n",
    "- Aprior algorithm is used to find itemsets that appear frequently, each iteration has two scans of data and a filtering in between. They have following steps:\n",
    " 1. generate a set $S_k$ for itemsets with size $k$ from the output of previous iteration $M_{k-1}$.\n",
    " 2. remove all members from the set whose support is less than the user specified threshold $t_i$\n",
    " 3. generate the final set $M_k$ for itemset of size $k$, based on output after filtering.\n",
    " \n",
    " \n",
    "- For example, to find itemsets of size $k$ from a shopping basket set, the procedure is described as :\n",
    " 1. count all single product from all baskets, output $S_1$\n",
    " 2. remove all words with support below threshold, output $M_1$\n",
    " 3. use $M_1$ to generate set for frequent pair set $S_2$\n",
    " 5. remove all pairs with support below threshold to obtain $M_2$\n",
    " 6. use $M_2$ to generate set for frequent triple set $S_3$\n",
    " 7. remove all triples with support below threshold, get $M_3$\n",
    " 8. Cotinue above to $M_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.8. Shopping Cart Analysis, Benchmark your results using the pyFIM implementation of the Apriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mappe3r8_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r8_1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get products and emit\n",
    "    for product in line.strip().split(' '):\n",
    "        print '%s\\t%d' %(product, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reduce3r8_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r8_1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "\n",
    "tmp_prod = None\n",
    "tmp_count = 0\n",
    "min_support = 100\n",
    "\n",
    "for line in sys.stdin:   \n",
    "    # get key value pair\n",
    "    product, count = line.strip().split('\\t', 1)\n",
    "    \n",
    "    # skip bad count\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # get count\n",
    "    if tmp_prod == product:\n",
    "        tmp_count += count\n",
    "    else:\n",
    "        if tmp_prod and tmp_count > min_support:\n",
    "            # emit product above min support\n",
    "            print '%s\\t%d' %(tmp_prod, tmp_count)\n",
    "        # reset product and count\n",
    "        tmp_prod = product\n",
    "        tmp_count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r8_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r8_2.py\n",
    "#!/usr/bin/python\n",
    "import sys, subprocess \n",
    "\n",
    "\n",
    "single = []\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"/user/cloudera/result3s8_1/part-00000\"], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:\n",
    "    single.append(line.strip().split('\\t')[0])\n",
    "\n",
    "# read the input data\n",
    "for line in sys.stdin:   \n",
    "    \n",
    "    line = line.strip()\n",
    "        \n",
    "    # get products for each cart\n",
    "    product = line.strip().split(' ')\n",
    "        \n",
    "    # keep product from set with single element only\n",
    "    products = [val for val in product if val in single]\n",
    "    products.sort()\n",
    "    \n",
    "    # get pairs to emit\n",
    "    size = len(products)\n",
    "    pairs = [products[i] + '_' + products[j] for i in range(size) for j in range(i+1, size)]\n",
    "    for p in pairs:\n",
    "        print '%s\\t%d' %(p, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second stage reducer is reduce3r8_1.py also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mappe3r8_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mappe3r8_3.py\n",
    "#!/usr/bin/python\n",
    "import sys, subprocess \n",
    "\n",
    "# load the frequent frequent Pairs given by Job 2\n",
    "Freq_Pair = []\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"/user/cloudera/result3s8_2/part-00000\"], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:\n",
    "    Freq_Pair.append(line.strip().split('\\t')[0])\n",
    "    \n",
    "# still read frequent freqPairs first, then session data to generate triples\n",
    "for line in sys.stdin:   \n",
    "    \n",
    "    line = line.strip()\n",
    "            \n",
    "    # Get product from each cart \n",
    "    products = line.split(' ')\n",
    "    products.sort()\n",
    "    size = len(products)\n",
    "    \n",
    "    # build Pairs and Triples from the session, in the format of a_b and a_b_c, alphabetically sorted\n",
    "    triples = [[products[i],products[j],products[k]] for i in range(size) for j in range(i+1,size) for k in range(i+2,size)]\n",
    "    pairs = [products[i]+'_'+products[j] for i in range(size) for j in range(i+1,size)]\n",
    "\n",
    "    # processing pairs\n",
    "    for pair in pairs:\n",
    "        # if the pair is in frequent Pair, emit a dummy key a_b_*\n",
    "        if pair in Freq_Pair:\n",
    "            print '%s_*\\t%d' %(pair, 1)\n",
    "\n",
    "    # processing triples\n",
    "    for tri in triples:\n",
    "        # from each triple a_b_c: check if the 3 child-pairs (a_b, b_c, a_c) are in the pair set\n",
    "        # If yes, it is associative rule\n",
    "        if tri[0]+'_'+tri[1] in Freq_Pair and tri[1]+'_'+tri[2] in Freq_Pair and tri[0]+'_'+tri[2] in Freq_Pair:\n",
    "            # if so, emit the triple a_b_c            \n",
    "            print '%s_%s_%s\\t%d' %(tri[0], tri[1], tri[2], 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reduce3r8_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce3r8_3.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "tmp_prod = None\n",
    "tmp_dummy = None\n",
    "tmp_count = 0\n",
    "min_support = 100\n",
    "marginal = 0\n",
    "\n",
    "for line in sys.stdin:   \n",
    "        \n",
    "    # get k-v freqPair\n",
    "    product, count = line.strip().split('\\t', 1)\n",
    "    \n",
    "    # skip bad count\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # handle marginal with dummy key\n",
    "    if '*' == product[-1]:        \n",
    "        if tmp_dummy == product:\n",
    "            # accumulate marginal\n",
    "            marginal += count\n",
    "        else:\n",
    "            # reset marginal for new dummy key\n",
    "            tmp_dummy = product\n",
    "            marginal = count\n",
    "        continue\n",
    "\n",
    "    # processing triple and emit rules\n",
    "    if tmp_prod == product:\n",
    "        tmp_count += count\n",
    "    else:\n",
    "        if tmp_prod and tmp_count > min_support and tmp_count <= marginal: # Removing some mismatch\n",
    "            # emit triples for the rule\n",
    "            w1,w2,w3 = tmp_prod.split('_')\n",
    "            conf = float(tmp_count)/float(marginal)\n",
    "            print '(%s, %s) => %s, %d, %d, %.2f' %(w1, w2, w3, tmp_count, marginal, conf)\n",
    "            \n",
    "        # reset for new triple\n",
    "        tmp_prod = product\n",
    "        tmp_count = count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `result3s8_1': No such file or directory\n",
      "16/09/15 11:37:24 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/15 11:37:24 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob7347751964987250476.jar tmpDir=null\n",
      "16/09/15 11:37:25 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/15 11:37:25 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/15 11:37:26 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/15 11:37:26 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/09/15 11:37:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0144\n",
      "16/09/15 11:37:27 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0144\n",
      "16/09/15 11:37:27 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0144/\n",
      "16/09/15 11:37:27 INFO mapreduce.Job: Running job: job_1473444507507_0144\n",
      "16/09/15 11:37:37 INFO mapreduce.Job: Job job_1473444507507_0144 running in uber mode : false\n",
      "16/09/15 11:37:37 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/15 11:37:57 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/09/15 11:37:58 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/09/15 11:38:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/15 11:38:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/15 11:38:07 INFO mapreduce.Job: Job job_1473444507507_0144 completed successfully\n",
      "16/09/15 11:38:07 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11017\n",
      "\t\tFILE: Number of bytes written=502999\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3467066\n",
      "\t\tHDFS: Number of bytes written=4791\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=56193\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6622\n",
      "\t\tTotal time spent by all map tasks (ms)=56193\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6622\n",
      "\t\tTotal vcore-seconds taken by all map tasks=56193\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6622\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=57541632\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6780928\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380824\n",
      "\t\tMap output bytes=4189064\n",
      "\t\tMap output materialized bytes=11029\n",
      "\t\tInput split bytes=357\n",
      "\t\tCombine input records=380824\n",
      "\t\tCombine output records=733\n",
      "\t\tReduce input groups=365\n",
      "\t\tReduce shuffle bytes=11029\n",
      "\t\tReduce input records=733\n",
      "\t\tReduce output records=364\n",
      "\t\tSpilled Records=1466\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=1385\n",
      "\t\tCPU time spent (ms)=11960\n",
      "\t\tPhysical memory (bytes) snapshot=1202286592\n",
      "\t\tVirtual memory (bytes) snapshot=6255378432\n",
      "\t\tTotal committed heap usage (bytes)=1125646336\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3466709\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4791\n",
      "16/09/15 11:38:07 INFO streaming.StreamJob: Output directory: result3s8_1\n"
     ]
    }
   ],
   "source": [
    "# job 1 - get M_1 for frequent singletons\n",
    "!chmod a+x mappe3r8_1.py\n",
    "!chmod a+x reduce3r8_1.py\n",
    "!hdfs dfs -rm -r result3s8_1\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mappe3r8_1.py,reduce3r8_1.py \\\n",
    "-mapper mappe3r8_1.py \\\n",
    "-reducer reduce3r8_1.py \\\n",
    "-combiner reduce3r8_1.py \\\n",
    "-input /user/shihyu/ProductPurchaseData.txt \\\n",
    "-output result3s8_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s8_2\n",
      "16/09/15 11:57:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/15 11:57:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob5428113389080488872.jar tmpDir=null\n",
      "16/09/15 11:57:07 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/15 11:57:08 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/15 11:57:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/15 11:57:09 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/09/15 11:57:09 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/09/15 11:57:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0146\n",
      "16/09/15 11:57:09 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0146\n",
      "16/09/15 11:57:10 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0146/\n",
      "16/09/15 11:57:10 INFO mapreduce.Job: Running job: job_1473444507507_0146\n",
      "16/09/15 11:57:21 INFO mapreduce.Job: Job job_1473444507507_0146 running in uber mode : false\n",
      "16/09/15 11:57:21 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/15 11:57:43 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "16/09/15 11:57:45 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "16/09/15 11:57:51 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "16/09/15 11:57:52 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "16/09/15 11:57:55 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/09/15 11:57:56 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "16/09/15 11:57:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/15 11:58:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/15 11:58:10 INFO mapreduce.Job: Job job_1473444507507_0146 completed successfully\n",
      "16/09/15 11:58:10 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=21377208\n",
      "\t\tFILE: Number of bytes written=43234113\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3467066\n",
      "\t\tHDFS: Number of bytes written=27705\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=99729\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11827\n",
      "\t\tTotal time spent by all map tasks (ms)=99729\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11827\n",
      "\t\tTotal vcore-seconds taken by all map tasks=99729\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=11827\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=102122496\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=12110848\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=971691\n",
      "\t\tMap output bytes=19433820\n",
      "\t\tMap output materialized bytes=21377220\n",
      "\t\tInput split bytes=357\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=58917\n",
      "\t\tReduce shuffle bytes=21377220\n",
      "\t\tReduce input records=971691\n",
      "\t\tReduce output records=1259\n",
      "\t\tSpilled Records=1943382\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=1899\n",
      "\t\tCPU time spent (ms)=42520\n",
      "\t\tPhysical memory (bytes) snapshot=1249947648\n",
      "\t\tVirtual memory (bytes) snapshot=6242480128\n",
      "\t\tTotal committed heap usage (bytes)=1321730048\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3466709\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=27705\n",
      "16/09/15 11:58:10 INFO streaming.StreamJob: Output directory: result3s8_2\n"
     ]
    }
   ],
   "source": [
    "# job 2 - get M_2 for frequent pairs\n",
    "!chmod a+x mappe3r8_2.py\n",
    "!chmod a+x reduce3r8_1.py\n",
    "!hdfs dfs -rm -r result3s8_2\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mappe3r8_2.py,reduce3r8_1.py \\\n",
    "-mapper mappe3r8_2.py \\\n",
    "-reducer reduce3r8_1.py \\\n",
    "-input /user/shihyu/ProductPurchaseData.txt \\\n",
    "-output result3s8_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s8_3\n",
      "16/09/15 12:45:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/15 12:45:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob1755665626735376634.jar tmpDir=null\n",
      "16/09/15 12:45:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/15 12:45:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/15 12:45:52 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/15 12:45:52 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/09/15 12:45:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0149\n",
      "16/09/15 12:45:53 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0149\n",
      "16/09/15 12:45:53 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0149/\n",
      "16/09/15 12:45:53 INFO mapreduce.Job: Running job: job_1473444507507_0149\n",
      "16/09/15 12:46:04 INFO mapreduce.Job: Job job_1473444507507_0149 running in uber mode : false\n",
      "16/09/15 12:46:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/15 12:46:23 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "16/09/15 12:46:25 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "16/09/15 12:46:26 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "16/09/15 12:47:00 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "16/09/15 12:47:15 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "16/09/15 12:47:22 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "16/09/15 12:48:17 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "16/09/15 12:48:28 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "16/09/15 12:49:24 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "16/09/15 12:49:35 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/09/15 12:49:46 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "16/09/15 12:50:31 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "16/09/15 12:50:55 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/09/15 12:51:13 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "16/09/15 12:51:16 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "16/09/15 12:51:23 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "16/09/15 12:51:53 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "16/09/15 12:52:33 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "16/09/15 12:52:51 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "16/09/15 12:53:13 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/15 12:53:33 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/09/15 12:54:41 INFO mapreduce.Job:  map 55% reduce 0%\n",
      "16/09/15 12:54:47 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "16/09/15 12:54:49 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "16/09/15 12:55:09 INFO mapreduce.Job:  map 69% reduce 11%\n",
      "16/09/15 12:55:46 INFO mapreduce.Job:  map 71% reduce 11%\n",
      "16/09/15 12:55:55 INFO mapreduce.Job:  map 74% reduce 11%\n",
      "16/09/15 12:56:28 INFO mapreduce.Job:  map 76% reduce 11%\n",
      "16/09/15 12:56:58 INFO mapreduce.Job:  map 78% reduce 11%\n",
      "16/09/15 12:57:41 INFO mapreduce.Job:  map 89% reduce 11%\n",
      "16/09/15 12:57:42 INFO mapreduce.Job:  map 89% reduce 22%\n",
      "16/09/15 12:58:19 INFO mapreduce.Job:  map 100% reduce 22%\n",
      "16/09/15 12:58:22 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "16/09/15 12:58:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/15 12:58:23 INFO mapreduce.Job: Job job_1473444507507_0149 completed successfully\n",
      "16/09/15 12:58:23 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11884987\n",
      "\t\tFILE: Number of bytes written=24249679\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3467066\n",
      "\t\tHDFS: Number of bytes written=11101\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1947728\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=211226\n",
      "\t\tTotal time spent by all map tasks (ms)=1947728\n",
      "\t\tTotal time spent by all reduce tasks (ms)=211226\n",
      "\t\tTotal vcore-seconds taken by all map tasks=1947728\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=211226\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1994473472\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=216295424\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=436010\n",
      "\t\tMap output bytes=11012961\n",
      "\t\tMap output materialized bytes=11884999\n",
      "\t\tInput split bytes=357\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7641\n",
      "\t\tReduce shuffle bytes=11884999\n",
      "\t\tReduce input records=436010\n",
      "\t\tReduce output records=221\n",
      "\t\tSpilled Records=872020\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=1850\n",
      "\t\tCPU time spent (ms)=1302590\n",
      "\t\tPhysical memory (bytes) snapshot=1535754240\n",
      "\t\tVirtual memory (bytes) snapshot=6247419904\n",
      "\t\tTotal committed heap usage (bytes)=1355808768\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3466709\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=11101\n",
      "16/09/15 12:58:23 INFO streaming.StreamJob: Output directory: result3s8_3\n"
     ]
    }
   ],
   "source": [
    "# job 3 - get M_3 for frequent triples\n",
    "!chmod a+x mappe3r8_3.py\n",
    "!chmod a+x reduce3r8_3.py\n",
    "!hdfs dfs -rm -r result3s8_3\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files mappe3r8_3.py,reduce3r8_3.py \\\n",
    "-mapper mappe3r8_3.py \\\n",
    "-reducer reduce3r8_3.py \\\n",
    "-input /user/shihyu/ProductPurchaseData.txt \\\n",
    "-output result3s8_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DAI22896, DAI62779) => GRO73461, 101, 297, 0.34\t\r\n",
      "(DAI31081, DAI62779) => ELE17451, 103, 364, 0.28\t\r\n",
      "(DAI31081, DAI75645) => FRO40251, 122, 206, 0.59\t\r\n",
      "(DAI31081, ELE32164) => GRO59710, 112, 312, 0.36\t\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat result3s8_3/* | head -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted result3s8_3_sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob5506711815659657084.jar tmpDir=null\n",
      "16/09/15 13:50:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/15 13:50:13 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/09/15 13:50:15 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/09/15 13:50:15 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/09/15 13:50:15 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/09/15 13:50:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/09/15 13:50:15 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/09/15 13:50:15 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/09/15 13:50:15 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/09/15 13:50:15 INFO Configuration.deprecation: map.output.key.value.fields.spec is deprecated. Instead, use mapreduce.fieldsel.map.output.key.value.fields.spec\n",
      "16/09/15 13:50:15 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/09/15 13:50:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1473444507507_0165\n",
      "16/09/15 13:50:16 INFO impl.YarnClientImpl: Submitted application application_1473444507507_0165\n",
      "16/09/15 13:50:17 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1473444507507_0165/\n",
      "16/09/15 13:50:17 INFO mapreduce.Job: Running job: job_1473444507507_0165\n",
      "16/09/15 13:50:28 INFO mapreduce.Job: Job job_1473444507507_0165 running in uber mode : false\n",
      "16/09/15 13:50:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/09/15 13:50:38 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/09/15 13:50:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/09/15 13:50:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/09/15 13:50:48 INFO mapreduce.Job: Job job_1473444507507_0165 completed successfully\n",
      "16/09/15 13:50:49 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11549\n",
      "\t\tFILE: Number of bytes written=380568\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=15437\n",
      "\t\tHDFS: Number of bytes written=2450\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18168\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6152\n",
      "\t\tTotal time spent by all map tasks (ms)=18168\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6152\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18168\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6152\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18604032\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6299648\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=221\n",
      "\t\tMap output records=221\n",
      "\t\tMap output bytes=11101\n",
      "\t\tMap output materialized bytes=11555\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=221\n",
      "\t\tReduce shuffle bytes=11555\n",
      "\t\tReduce input records=221\n",
      "\t\tReduce output records=50\n",
      "\t\tSpilled Records=442\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=275\n",
      "\t\tCPU time spent (ms)=3550\n",
      "\t\tPhysical memory (bytes) snapshot=690966528\n",
      "\t\tVirtual memory (bytes) snapshot=4682575872\n",
      "\t\tTotal committed heap usage (bytes)=640679936\n",
      "\tHW3_5\n",
      "\t\tMapper_s_counter=2\n",
      "\t\tReducer_s_counter=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=15197\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2450\n",
      "16/09/15 13:50:49 INFO streaming.StreamJob: Output directory: result3s8_3_sorted\n",
      "(DAI62779\t DAI88079) => FRO40251\t 117\t 117\t 1.00\n",
      "(DAI88079\t ELE17451) => FRO40251\t 123\t 124\t 0.99\n",
      "(DAI75645\t DAI88079) => FRO40251\t 148\t 149\t 0.99\n",
      "(DAI62779\t ELE17451) => SNA96271\t 111\t 113\t 0.98\n",
      "(DAI62779\t FRO92469) => SNA80324\t 115\t 118\t 0.97\n",
      "(DAI75645\t GRO94758) => SNA80324\t 120\t 124\t 0.97\n",
      "(DAI88079\t FRO40251) => SNA80324\t 139\t 145\t 0.96\n",
      "(DAI75645\t GRO85051) => SNA80324\t 192\t 203\t 0.95\n",
      "(DAI62779\t FRO19221) => SNA93860\t 125\t 133\t 0.94\n",
      "(FRO73056\t GRO44993) => GRO73461\t 106\t 116\t 0.91\n"
     ]
    }
   ],
   "source": [
    "### Begin sorting\n",
    "!chmod a+x break_tie.py\n",
    "!hdfs dfs -rm -r result3s8_3_sorted\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D map.output.key.field.separator=',' \\\n",
    "-D map.output.key.value.fields.spec=0-2:3- \\\n",
    "-D mapred.text.key.comparator.options='-k5,5r' \\\n",
    "-mapper /home/cloudera/mappe3r5_s.py \\\n",
    "-reducer /home/cloudera/reduce3r5_s.py \\\n",
    "-input result3s8_3 \\\n",
    "-output result3s8_3_sorted\n",
    "\n",
    "!hdfs dfs -cat result3s8_3_sorted/* | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
